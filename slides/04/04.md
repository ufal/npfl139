title: NPFL139, Lecture 4
class: title, langtech, cc-by-sa
# Function Approximation,<br>Deep Q Network, Rainbow

## Milan Straka

### March 12, 2025

---
class: section
# Refresh

---
# Function Approximation

We approximate value function $v$ and/or action-value function $q$, selecting it
from a family of functions parametrized by a weight vector $â†’w âˆˆ â„^d$.

We denote the approximations as
$$\begin{gathered}
  vÌ‚(s; â†’w),\\
  qÌ‚(s, a; â†’w).
\end{gathered}$$

~~~
We utilize the _Mean Squared Value Error_ objective, denoted $\overline{VE}$:
$$\overline{VE}(â†’w) â‰ âˆ‘_{sâˆˆğ“¢} Î¼(s) \left[v_Ï€(s) - vÌ‚(s, â†’w)\right]^2,$$
where the state distribution $Î¼(s)$ is usually the on-policy distribution.

---
# Function Approximation

The on-policy distribution is defined as:

- For **episodic tasks**, let $h(s)$ be the probability that an episodes starts in state $s$,
  and let $Î·(s)$ denote the number of time steps spent, on average, in state $s$
  in a single episode:
  $$Î·(s) = h(s) + âˆ‘\nolimits_{s'}Î·(s')âˆ‘\nolimits_a Ï€(a|s') p(s|s', a).$$

  The on-policy distribution is then obtained by normalizing: $Î¼(s) â‰ \frac{Î·(s)}{âˆ‘_{s'} Î·(s')}.$

  ![w=30%,f=right](../03/discounting_as_termination.svgz)

  If there is discounting ($Î³<1$), it should be treated as a form of
  termination, by including a factor $Î³$ to the second term of the $Î·(s)$ equation.

- For **continuing tasks**, we require $Î³<1$, and employ the same definition as
  in the episodic case.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $â†’w$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  â†’w_{t+1} &â† â†’w_t - \tfrac{1}{2} Î± âˆ‡_{â†’w_t} \big(v_Ï€(S_t) - vÌ‚(S_t; â†’w_t)\big)^2\\
           &â† â†’w_t + Î±\big(v_Ï€(S_t) - vÌ‚(S_t; â†’w_t)\big) âˆ‡_{â†’w_t} vÌ‚(S_t; â†’w_t).\\
\end{aligned}$$

As usual, the $v_Ï€(S_t)$ is estimated by a suitable sample of a return:
- in Monte Carlo methods, we use episodic return $G_t$,
- in temporal difference methods, we employ bootstrapping and use
  one-step return
  $$R_{t+1} + [Â¬\textrm{done}]â‹…Î³vÌ‚(S_{t+1}; â†’w)$$
  or an $n$-step return.

---
class: middle
# Monte Carlo Gradient Policy Evaluation

![w=100%](../03/grad_mc_estimation.svgz)

If the return estimate $G_t$ is unbiased (which it is in a Monte Carlo method),
the policy evaluation algorithm is guaranteed to converge to a local optimum of
the mean squared value error under the usual SGD conditions.

---
section: Semi-Gradient
class: section
# Gradient and Semi-Gradient Methods

---
# Temporal Difference Semi-Gradient Policy Evaluation

In TD methods, we again bootstrap the estimate $v_Ï€(S_t)$ as
$R_{t+1} + [Â¬\textrm{done}]â‹…Î³vÌ‚(S_{t+1}; â†’w)$.

~~~
![w=70%,h=center](grad_td_estimation.svgz)

---
# Why Semi-Gradient TD

Note that the above algorithm is called **semi-gradient**, because it does not
backpropagate through $vÌ‚(S_{t+1}; â†’w)$:
$$â†’w â† â†’w + Î±\big(R_{t+1} + [Â¬\textrm{done}]â‹…Î³vÌ‚(S_{t+1}; â†’w) - vÌ‚(S_t; â†’w)\big) âˆ‡_{â†’w} vÌ‚(S_t; â†’w).$$

~~~
In other words, the above rule is in fact not an SGD update, because there does
not exist a function $J(â†’w)$, for which we would get the above update.

~~~
To sketch a proof, consider a linear $vÌ‚(S_t; â†’w) = âˆ‘_i x(S_t)_i w_i$ and assume such a $J(â†’w)$ exists.
Then
$$\tfrac{âˆ‚}{âˆ‚w_i}J(â†’w) = \big(R_{t+1} + Î³vÌ‚(S_{t+1}; â†’w) - vÌ‚(S_t; â†’w)\big) x(S_t)_i.$$

~~~
Now considering second derivatives, we see they are not equal, which is a contradiction:
$$\begin{aligned}
  \tfrac{âˆ‚}{âˆ‚w_i}\tfrac{âˆ‚}{âˆ‚w_j}J(â†’w) &= \big(Î³x(S_{t+1})_i - x(S_t)_i\big) x(S_t)_j = Î³x(S_{t+1})_i x(S_t)_j - x(S_t)_i x(S_t)_j \\
  \tfrac{âˆ‚}{âˆ‚w_j}\tfrac{âˆ‚}{âˆ‚w_i}J(â†’w) &= \big(Î³x(S_{t+1})_j - x(S_t)_j\big) x(S_t)_i = Î³x(S_{t+1})_j x(S_t)_i - x(S_t)_i x(S_t)_j
\end{aligned}$$

---
# Gradient and Semi-Gradient Methods

Note that â€œfixingâ€ the algorithm by allowing to backpropagate through the
bootstrap estimate $R_{t+1} + vÌ‚(S_{t+1}; â†’w)$ would not work at all. If
we consider such an update

$$\begin{aligned}
  â†’w_{t+1} &â† â†’w_t - \tfrac{1}{2} Î± âˆ‡_{â†’w_t} \big(R_{t+1} + vÌ‚(S_{t+1}; â†’w) - vÌ‚(S_t; â†’w_t)\big)^2\\
           &â† â†’w_t + Î±\big(R_{t+1} + vÌ‚(S_{t+1}; â†’w) - vÌ‚(S_t; â†’w_t)\big) âˆ‡_{â†’w_t} \big(vÌ‚(S_t; â†’w_t) \textcolor{red}{- vÌ‚(S_{t+1}; â†’w)}\big),\\
\end{aligned}$$

~~~
then for the tabular method, i.e., the linear method $vÌ‚\big(â†’x(s); â†’w\big) â‰ â†’x(s)^T â†’w$
with one-hot feature vectors, we would get
$$â†’w_{t+1} â† â†’w_t + Î±\big(R_{t+1} + vÌ‚(S_{t+1}; â†’w) - vÌ‚(S_t; â†’w_t)\big) \big(â†’x(S_t) \textcolor{red}{- â†’x(S_{t+1})}\big).$$

~~~
Therefore, we would update not only the value estimate for state $S_t$, but also
the value estimate for $S_{t+1}$ in the opposite direction.

---
# Temporal Difference Semi-Gradient Convergence

It can be proven (by using separate theory than for SGD) that the linear
semi-gradient TD methods do converge.

~~~
However, they do not converge to the optimum of $\overline{VE}$. Instead, they
converge to a different **TD fixed point** $â†’w_\mathrm{TD}$.

~~~
It can be proven that
$$\overline{VE}(â†’w_\mathrm{TD}) â‰¤ \frac{1}{1-Î³} \min_{â†’w} \overline{VE}(â†’w).$$

~~~
However, when $Î³$ is close to one, the multiplication factor in the above bound
is quite large.

---
# Temporal Difference Semi-Gradient Policy Evaluation

As before, we can utilize $n$-step TD methods.

![w=65%,h=center](grad_td_nstep_estimation.svgz)

---
# Temporal Difference Semi-Gradient Policy Evaluation

![w=35%,f=right](../03/grad_mc_estimation_example.svgz)

Recall the previous described 1000-state random walk, where transitions lead
uniformly randomly to any of 100 neighboring states on the left or on the right.
Using state aggregation, we can partition the 1000 states into 10 groups of 100
states. Monte Carlo policy evaluation result is on the right:

~~~
The results using one-step TD(0) are presented below (left); the effect of
increasing $n$ in an $n$-step variant is on the right.

![w=78%,h=center](grad_td_estimation_example.svgz)

---
# Sarsa with Function Approximation

Until now, we talked only about policy evaluation. Naturally, we can extend it
to a full Sarsa algorithm:

![w=80%,h=center](grad_sarsa.svgz)

---
# Sarsa with Function Approximation

Additionally, we can incorporate $n$-step returns:

![w=55%,h=center](grad_sarsa_nstep.svgz)

---
# Mountain Car Example via Sarsa

![w=65%,h=center](mountain_car.png)

The performances are for semi-gradient Sarsa($Î»$) algorithm (which we did not
talked about yet) with tile coding of 8 overlapping tiles covering position and
velocity, with offsets of $(1, 3)$.

---
# Mountain Car Example via Sarsa

![w=50%,h=center](mountain_car_performance_1and8_step.svgz)
![w=50%,h=center](mountain_car_performance_nstep.svgz)

---
section: Off-policy Divergence
class: section
# Off-policy Divergence With Function Approximation

---
# Off-policy Divergence With Function Approximation

Consider a deterministic transition between two states whose values are computed
using the same weight:

![w=20%,h=center](off_policy_divergence_idea.svgz)

~~~
- If initially $w=10$, the TD error will be also 10 (or nearly 10 if $Î³<1$).
~~~
- If for example $Î±=0.1$, $w$ will be increased to 11 (by 10%).
~~~
- This process can continue indefinitely.

~~~
However, the problem arises only in off-policy setting, where we do not decrease
value of the second state from further observation.

---
# Off-policy Divergence With Function Approximation

The previous idea can be implemented for instance by the following **Baird's
counterexample**:

![w=77%,h=center](off_policy_divergence_example.svgz)

The rewards are zero everywhere, so the value function is also zero everywhere.
We assume the initial values of weights are 1, except for $w_7=10$, and that the
learning rate $Î±=0.01$.

---
# Off-policy Divergence With Function Approximation

For off-policy semi-gradient Sarsa, or even for off-policy
dynamic-programming update (where we compute expectation over all following
states and actions), the weights diverge to $+âˆ$.
Using on-policy distribution converges fine.

$$â†’w â† â†’w + \frac{Î±}{|ğ“¢|} âˆ‘_s \Big(ğ”¼_Ï€ \big[R_{t+1} + Î³vÌ‚(S_{t+1}; â†’w) | S_t=s\big] - vÌ‚(s; â†’w)\Big) âˆ‡vÌ‚(s; â†’w)$$

![w=47%](off_policy_divergence_example.svgz)![w=53%](off_policy_divergence_results.svgz)

---
# Off-policy Divergence With Function Approximation

The divergence can happen when all following elements are combined:

- functional approximation;

~~~
- bootstrapping;

~~~
- off-policy training.

In the Sutton's and Barto's book, these are called **the deadly triad**.

---
section: DQN
class: section
# Deep Q Networks

---
# Deep Q Networks

Volodymyr Mnih et al.: _Playing Atari with Deep Reinforcement Learning_ (Dec 2013 on arXiv),

~~~
in Feb 2015 accepted in Nature as _Human-level control through deep reinforcement learning_.

~~~
Off-policy Q-learning algorithm with a convolutional neural network function
approximation of action-value function.

~~~
Training can be extremely brittle (and can even diverge as shown earlier).

---
# Deep Q Network

![w=85%,h=center](dqn_architecture.svgz)

---
# Deep Q Networks

- Preprocessing: $210Ã—160$ 128-color images are converted to grayscale and
  then resized to $84Ã—84$.
~~~
- **Frame skipping** technique is used, i.e., only every $4^\textrm{th}$ frame
  (out of 60 per second) is considered, and the selected action is repeated on
  the other frames.
~~~
- **Frame stacking** is utilizied â€“ the input to the network are the last $4$
  frames (considering only the frames kept by frame skipping), i.e., the network
  inpus is an image with $4$ channels.
~~~
- The network is fairly standard, performing
  - 32 filters of size $8Ã—8$ with stride 4 and ReLU,
  - 64 filters of size $4Ã—4$ with stride 2 and ReLU,
  - 64 filters of size $3Ã—3$ with stride 1 and ReLU,
  - fully connected layer with 512 units and ReLU,
  - output layer with 18 output units (one for each action)

---
# Deep Q Networks

- Network is trained with RMSProp to minimize the following loss:
  $$ğ“› â‰ ğ”¼_{(s, a, r, s')âˆ¼\mathrm{data}}\left[(r + \left[Â¬\textrm{done}\right] â‹… Î³ \max\nolimits_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸))^2\right].$$
~~~
- An $Îµ$-greedy behavior policy is utilized (starts at $Îµ=1$ and gradually decreases to $0.1$).

Important improvements:
~~~
- **experience replay**: the generated episodes are stored in a buffer as $(s, a, r,
  s')$ quadruples, and for training a transition is sampled uniformly
  (off-policy training);
~~~
- separate **target network** $â†’Î¸Ì„$: to prevent instabilities, a separate _target
  network_ is used to estimate one-step returns. The weights are not trained,
  but copied from the trained network after a fixed number of gradient updates;
~~~
- reward clipping: because rewards have wildly different scale in different
  games, all positive rewards are replaced by $+1$ and negative by $-1$;
  life loss is used as an end of episode.
~~~
  - furthermore, $(r + \left[Â¬\textrm{done}\right] â‹… Î³ \max_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸))$ is
    also clipped to $[-1, 1]$ (i.e., a $\textrm{smooth}_{L_1}$ loss or Huber loss).

---
# Deep Q Networks

![w=60%,h=center](dqn_algorithm.svgz)

---
# Deep Q Network

![w=40%,h=center](dqn_results.svgz)

---
# Deep Q Network

![w=80%,h=center](dqn_visualization_breakout.svgz)

---
# Deep Q Network

![w=100%,v=middle](dqn_visualization_pong.svgz)


---
class: tablewide
style: td:nth-of-type(1) {width: 75%}
# Deep Q Networks Hyperparameters

| Hyperparameter | Value |
|----------------|-------|
| minibatch size | 32 |
~~~
| replay buffer size | 1M |
~~~
| target network update frequency | 10k |
~~~
| discount factor | 0.99 |
~~~
| training frames | 50M |
~~~
| RMSProp learning rate and both momentums | 0.00025, 0.95 |
~~~
| initial $Îµ$, final $Îµ$ (linear decay) and frame of final $Îµ$ | 1.0, 0.1, 1M |
~~~
| replay start size | 50k |
~~~
| no-op max | 30 |

---
section: Rainbow
class: section
# Rainbow

---
# Rainbow

There have been many suggested improvements to the DQN architecture. In the end
of 2017, the _Rainbow: Combining Improvements in Deep Reinforcement Learning_
paper combines 6 of them into a single architecture they call **Rainbow**.

~~~
![w=38%,h=center](rainbow_results.svgz)

---
section: DDQN
class: section
# Double Deep Q-Network

---
# Q-learning and Maximization Bias

Because behaviour policy in Q-learning is $Îµ$-greedy variant of the target
policy, the same samples (up to $Îµ$-greedy) determine both the maximizing action
and estimate its value.

~~~
![w=75%,h=center](../03/double_q_learning_example.svgz)

---
# Double Q-learning

![w=80%,h=center](../03/double_q_learning.svgz)

---
style: .katex-display { margin: .5em 0 }
# Rainbow DQN Extensions

## Double Deep Q-Network

Similarly to double Q-learning, instead of
$$r + Î³ \max_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸),$$
we minimize
$$r + Î³ Q(s', \argmax_{a'}Q(s', a'; â†’Î¸); â†’Î¸Ì„) - Q(s, a; â†’Î¸).$$

~~~
![w=39%,h=center](ddqn_errors.svgz)

---
# Rainbow DQN Extensions

## Double Deep Q-Network

![w=83%,h=center](ddqn_errors_analysis.svgz)
![w=83%,h=center](ddqn_errors_analysis_caption.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=64%,h=center](ddqn_analysis.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

Performance on episodes taking at most 5 minutes and no-op starts on 49 games:
![w=35%,h=center](ddqn_results_5min.svgz)

~~~
Performance on episodes taking at most 30 minutes and using 100 human starts on
each of the 49 games:
![w=55%,h=center](ddqn_results_30min.svgz)

~~~
The Double DQN follows the training protocol of DQN; the tuned version increases
the target network update from 10k to 30k steps, decreases exploration during
training from $Îµ=0.1$ to $Îµ=0.01$, and uses a shared bias for all action values
in the output layer of the network.

---
section: Prioritized Replay
class: section
# Prioritized Replay

---
# Rainbow DQN Extensions

## Prioritized Replay

Instead of sampling the transitions uniformly from the replay buffer,
we instead prefer those with a large TD error. Therefore, we sample transitions
according to their probability
$$p_t âˆ \Big|r + Î³ \max_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸)\Big|^Ï‰,$$
~~~
where $Ï‰$ controls the shape of the distribution (which is uniform for $Ï‰=0$
and corresponds to TD error for $Ï‰=1$).

~~~
New transitions are inserted into the replay buffer with maximum probability
to support exploration of all encountered transitions.

~~~
When combined with DDQN, the probabilities are naturally computed as
$$p_t âˆ \Big|r + Î³ Q(s', \argmax_{a'}Q(s', a'; â†’Î¸); â†’Î¸Ì„) - Q(s, a; â†’Î¸)\Big|^Ï‰,$$

---
# Rainbow DQN Extensions

## Prioritized Replay

Because we now sample transitions according to $p_t$ instead of uniformly,
on-policy distribution and sampling distribution differ. To compensate, we
therefore utilize importance sampling with ratio
$$Ï_t = \left( \frac{1/N}{p_t} \right) ^Î².$$

~~~
The authors utilize in fact â€œfor stability reasonsâ€
$$Ï_t / \max_i Ï_i.$$

---
# Rainbow DQN Extensions

## Prioritized Replay

![w=75%,h=center](prioritized_dqn_algorithm.svgz)

---
section: Dueling Networks
class: section
# Dueling Networks

---
# Rainbow DQN Extensions

## Dueling Networks

Instead of computing directly $Q(s, a; â†’Î¸)$, we compose it from the following quantities:
~~~
- average return in a given state $s$, $V(s; â†’Î¸) = \frac{1}{|ğ“|} âˆ‘_a Q(s, a; â†’Î¸)$,
~~~
- advantage function computing an **advantage** $Q(s, a; â†’Î¸) - V(s; Î¸)$ of action $a$ in state $s$.
~~~

![w=25%,h=center](dqn_dueling_architecture.png)

~~~
$$Q(s, a) â‰ V\big(f(s; Î¶); Î·\big) + A\big(f(s; Î¶), a; Ïˆ\big) - \frac{\sum_{a' âˆˆ ğ“} A(f(s; Î¶), a'; Ïˆ)}{|ğ“|}$$

---
# Rainbow DQN Extensions

## Dueling Networks

![w=100%,h=center](dqn_dueling_corridor.svgz)

Evaluation is performed using $Îµ$-greedy exploration with $Îµ=0.001$;
in the experiment, the horizontal corridor has a length of 50 steps, while the
vertical sections have both 10 steps.

---
# Rainbow DQN Extensions

## Dueling Networks

![w=32%,h=center](dqn_dueling_visualization.svgz)

---
# Rainbow DQN Extensions

## Dueling Networks

Results on all 57 games (retraining the original DQN on the 8 missing games).
`Single` refers to DDQN with a direct computation of $Q(s, a; â†’Î¸)$, `Single Clip`
corresponds to additional gradient clipping to norm at most 10 and larger
first hidden layer (so that duelling and single have roughly the same
number of parameters).

![w=70%,h=center,mh=65%,v=middle](dqn_dueling_results.svgz)

---
section: $N$-step
class: section
# Multi-step DQN

---
# Rainbow DQN Extensions
## Multi-step DQN

Instead of Q-learning, we use $n$-step variant of Q-learning, which estimates
return as
$$âˆ‘_{i=1}^n Î³^{i-1} R_i + Î³^n \max_{a'} Q(s', a'; â†’Î¸Ì„).$$

~~~
This changes the off-policy algorithm to on-policy (because the â€œinnerâ€ actions
are sampled from the behaviour distribution, but should follow the target distribution);
however, it is not discussed in any way by the authors.

---
section: NoisyNets
class: section
# Noisy Nets

---
# Rainbow DQN Extensions

## Noisy Nets

Noisy Nets are neural networks whose weights and biases are perturbed by
a parametric function of a noise.

~~~
The parameters $â†’Î¸$ of a regular neural network are in Noisy nets represented as
$$â†’Î¸ â‰ˆ â†’Î¼ + â†’Ïƒ âŠ™ â†’Îµ,$$
where $â†’Îµ$ is zero-mean noise with fixed statistics. We therefore learn the
parameters $(â†’Î¼, â†’Ïƒ)$.

~~~
A fully connected layer $â†’y = â†’w â†’x + â†’b$ with parameters $(â†’w, â†’b)$ is
represented in the following way in Noisy nets:
$$â†’y = (â†’Î¼_w + â†’Ïƒ_w âŠ™ â†’Îµ_w) â†’x + (â†’Î¼_b + â†’Ïƒ_b âŠ™ â†’Îµ_b).$$

~~~
Each $Ïƒ_{i,j}$ is initialized to $\frac{Ïƒ_0}{\sqrt{n}}$, where $n$ is the number
of input neurons of the layer in question, and $Ïƒ_0$ is a hyperparameter; commonly 0.5.

---
# Rainbow DQN Extensions

## Noisy Nets

The noise $Îµ$ can be for example independent Gaussian noise. However, for
performance reasons, factorized Gaussian noise is used to generate a matrix of
noise. If $Îµ_{i, j}$ is noise corresponding to a layer with $n$ inputs and $m$
outputs, we generate independent noise $Îµ_i$ for input neurons, independent
noise $Îµ_j$ for output neurons, and set
$$Îµ_{i,j} = f(Îµ_i) f(Îµ_j)~~~\textrm{for}~~~f(x) = \operatorname{sign}(x) \sqrt{|x|}.$$
~~~
The authors generate noise samples for every batch, sharing the noise for all
batch instances (consequently, during loss computation, online and target
network use independent noise).

~~~
### Deep Q Networks
When training a DQN, $Îµ$-greedy is no longer used (all policies are greedy), and
all fully connected layers are parametrized as noisy nets in both the current
and target network (i.e., networks produce samples from the distribution of
returns, and greedy actions still explore).

---
# Rainbow DQN Extensions

## Noisy Nets

![w=50%,h=center](dqn_noisynets_results.svgz)

![w=65%,h=center](dqn_noisynets_curves.svgz)

---
# Rainbow DQN Extensions

## Noisy Nets

![w=100%](dqn_noisynets_noise_study.svgz)

The $Î£Ì„$ is the mean-absolute of the noise weights $â†’Ïƒ_w$, i.e.,
$Î£Ì„ = \frac{1}{\textit{layer size}} \|â†’Ïƒ_w\|_1$.

---
section: DistributionalRL
class: section
# Distributional RL

---
# Rainbow DQN Extensions

## Distributional RL

Instead of an expected return $Q(s, a)$, we could estimate the distribution of
expected returns $Z(s, a)$ â€“ the _value distribution_.

~~~
The authors define the distributional Bellman operator $ğ“£^Ï€$ as:
$$ğ“£^Ï€ Z(s, a) â‰ R(s, a) + Î³ Z(S', A')~~~\textrm{for}~~~S'âˆ¼p(s, a), A'âˆ¼Ï€(S').$$

~~~
The authors of the paper prove similar properties of the distributional Bellman
operator compared to the regular Bellman operator, mainly being a contraction
under a suitable metric
~~~
(for Wasserstein metric $W_p$, the authors define
$WÌ„_p(Z_1, Z_2)â‰\sup_{s, a} W_p\big(Z_1(s, a), Z_2(s, a)\big)$ and prove that
$ğ“£^Ï€$ is a Î³-contraction in $WÌ„_p$).

---
style: .katex-display { margin: .8em 0 }
class: dbend
# Wasserstein Metric

For two probability distributions $Î¼, Î½$, Wasserstein metric $W_p$ is defined as
$$W_p(Î¼, Î½) â‰ \inf_{Î³âˆˆÎ“(Î¼,Î½)} \big(ğ”¼_{(x, y)âˆ¼Î³} \|x-y\|^d\big)^{1/p},$$
~~~
where $Î“(Î¼,Î½)$ is a set of all _couplings_, each being a a joint probability
distribution whose marginals are $Î¼$ and $Î½$, respectively.
~~~
A possible intuition is the optimal transport of probability mass from $Î¼$ to
$Î½$.

~~~
For distributions over reals with CDFs $F, G$, the optimal transport has an
analytic solution:

![w=27.5%,f=right](wasserstein-1.svgz)

$$W_p(Î¼, Î½) = \bigg(âˆ«\nolimits_0^1 |F^{-1}(q) - G^{-1}(q)|^p \d q\bigg)^{1/p},$$
where $F^{-1}$ and $G^{-1}$ are _quantile functions_, i.e., inverse CDFs.

~~~
For $p=1$, the 1-Wasserstein metric correspond to area â€œbetweenâ€ F and G, and
in that case we can compute it also as $W_1(Î¼, Î½) = âˆ«\nolimits_x \big|F(x)- G(x)\big| \d x.$

---
# Rainbow DQN Extensions

## Distributional RL

The distribution of returns is modeled as a discrete distribution parametrized
by the number of atoms $N âˆˆ â„•$ and by $V_\textrm{MIN}, V_\textrm{MAX} âˆˆ â„$.
Support of the distribution are atoms
$$\{z_i â‰ V_\textrm{MIN} + i Î”z : 0 â‰¤ i < N\}\textrm{~~~for~}Î”z â‰ \frac{V_\textrm{MAX} - V_\textrm{MIN}}{N-1}.$$

~~~
The atom probabilities are predicted using a $\softmax$ distribution as
$$Z_{â†’Î¸}(s, a) = \left\{z_i\textrm{ with probability }p_i = \frac{e^{f_i(s, a; â†’Î¸)}}{âˆ‘_j e^{f_j(s, a; â†’Î¸)}}\right\}.$$

---
# Rainbow DQN Extensions

## Distributional RL

![w=30%,f=right](dqn_distributional_operator.svgz)

After the Bellman update, the support of the distribution $R(s, a) + Î³Z(s', a')$
is not the same as the original support. We therefore project it to the original
support by proportionally mapping each atom of the Bellman update to immediate
neighbors in the original support.

~~~
$$Î¦\big(R(s, a) + Î³Z(s', a')\big)_i â‰
  âˆ‘_{j=1}^N \left[ 1 - \frac{\left|[r + Î³z_j]_{V_\textrm{MIN}}^{V_\textrm{MAX}}-z_i\right|}{Î”z} \right]_0^1 p_j(s', a').$$

~~~
The network is trained to minimize the Kullbeck-Leibler divergence between the
current distribution and the (mapped) distribution of the one-step update
$$D_\textrm{KL}\Big(Î¦\big(R + Î³Z_{â†’Î¸Ì„}\big(s', \argmax_{a'} ğ”¼Z_{â†’Î¸Ì„}(s', a')\big)\big) \Big\| Z_{â†’Î¸}\big(s, a\big)\Big).$$

---
# Rainbow DQN Extensions

## Distributional RL

![w=50%,h=center](dqn_distributional_algorithm.svgz)


---
# Rainbow DQN Extensions

## Distributional RL

![w=40%,h=center](dqn_distributional_results.svgz)

![w=40%,h=center](dqn_distributional_example_distribution.svgz)

---
# Rainbow DQN Extensions

## Distributional RL

![w=100%](dqn_distributional_example_distributions.svgz)

---
# Rainbow DQN Extensions

## Distributional RL

![w=100%](dqn_distributional_atoms_ablation.svgz)
