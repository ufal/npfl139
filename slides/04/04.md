title: NPFL139, Lecture 4
class: title, langtech, cc-by-sa
# Function Approximation,<br>Deep Q Network, Rainbow

## Milan Straka

### March 11, 2024

---
# Function Approximation

We will approximate value function $v$ and/or action-value function $q$,
selecting it from a family of functions parametrized by a weight vector $â†’w âˆˆ â„^d$.

We denote the approximations as
$$\begin{gathered}
  vÌ‚(s; â†’w),\\
  qÌ‚(s, a; â†’w).
\end{gathered}$$

~~~
We utilize the _Mean Squared Value Error_ objective, denoted $\overline{VE}$:
$$\overline{VE}(â†’w) â‰ âˆ‘_{sâˆˆğ“¢} Î¼(s) \left[v_Ï€(s) - vÌ‚(s, â†’w)\right]^2,$$
where the state distribution $Î¼(s)$ is usually on-policy distribution.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $â†’w$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  â†’w_{t+1} &â† â†’w_t - \tfrac{1}{2} Î± âˆ‡_{â†’w_t} \big(v_Ï€(S_t) - vÌ‚(S_t; â†’w_t)\big)^2\\
           &â† â†’w_t + Î±\big(v_Ï€(S_t) - vÌ‚(S_t; â†’w_t)\big) âˆ‡_{â†’w_t} vÌ‚(S_t; â†’w_t).\\
\end{aligned}$$

As usual, the $v_Ï€(S_t)$ is estimated by a suitable sample of a return:
- in Monte Carlo methods, we use episodic return $G_t$,
- in temporal difference methods, we employ bootstrapping and use
  one-step return
  $$R_{t+1} + [Â¬\textrm{done}]â‹…Î³vÌ‚(S_{t+1}; â†’w)$$
  or an $n$-step return.

---
section: Semi-Gradient
# Temporal Difference Semi-Gradient Policy Evaluation

In TD methods, we again bootstrap the estimate $v_Ï€(S_t)$ as
$R_{t+1} + [Â¬\textrm{done}]â‹…Î³vÌ‚(S_{t+1}; â†’w)$.

~~~
![w=70%,h=center](grad_td_estimation.svgz)

---
# Why Semi-Gradient TD

Note that the above algorithm is called **semi-gradient**, because it does not
backpropagate through $vÌ‚(S_{t+1}; â†’w)$:
$$â†’w â† â†’w + Î±\big(R_{t+1} + [Â¬\textrm{done}]â‹…Î³vÌ‚(S_{t+1}; â†’w) - vÌ‚(S_t; â†’w)\big) âˆ‡_{â†’w} vÌ‚(S_t; â†’w).$$

~~~
In other words, the above rule is in fact not an SGD update, because there does
not exist a function $J(â†’w)$, for which we would get the above update.

~~~
To sketch a proof, consider a linear $vÌ‚(S_t; â†’w) = âˆ‘_i x(S_t)_i w_i$ and assume such a $J(â†’w)$ exists.
Then
$$\tfrac{âˆ‚}{âˆ‚w_i}J(â†’w) = \big(R_{t+1} + Î³vÌ‚(S_{t+1}; â†’w) - vÌ‚(S_t; â†’w)\big) x(S_t)_i.$$

~~~
Now considering second derivatives, we see they are not equal, which is a contradiction:
$$\begin{aligned}
  \tfrac{âˆ‚}{âˆ‚w_i}\tfrac{âˆ‚}{âˆ‚w_j}J(â†’w) &= \big(Î³x(S_{t+1})_i - x(S_t)_i\big) x(S_t)_j = Î³x(S_{t+1})_i x(S_t)_j - x(S_t)_i x(S_t)_j \\
  \tfrac{âˆ‚}{âˆ‚w_j}\tfrac{âˆ‚}{âˆ‚w_i}J(â†’w) &= \big(Î³x(S_{t+1})_j - x(S_t)_j\big) x(S_t)_i = Î³x(S_{t+1})_j x(S_t)_i - x(S_t)_i x(S_t)_j
\end{aligned}$$

---
# Temporal Difference Semi-Gradient Convergence

It can be proven (by using separate theory than for SGD) that the linear
semi-gradient TD methods do converge.

~~~
However, they do not converge to the optimum of $\overline{VE}$. Instead, they
converge to a different **TD fixed point** $â†’w_\mathrm{TD}$.

~~~
It can be proven that
$$\overline{VE}(â†’w_\mathrm{TD}) â‰¤ \frac{1}{1-Î³} \min_{â†’w} \overline{VE}(â†’w).$$

~~~
However, when $Î³$ is close to one, the multiplication factor in the above bound
is quite large.

---
# Temporal Difference Semi-Gradient Policy Evaluation

As before, we can utilize $n$-step TD methods.

![w=65%,h=center](grad_td_nstep_estimation.svgz)

---
# Temporal Difference Semi-Gradient Policy Evaluation

On the left, the results of one-step TD(0) algorithm are presented.
The effect of increasing $n$ in an $n$-step variant is displayed on the right.

![w=100%](grad_td_estimation_example.svgz)

---
# Sarsa with Function Approximation

Until now, we talked only about policy evaluation. Naturally, we can extend it
to a full Sarsa algorithm:

![w=80%,h=center](grad_sarsa.svgz)

---
# Sarsa with Function Approximation

Additionally, we can incorporate $n$-step returns:

![w=55%,h=center](grad_sarsa_nstep.svgz)

---
# Mountain Car Example

![w=65%,h=center](mountain_car.png)

The performances are for semi-gradient Sarsa($Î»$) algorithm (which we did not
talked about yet) with tile coding of 8 overlapping tiles covering position and
velocity, with offsets of $(1, 3)$.

---
# Mountain Car Example

![w=50%,h=center](mountain_car_performance_1and8_step.svgz)
![w=50%,h=center](mountain_car_performance_nstep.svgz)

---
section: Off-policy Divergence
# Off-policy Divergence With Function Approximation

Consider a deterministic transition between two states whose values are computed
using the same weight:

![w=20%,h=center](off_policy_divergence_idea.svgz)

~~~
- If initially $w=10$, the TD error will be also 10 (or nearly 10 if $Î³<1$).
~~~
- If for example $Î±=0.1$, $w$ will be increased to 11 (by 10%).
~~~
- This process can continue indefinitely.

~~~
However, the problem arises only in off-policy setting, where we do not decrease
value of the second state from further observation.

---
# Off-policy Divergence With Function Approximation

The previous idea can be implemented for instance by the following **Baird's
counterexample**:

![w=77%,h=center](off_policy_divergence_example.svgz)

The rewards are zero everywhere, so the value function is also zero everywhere.
We assume the initial values of weights are 1, except for $w_7=10$, and that the
learning rate $Î±=0.01$.

---
# Off-policy Divergence With Function Approximation

For off-policy semi-gradient Sarsa, or even for off-policy
dynamic-programming update (where we compute expectation over all following
states and actions), the weights diverge to $+âˆ$.
Using on-policy distribution converges fine.

$$â†’w â† â†’w + \frac{Î±}{|ğ“¢|} âˆ‘_s \Big(ğ”¼_Ï€ \big[R_{t+1} + Î³vÌ‚(S_{t+1}; â†’w) | S_t=s\big] - vÌ‚(s; â†’w)\Big) âˆ‡vÌ‚(s; â†’w)$$

![w=47%](off_policy_divergence_example.svgz)![w=53%](off_policy_divergence_results.svgz)

---
# Off-policy Divergence With Function Approximation

The divergence can happen when all following elements are combined:

- functional approximation;

~~~
- bootstrapping;

~~~
- off-policy training.

In the Sutton's and Barto's book, these are called **the deadly triad**.

---
section: DQN
# Deep Q Networks

Volodymyr Mnih et al.: _Playing Atari with Deep Reinforcement Learning_ (Dec 2013 on arXiv),

~~~
in Feb 2015 accepted in Nature as _Human-level control through deep reinforcement learning_.

~~~
Off-policy Q-learning algorithm with a convolutional neural network function
approximation of action-value function.

~~~
Training can be extremely brittle (and can even diverge as shown earlier).

---
# Deep Q Network

![w=85%,h=center](dqn_architecture.svgz)

---
# Deep Q Networks

- Preprocessing: $210Ã—160$ 128-color images are converted to grayscale and
  then resized to $84Ã—84$.
~~~
- **Frame skipping** technique is used, i.e., only every $4^\textrm{th}$ frame
  (out of 60 per second) is considered, and the selected action is repeated on
  the other frames.
~~~
- **Frame stacking** is utilizied â€“ the input to the network are the last $4$
  frames (considering only the frames kept by frame skipping), i.e., the network
  inpus is an image with $4$ channels.
~~~
- The network is fairly standard, performing
  - 32 filters of size $8Ã—8$ with stride 4 and ReLU,
  - 64 filters of size $4Ã—4$ with stride 2 and ReLU,
  - 64 filters of size $3Ã—3$ with stride 1 and ReLU,
  - fully connected layer with 512 units and ReLU,
  - output layer with 18 output units (one for each action)

---
# Deep Q Networks

- Network is trained with RMSProp to minimize the following loss:
  $$ğ“› â‰ ğ”¼_{(s, a, r, s')âˆ¼\mathrm{data}}\left[(r + \left[Â¬\textrm{done}\right] â‹… Î³ \max\nolimits_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸))^2\right].$$
~~~
- An $Îµ$-greedy behavior policy is utilized (starts at $Îµ=1$ and gradually decreases to $0.1$).

Important improvements:
~~~
- **experience replay**: the generated episodes are stored in a buffer as $(s, a, r,
  s')$ quadruples, and for training a transition is sampled uniformly
  (off-policy training);
~~~
- separate **target network** $â†’Î¸Ì„$: to prevent instabilities, a separate _target
  network_ is used to estimate one-step returns. The weights are not trained,
  but copied from the trained network after a fixed number of gradient updates;
~~~
- reward clipping: because rewards have wildly different scale in different
  games, all positive rewards are replaced by $+1$ and negative by $-1$;
  life loss is used as an end of episode.
~~~
  - furthermore, $(r + \left[Â¬\textrm{done}\right] â‹… Î³ \max_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸))$ is
    also clipped to $[-1, 1]$ (i.e., a $\textrm{smooth}_{L_1}$ loss or Huber loss).

---
# Deep Q Networks

![w=60%,h=center](dqn_algorithm.svgz)

---
# Deep Q Network

![w=40%,h=center](dqn_results.svgz)

---
# Deep Q Network

![w=80%,h=center](dqn_visualization_breakout.svgz)

---
# Deep Q Network

![w=100%,v=middle](dqn_visualization_pong.svgz)


---
class: tablewide
style: td:nth-of-type(1) {width: 75%}
# Deep Q Networks Hyperparameters

| Hyperparameter | Value |
|----------------|-------|
| minibatch size | 32 |
~~~
| replay buffer size | 1M |
~~~
| target network update frequency | 10k |
~~~
| discount factor | 0.99 |
~~~
| training frames | 50M |
~~~
| RMSProp learning rate and both momentums | 0.00025, 0.95 |
~~~
| initial $Îµ$, final $Îµ$ (linear decay) and frame of final $Îµ$ | 1.0, 0.1, 1M |
~~~
| replay start size | 50k |
~~~
| no-op max | 30 |

---
section: Rainbow
# Rainbow

There have been many suggested improvements to the DQN architecture. In the end
of 2017, the _Rainbow: Combining Improvements in Deep Reinforcement Learning_
paper combines 6 of them into a single architecture they call **Rainbow**.

~~~
![w=38%,h=center](rainbow_results.svgz)

---
section: DDQN
# Q-learning and Maximization Bias

Because behaviour policy in Q-learning is $Îµ$-greedy variant of the target
policy, the same samples (up to $Îµ$-greedy) determine both the maximizing action
and estimate its value.

~~~
![w=75%,h=center](../03/double_q_learning_example.svgz)

---
# Double Q-learning

![w=80%,h=center](../03/double_q_learning.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

Similarly to double Q-learning, instead of
$$r + Î³ \max_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸),$$
we minimize
$$r + Î³ Q(s', \argmax_{a'}Q(s', a'; â†’Î¸); â†’Î¸Ì„) - Q(s, a; â†’Î¸).$$

~~~
![w=30%,h=center](ddqn_errors.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=100%,h=center](ddqn_errors_analysis.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=60%,h=center](ddqn_analysis.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

Performance on episodes taking at most 5 minutes and no-op starts on 49 games:
![w=40%,h=center,mh=40%,v=middle](ddqn_results_5min.svgz)

~~~
Performance on episodes taking at most 30 minutes and using human starts on 49
games:
![w=55%,h=center,mh=40%,v=middle](ddqn_results_30min.svgz)

---
section: Prioritized Replay
# Rainbow DQN Extensions

## Prioritized Replay

Instead of sampling the transitions uniformly from the replay buffer,
we instead prefer those with a large TD error. Therefore, we sample transitions
according to their probability
$$p_t âˆ \Big|r + Î³ \max_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸)\Big|^Ï‰,$$
~~~
where $Ï‰$ controls the shape of the distribution (which is uniform for $Ï‰=0$
and corresponds to TD error for $Ï‰=1$).

~~~
New transitions are inserted into the replay buffer with maximum probability
to support exploration of all encountered transitions.

~~~
When combined with DDQN, the probabilities are naturally computed as
$$p_t âˆ \Big|r + Î³ Q(s', \argmax_{a'}Q(s', a'; â†’Î¸); â†’Î¸Ì„) - Q(s, a; â†’Î¸)\Big|^Ï‰,$$

---
# Rainbow DQN Extensions

## Prioritized Replay

Because we now sample transitions according to $p_t$ instead of uniformly,
on-policy distribution and sampling distribution differ. To compensate, we
therefore utilize importance sampling with ratio
$$Ï_t = \left( \frac{1/N}{p_t} \right) ^Î².$$

~~~
The authors utilize in fact â€œfor stability reasonsâ€
$$Ï_t / \max_i Ï_i.$$

---
# Rainbow DQN Extensions

## Prioritized Replay

![w=75%,h=center](prioritized_dqn_algorithm.svgz)

---
section: Dueling Networks
# Rainbow DQN Extensions

## Dueling Networks

Instead of computing directly $Q(s, a; â†’Î¸)$, we compose it from the following quantities:
- average return in a given state $s$;
- advantage function computing an **advantage** of using action $a$ in state $s$.

![w=25%,h=center](dqn_dueling_architecture.png)

~~~
$$Q(s, a) â‰ V\big(f(s; Î¶); Î·\big) + A\big(f(s; Î¶), a; Ïˆ\big) - \frac{\sum_{a' âˆˆ ğ“} A(f(s; Î¶), a'; Ïˆ)}{|ğ“|}$$

---
# Rainbow DQN Extensions

## Dueling Networks

![w=100%,h=center](dqn_dueling_corridor.svgz)

---
# Rainbow DQN Extensions

## Dueling Networks

![w=32%,h=center](dqn_dueling_visualization.svgz)

---
# Rainbow DQN Extensions

## Dueling Networks

Results on all 57 games (retraining the original DQN on the 8 missing games).
`Single` refers to DDQN with a direct computation of $Q(s, a; â†’Î¸)$, `Clip`
corresponds to additional gradient clipping to norm at most 10 and larger
first hidden layer (so that duelling and single have roughly the same
number of parameters).

![w=70%,h=center,mh=65%,v=middle](dqn_dueling_results.svgz)
