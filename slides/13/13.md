title: NPFL139, Lecture 13
class: title, langtech, cc-by-sa
# PlaNet, ST and Gumbel-softmax, DreamerV2, DreamerV3

## Milan Straka

### May 13, 2024

---
section: PlaNet
# PlaNet

In Nov 2018, an interesting paper from D. Hafner et al. proposed
a **Deep Planning Network (PlaNet)**, which is a model-based agent
that learns the MDP dynamics from pixels, and then chooses actions
using a CEM planner utilizing the learned compact latent space.

~~~
The PlaNet is evaluated on selected tasks from the DeepMind control suite
![w=100%,mh=50%,v=middle](planet_environments.svgz)

---
# PlaNet

In PlaNet, partially observable MDPs following the stochastic dynamics are
considered:
$$\begin{aligned}
\textrm{transition function:} && s_t &âˆ¼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &âˆ¼p(o_t|s_t), \\
\textrm{reward function:} && r_t &âˆ¼p(r_t|s_t), \\
\textrm{policy:} && a_t &âˆ¼p(a_t|o_{â‰¤t},a_{< t}).
\end{aligned}$$

~~~
The main goal is to train the first three â€“ the transition function, the
observation function, and the reward function.

---
# PlaNet â€“ Data Collection

![w=32%,f=left](planet_algorithm.svgz)

Because an untrained agent will most likely not cover all needed environment
states, we need to iteratively collect new experience and train the model.
The authors propose $S=5$, $C=100$, $B=50$, $L=50$, $R$ between 2 and 8.

~~~
For planning, CEM algorithm (capable of solving all tasks with a true model)
is used; $H=12$, $I=10$, $J=1000$, $K=100$.

![w=85%,mw=66%,h=center](planet_planner.svgz)

---
section: LatentModel
# PlaNet â€“ Latent Dynamics

![w=47%,f=right](planet_rssm.svgz)

First let us consider a typical latent-space model, consisting of
$$\begin{aligned}
\textrm{transition function:} && s_t &âˆ¼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &âˆ¼p(o_t|s_t), \\
\textrm{reward function:} && r_t &âˆ¼p(r_t|s_t).
\end{aligned}$$

~~~
The transition model is Gaussian with mean and variance predicted by a network,
the observation model is Gaussian with identity covariance and mean predicted by
a deconvolutional network, and the reward model is a scalar Gaussian with unit
variance and mean predicted by a neural network.

~~~
To train such a model, we turn to variational inference, and use an
encoder $q(s_{1:T}|o_{1:T},a_{1:T-1})=âˆ_{t=1}^T q(s_t|s_{t-1},a_{t-1},o_t)$,
which is a Gaussian with mean and variance predicted by a convolutional neural
network.

---
# PlaNet â€“ Training Objective

Using the encoder, we obtain the following variational lower bound on the
log-likelihood of the observations (for rewards the bound is analogous):
$$\begin{aligned}
  &\log p(o_{1:T}|a_{1:T}) \\
  &\quad = \log âˆ« âˆ_t p(s_t|s_{t-1},a_{t-1}) p(o_t|s_t)\d s_{1:T} \\
  &\quad â‰¥ âˆ‘_{t=1}^T \Big(
    \underbrace{ğ”¼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
    \underbrace{ğ”¼_{q(s_{t-1}|o_{â‰¤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{â‰¤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
  \Big).
\end{aligned}$$

~~~
We evaluate the expectations using a single sample, and use the reparametrization
trick to allow backpropagation through the sampling.

---
# PlaNet â€“ Training Objective Derivation

To derive the training objective, we employ importance sampling and the Jensenâ€™s
inequality:  
<br>

$\displaystyle \quad\log p(o_{1:T}|a_{1:T})$

$\displaystyle \qquad= \log ğ”¼_{p(s_{1:T}|a_{1:T})} âˆ_{t=1}^T p(o_t|s_t)$

~~~
$\displaystyle \qquad= \log ğ”¼_{q(s_{1:T}|o_{1:T},a_{1:T})} âˆ_{t=1}^T p(o_t|s_t) p(s_t|s_{t-1},a_{t-1}) / q(s_t|o_{â‰¤ t},a_{< t})$

~~~
$\displaystyle \qquadâ‰¥ ğ”¼_{q(s_{1:T}|o_{1:T},a_{1:T})} âˆ‘_{t=1}^T \log p(o_t|s_t) + \log p(s_t|s_{t-1},a_{t-1}) - \log q(s_t|o_{â‰¤ t},a_{< t})$

~~~
$\displaystyle \quad= \mathrlap{âˆ‘_{t=1}^T \Big(
  \underbrace{ğ”¼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
  \underbrace{ğ”¼_{q(s_{t-1}|o_{â‰¤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{â‰¤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
\Big).}$

---
section: RSSM
# PlaNet â€“ Recurrent State-Space Model

The purely stochastic transitions struggle to store information for multiple
timesteps. Therefore, the authors propose to include a deterministic path to the
model (providing access to all previous states), obtaining the
**recurrent state-space model (RSSM)**:
![w=55%,h=center](planet_rssm.svgz)

~~~
$$\begin{aligned}
\textrm{deterministic state model:} && h_t &= f(h_{t-1}, s_{t-1}, a_{t-1}), \\
\textrm{stochastic state function:} && s_t &âˆ¼p(s_t|h_t), \\
\textrm{observation function:} && o_t &âˆ¼p(o_t|h_t, s_t), \\
\textrm{reward function:} && r_t &âˆ¼p(r_t|h_t, s_t), \\
\textrm{encoder:} && q_t &âˆ¼q(s_t|h_t, o_t).
\end{aligned}$$

---
# PlaNet â€“ Results

![w=100%,v=middle](planet_results.svgz)

---
# PlaNet â€“ Ablations

![w=78%,h=center](planet_ablations1.svgz)

---
# PlaNet â€“ Ablations

![w=78%,h=center](planet_ablations2.svgz)

Random collection: random actions; random shooting: best action out of 1000 random seqs.

---
section: ST
# Discrete Latent Variables

Consider that we would like to have discrete neurons on the hidden layer
of a neural network.

~~~
Note that on the output layer, we relaxed discrete prediction (i.e.,
an $\argmax$) with a continuous relaxation â€“ $\softmax$. This way, we can
compute the derivatives and also predict the most probable class.
(It is possible to derive $\softmax$ as an entropy-regularized $\argmax$.)

~~~
However, on a hidden layer, we also need to _sample_ from the predicted
categorical distribution, and then backpropagate the gradients.

---
# Stochastic Gradient Estimators

![w=64%,h=center](stochastic_estimators.svgz)

---
# Stochastic Gradient Estimators

Consider a model with a discrete categorical latent variable $â†’z$ sampled from
$p(â†’z; â†’Î¸)$, with a loss $L(â†’z; â†’Ï‰)$.
~~~
Several gradient estimators have been proposed:

- A REINFORCE-like gradient estimation.

~~~
  Using the identity $âˆ‡_{â†’Î¸} p(â†’z; â†’Î¸) = p(â†’z; â†’Î¸) âˆ‡_{â†’Î¸} \log p(â†’z; â†’Î¸)$, we
  obtain that
~~~
  $$âˆ‡_{â†’Î¸} ğ”¼_{â†’z} \big[L(â†’z; â†’Ï‰)\big] = ğ”¼_{â†’z} \big[L(â†’z; â†’Ï‰) âˆ‡_{â†’Î¸} \log p(â†’z; â†’Î¸)\big].$$

~~~
  Analogously as before, we can also include the baseline for variance
  reduction, resulting in
  $$âˆ‡_{â†’Î¸} ğ”¼_{â†’z} \big[L(â†’z; â†’Ï‰)\big] = ğ”¼_{â†’z} \big[(L(â†’z; â†’Ï‰) - b) âˆ‡_{â†’Î¸} \log p(â†’z; â†’Î¸)\big].$$

~~~
- A **straight-through (ST)** estimator.

  The straight-through estimator has been proposed by Y. Bengio in 2013. It is
  a biased estimator, which assumes that $âˆ‡_{â†’Î¸} â†’z â‰ˆ âˆ‡_{â†’Î¸} p(â†’z; â†’Î¸)$, which implies
  $âˆ‡_{p(â†’z; â†’Î¸)} â†’z â‰ˆ 1$. Even if the bias can be considerable, it seems to work
  quite well in practice.

---
section: Gumbel-Softmax
# Gumbel-Softmax

The **Gumbel-softmax** distribution was proposed independently in two papers
in Nov 2016 (under the name of **Concrete** distribution in the other paper).

~~~
It is a continuous distribution over the simplex (over categorical
distributions) that can approximate _sampling_ from a categorical distribution.

~~~
Let $z$ be a categorical variable with class probabilities $â†’p = (p_1, p_2, â€¦, p_K)$.

~~~
Recall that the Gumbel-Max trick (based on a 1954 theorem from E. J. Gumbel) states that
we can draw samples $z âˆ¼ â†’p$ using
$$z = \operatorname{one-hot}\Big(\argmax_i \big(g_i + \log p_i\big)\Big),$$
where $g_i$ are independent samples drawn from the $\operatorname{Gumbel}(0, 1)$
distribution.

To sample $g$ from the distribution $\operatorname{Gumbel}(0, 1)$, we can sample
$u âˆ¼ U(0, 1)$ and then compute $g = -\log(-\log u)$.

---
# Gumbel-Softmax

To obtain a continuous distribution, we relax the $\argmax$ into a $\softmax$
with temperature $T$ as
$$z_i = \frac{e^{(g_i + \log p_i)/T}}{âˆ‘_j e^{(g_j + \log p_j)/T}}.$$

~~~
As the temperature $T$ goes to zero, the generated samples become one-hot, and
therefore the Gumbel-softmax distribution converges to the categorical
distribution $p(z)$.

![w=74%,h=center](gumbel_temperatures.svgz)

---
# Gumbel-Softmax Estimator

The Gumbel-softmax distribution can be used to reparametrize the sampling of the
discrete variable using a fully differentiable estimator.

![w=54%,f=right](stochastic_estimators.svgz)

~~~
However, the resulting sample is not discrete, it only converges to a discrete
sample as the temperature $T$ goes to zero.

~~~
If it is a problem, we can combine the Gumbel-softmax with a straight-through estimator,
obtaining ST Gumbel-softmax, where we:
- discretize $â†’y$ as $â†’z = \argmax â†’y$,
~~~
- assume $âˆ‡_{â†’Î¸}â†’z â‰ˆ âˆ‡_{â†’Î¸}â†’y$, or in other words, $\frac{âˆ‚â†’z}{âˆ‚â†’y} â‰ˆ 1$.

---
# Gumbel-Softmax Estimator Results

![w=68%,h=center](gumbel_results.svgz)

![w=49%](gumbel_results_sbn.svgz)![w=49%](gumbel_results_vae.svgz)

---
# Applications of Discrete Latent Variables

The discrete latent variables can be used among others to:
- allow the SAC algorithm to be used on **discrete** actions,
  using either Gumbel-softmax relaxation (if the critic takes
  the actions as binary indicators, it is possible to pass not
  just one-hot encoding, but the result of Gumbel-softmax directly),
  or a straight-through estimator;

~~~
- model images using discrete latent variables
  - VQ-VAE, VQ-VAE-2 use â€œcodebook lossâ€ with a straight-through estimator

    ![w=100%](vqvae.png)

---
# Applications of Discrete Latent Variables

- VQ-GAN combines the VQ-VAE and Transformers, where the latter is used
  to generate a sequence of the _discrete_ latents.

![w=100%](vqgan_architecture.png)

---
class: center
# Applications of Discrete Latent Variables â€“ VQ-GAN

<video controls style="width: 90%">
  <source src="https://github.com/CompVis/taming-transformers/raw/9539a92f08ebea816ec6ddecb2dedd6c8664ef08/images/taming.mp4" type="video/mp4">
</video>

---
# Applications of Discrete Latent Variables â€“ DALL-E

- In DALL-E, Transformer is used to model a sequence of words followed by
  a sequence of the discrete image latent variables.

  The Gumbel-softmax relaxation is used to train the discrete latent states,
  with temperature annealed with a cosine decay from 1 to 1/16 over the first
  150k (out of 3M) updates.

![w=100%,h=center](dalle.png)

---
section: DreamerV2
# DreamerV2

![w=40%,f=right](dreamerv2_performance.svgz)

The PlaNet model was followed by Dreamer (Dec 2019) and DreamerV2 (Oct 2020),
which train an agent using reinforcement learning using the model alone.
After 200M environment steps, it surpasses Rainbow on a collection of 55
Atari games (the authors do not mention why they do not use all 57 games)
when training on a single GPU for 10 days per game.

~~~
During training, a policy is learned from 486B compact states â€œdreamedâ€
by the model, which is 10,000 times more than the 50M observations from
the real environment (with action repeat 4).

~~~
Interestingly, the latent states are represented as a vector of several
**categorical** variables â€“ 32 variables with 32 classes each are utilized in
the paper.

---
section: ğŸ˜´â‚‚Model
# DreamerV2 â€“ Model Learning

The model in DreamerV2 is learned using the RSSM, collecting agent experiences
of observations, actions, rewards, and discount factors (0.995 within episode
and 0 at an episode end). Training is performed on batches of 50 sequences
of length at most 50 each.

~~~
![w=45%,f=right](dreamerv2_model_learning.svgz)

$$\begin{aligned}
\textrm{recurrent model:}      && h_t &= f_Ï†(h_{t-1},s_{t-1},a_{t-1}), \\
\textrm{representation model:} && s_t &âˆ¼ q_Ï†(s_t | h_t,x_t), \\
\textrm{transition predictor:} && sÌ„_t &âˆ¼ p_Ï†(sÌ„_t | h_t), \\
\textrm{image predictor:}      && xÌ„_t &âˆ¼ p_Ï†(xÌ„_t | h_t,s_t), \\
\textrm{reward predictor:}     && rÌ„_t &âˆ¼ p_Ï†(rÌ„_t | h_t,s_t), \\
\textrm{discount predictor:}     && Î³Ì„_t &âˆ¼ p_Ï†(Î³Ì„_t | h_t,s_t).
\end{aligned}$$

~~~
![w=75%,h=center](dreamerv2_st_gradients.svgz)

---
# DreamerV2 â€“ Model Learning

The following loss function is used:

$$\begin{aligned}
ğ“›(Ï†) = ğ”¼_{q_Ï†(s_{1:T} | a_{1:T}, x_{1:T})}\Big[âˆ‘\nolimits_{t=1}^T &
  \underbrace{-\log p_Ï†(x_t | h_t,s_t)}_\textrm{image log loss}
  \underbrace{-\log p_Ï†(r_t | h_t,s_t)}_\textrm{reward log loss}
  \underbrace{-\log p_Ï†(Î³_t | h_t,s_t)}_\textrm{discount log loss} \\
 &\underbrace{+Î² D_\textrm{KL}\big[q_Ï†(s_t | h_t,x_t) \| p_Ï†(s_t | h_t)\big]}_\textrm{KL loss}
\Big].
\end{aligned}$$

~~~
In the KL term, we train both the prior and the encoder. However, regularizing
the encoder towards the prior makes training harder (especially at the
beginning), so the authors propose **KL balancing**, minimizing the KL term
faster for the prior ($Î±=0.8$) than for the posterior.

![w=100%](dreamerv2_kl_balancing.svgz)

---
section: ğŸ˜´â‚‚Policy
# DreamerV2 â€“ Policy Learning

![w=50%,f=right](dreamerv2_policy_learning.svgz)

The policy is trained solely from the model, starting from the encountered
posterior states and then considering $H=15$ actions simulated in the compact
latent state.

~~~
We train an actor predicting $Ï€_Ïˆ(a_t | s_t)$ and a critic predicting
$$\textstyle v_Î¾(s_t) = ğ”¼_{p_Ï†, Ï€_Ïˆ} \big[âˆ‘_{r â‰¥ t} (âˆ_{r'=t+1}^r Î³_{r'}) r_t\big].$$

~~~
The critic is trained by estimating the truncated $Î»$-return as
$$V_t^Î» = r_t + \gamma_t\begin{cases}
  (1 - Î») v_Î¾(\hat{z}_{t+1}) + Î» V_{t+1}^Î» & \textrm{if~~}t<H, \\
  v_Î¾(\hat{z}_H) & \textrm{if~~}t=H. \\
\end{cases}$$
and then minimizing the MSE.

---
# DreamerV2 â€“ Policy Learning

The actor is trained using two approaches:
- the REINFORCE-like loss (with a baseline), which is unbiased, but has a high
  variance (even with the baseline);
~~~
- the reparametrization of discrete actions using a straight-through gradient
  estimation, which is biased, but has lower variance.

~~~
$$\begin{aligned}
ğ“›(Ïˆ) = ğ”¼_{p_Ï†,Ï€_Ïˆ}\Big[âˆ‘\nolimits_{t=1}^{H-1} \big(&
    \underbrace{-Ï \log Ï€_Ïˆ(a_t|s_t)\operatorname{stop\char`_gradient}(V_t^Î»-v_Î¾({s_t}))}_\textrm{reinforce} \\
   &\underbrace{-(1-Ï) V_t^Î»}_\textrm{dynamics backprop}\,\,
    \underbrace{-Î· H(a_t|s_t)}_\textrm{entropy regularizer}\big)\Big]
\end{aligned}$$

For Atari domains, authors use $Ï = 1$ and $Î·=10^{-3}$ (they say it works
â€œsubstantially betterâ€), while for continuous actions, $Ï = 0$ works
â€œsubstantially betterâ€ (presumably because of the bias in case of discrete
actions) and $Î·=10^{-4}$ is used.

---
# DreamerV2 â€“ Results

The authors evaluate on 55 Atari games. They argue that the commonly used
metrics have various flaws:
- **gamer-normalized median** ignores scores on half of the games,
- **gamer-normalized mean** is dominated by several games where the agent
  achieves super-human performance by several orders.

~~~
They therefore propose two additional ones:
- **record-normalized mean** normalizes with respect to any registered human
  world record for each game; however, in some games the agents still achieve
  super-human-record performance;
- **clipped record-normalized mean** additionally clips each score to 1;
  this measure is used as the primary metric in the paper.

---
# DreamerV2 â€“ Results

![w=100%](dreamerv2_results_graph.svgz)

![w=67%,h=center](dreamerv2_results_table.svgz)

Scheduling anneals actor gradient mixing $Ï$ (from 0.1 to 0), entropy loss scale,
KL, lr.

---
# DreamerV2 â€“ Ablations

![w=100%](dreamerv2_ablations.svgz)

![w=67%,h=center](dreamerv2_ablations_table.svgz)

---
# DreamerV2 â€“ Discrete Latent Variables

Categorical latent variables outperform Gaussian latent variables on 42 games,
tie on 5 games and decrease performance on 8 games (where a tie is defined as
being within 5\%).

The authors provide several hypotheses why could the categorical latent
variables be better:
- Categorical prior can perfectly match aggregated posterior, because mixture of
  categoricals is categorical, which is not true for Gaussians.

~~~
- Sparsity achieved by the 32 categorical variables with 32 classes each could
  be beneficial for generalization.

~~~
- Contrary to intuition, optimizing categorical variables might be easier than
  optimizing Gaussians, because the straight-through estimator ignores a term
  which would otherwise scale the gradient, which could reduce
  exploding/vanishing gradient problem.

~~~
- Categorical variables could be a better match for modeling discrete aspect
  of the Atari games (defeating an enemy, collecting reward, entering a room, â€¦).

---
# DreamerV2 â€“ Comparison, Hyperparametres

![w=100%](dreamerv2_comparison.svgz)

~~~
![w=100%](dreamerv2_hyperparameters.svgz)

---
section: ğŸ˜´â‚ƒ
# DreamerV3

![w=98%,h=center](dreamerv3_summary.svgz)

---
# DreamerV3

![w=100%,v=middle](dreamerv3_tasks.svgz)

---
# DreamerV3

![w=92%,h=center](dreamerv3_overview.svgz)

---
# DreamerV3

![w=86%,h=center](dreamerv3_observation_predictions.svgz)

---
# DreamerV3

![w=30%,f=right](dreamerv3_symlog.svgz)

$$\begin{aligned}
  \operatorname{symlog}(x) &â‰ \sign(x)\log\big(|x| + 1\big), \\
  \operatorname{symexp}(x) &â‰ \sign(x)\big(\exp(|x|) - 1\big). \\
\end{aligned}$$

~~~
$$ğ“›(â†’Î¸) â‰ \tfrac{1}{2} \big(f(â†’x; â†’Î¸) - \operatorname{symlog}(y)\big)^2$$

~~~
$$yÌ‚ â‰ \operatorname{symexp}\big(f(â†’x; â†’Î¸)\big)$$

---
# DreamerV3

$$\begin{aligned}
\textrm{RSSM}~~ & \textrm{sequence model:}      && h_t = f_Ï†(h_{t-1},s_{t-1},a_{t-1}), \\
\textrm{RSSM}~~ & \textrm{encoder:}             && s_t âˆ¼ q_Ï†(s_t | h_t,x_t), \\
\textrm{RSSM}~~ & \textrm{dynamics predictor:}  && sÌ„_t âˆ¼ p_Ï†(sÌ„_t | h_t), \\
& \textrm{decoder:}             && xÌ„_t âˆ¼ p_Ï†(xÌ„_t | h_t,s_t), \\
& \textrm{reward predictor:}    && rÌ„_t âˆ¼ p_Ï†(rÌ„_t | h_t,s_t), \\
& \textrm{continue predictor:}  && Î³Ì„_t âˆ¼ p_Ï†(Î³Ì„_t | h_t,s_t).
\end{aligned}$$

---
# DreamerV3

$$ğ“›(â†’Ï†) â‰ ğ”¼_{q_Ï†} \Big[âˆ‘_{t=1}^T\big(
  \underbrace{Î²_\textrm{pred}}_{1.0} ğ“›_\textrm{pred}(â†’Ï†)
+ \underbrace{Î²_\textrm{dyn}}_{0.5} ğ“›_\textrm{dyn}(â†’Ï†)
+ \underbrace{Î²_\textrm{rep}}_{0.1} ğ“›_\textrm{rep}(â†’Ï†)\big)\Big],$$

~~~
$$\begin{aligned}
ğ“›_\textrm{pred}(â†’Ï†) &â‰ -\log p_{â†’Ï†}(x_t | h_t, s_t) -\log p_{â†’Ï†}(r_t | h_t, s_t) -\log p_{â†’Ï†}(Î³_t | h_t, s_t), \\
ğ“›_\textrm{dyn}(â†’Ï†) &â‰ \max\Big(1, D_\textrm{KL}\big(\operatorname{sg}(q_{â†’Î¸}(s_t | h_t, x_t)) \big\| \hphantom{\operatorname{sg}(\,}p_{â†’Ï†}(s_t | h_t)\hphantom{)}\big)\Big), \\
ğ“›_\textrm{rep}(â†’Ï†) &â‰ \max\Big(1, D_\textrm{KL}\big(\hphantom{\operatorname{sg}(\,}q_{â†’Î¸}(s_t | h_t, x_t)\hphantom{)} \big\| \operatorname{sg}(p_{â†’Ï†}(s_t | h_t))\big)\Big). \\
\end{aligned}$$

---
# DreamerV3

![w=83%,h=center](dreamerv3_ablations_scaling.svgz)

---
# DreamerV3

![w=100%,v=middle](dreamerv3_ablations_kl.svgz)

---
# DreamerV3

![w=100%,v=middle](dreamerv3_ablations_critic.svgz)

---
# DreamerV3

![w=100%,v=middle](dreamerv3_ablations_actor.svgz)

---
# DreamerV3

![w=90%,h=center](dreamerv3_benchmark_parameters.svgz)

---
# DreamerV3

![w=100%,v=middle](dreamerv3_model_sizes.svgz)

---
# DreamerV3

![w=100%,v=middle](dreamerv3_hyperparameters.svgz)
