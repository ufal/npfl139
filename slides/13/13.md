title: NPFL139, Lecture 13
class: title, langtech, cc-by-sa
# PlaNet, ST and Gumbel-softmax, DreamerV2+3, MERLIN

## Milan Straka

### May 14, 2025

---
section: PlaNet
class: section
# PlaNet

---
# PlaNet

In Nov 2018, an interesting paper from D. Hafner et al. proposed
a **Deep Planning Network (PlaNet)**, which is a model-based agent
that learns the MDP dynamics from pixels, and then chooses actions
using a CEM planner utilizing the learned compact latent space.

~~~
The PlaNet is evaluated on selected tasks from the DeepMind control suite
![w=100%,mh=50%,v=middle](planet_environments.svgz)

---
# PlaNet

In PlaNet, partially observable MDPs following the stochastic dynamics are
considered:
$$\begin{aligned}
\textrm{transition function:} && s_t &âˆ¼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &âˆ¼p(o_t|s_t), \\
\textrm{reward function:} && r_t &âˆ¼p(r_t|s_t), \\
\textrm{policy:} && a_t &âˆ¼p(a_t|o_{â‰¤t},a_{< t}).
\end{aligned}$$

~~~
The main goal is to train the first three â€“ the transition function, the
observation function, and the reward function.

---
# PlaNet â€“ Data Collection

![w=32%,f=left](planet_algorithm.svgz)

Because an untrained agent will most likely not cover all needed environment
states, we need to iteratively collect new experience and train the model.
The authors propose $S=5$, $C=100$, $B=50$, $L=50$, $R$ between 2 and 8.

~~~
For planning, CEM algorithm (capable of solving all tasks with a true model)
is used; $H=12$, $I=10$, $J=1000$, $K=100$.

![w=85%,mw=66%,h=center](planet_planner.svgz)

---
section: LatentModel
# PlaNet â€“ Latent Dynamics

![w=47%,f=right](planet_rssm.svgz)

First let us consider a typical latent-space model, consisting of
$$\begin{aligned}
\textrm{transition function:} && s_t &âˆ¼p(s_t|s_{t-1},a_{t-1}), \\
\textrm{observation function:} && o_t &âˆ¼p(o_t|s_t), \\
\textrm{reward function:} && r_t &âˆ¼p(r_t|s_t).
\end{aligned}$$

~~~
The transition model is Gaussian with mean and variance predicted by a network,
the observation model is Gaussian with identity covariance and mean predicted by
a deconvolutional network, and the reward model is a scalar Gaussian with unit
variance and mean predicted by a neural network.

~~~
To train such a model, we turn to variational inference, and use an
encoder $q(s_{1:T}|o_{1:T},a_{1:T-1})=âˆ_{t=1}^T q(s_t|s_{t-1},a_{t-1},o_t)$,
which is a Gaussian with mean and variance predicted by a convolutional neural
network.

---
# PlaNet â€“ Training Objective

Using the encoder, we obtain the following variational lower bound on the
log-likelihood of the observations (for rewards the bound is analogous):
$$\begin{aligned}
  &\log p(o_{1:T}|a_{1:T}) \\
  &\quad = \log âˆ« âˆ_t p(s_t|s_{t-1},a_{t-1}) p(o_t|s_t)\d s_{1:T} \\
  &\quad â‰¥ âˆ‘_{t=1}^T \Big(
    \underbrace{ğ”¼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
    \underbrace{ğ”¼_{q(s_{t-1}|o_{â‰¤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{â‰¤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
  \Big).
\end{aligned}$$

~~~
We evaluate the expectations using a single sample, and use the reparametrization
trick to allow backpropagation through the sampling.

---
# PlaNet â€“ Training Objective Derivation

To derive the training objective, we employ importance sampling and the Jensenâ€™s
inequality:  
<br>

$\displaystyle \quad\log p(o_{1:T}|a_{1:T})$

$\displaystyle \qquad= \log ğ”¼_{p(s_{1:T}|a_{1:T})} âˆ_{t=1}^T p(o_t|s_t)$

~~~
$\displaystyle \qquad= \log ğ”¼_{q(s_{1:T}|o_{1:T},a_{1:T})} âˆ_{t=1}^T p(o_t|s_t) p(s_t|s_{t-1},a_{t-1}) / q(s_t|o_{â‰¤ t},a_{< t})$

~~~
$\displaystyle \qquadâ‰¥ ğ”¼_{q(s_{1:T}|o_{1:T},a_{1:T})} âˆ‘_{t=1}^T \log p(o_t|s_t) + \log p(s_t|s_{t-1},a_{t-1}) - \log q(s_t|o_{â‰¤ t},a_{< t})$

~~~
$\displaystyle \quad= \mathrlap{âˆ‘_{t=1}^T \Big(
  \underbrace{ğ”¼_{q(s_t|o_{\leq t},a_{< t})} \log p(o_t|s_t)}_\textrm{reconstruction} -
  \underbrace{ğ”¼_{q(s_{t-1}|o_{â‰¤ t-1},a_{<t-1})} D_\textrm{KL}\big(q(s_t|o_{â‰¤ t},a_{< t}) \| p(s_t|s_{t-1},a_{t-1})\big)}_\textrm{complexity}
\Big).}$

---
section: RSSM
# PlaNet â€“ Recurrent State-Space Model

The purely stochastic transitions struggle to store information for multiple
timesteps. Therefore, the authors propose to include a deterministic path to the
model (providing access to all previous states), obtaining the
**recurrent state-space model (RSSM)**:
![w=55%,h=center](planet_rssm.svgz)

~~~
$$\begin{aligned}
\textrm{deterministic state model:} && h_t &= f(h_{t-1}, s_{t-1}, a_{t-1}), \\
\textrm{stochastic state function:} && s_t &âˆ¼p(s_t|h_t), \\
\textrm{observation function:} && o_t &âˆ¼p(o_t|h_t, s_t), \\
\textrm{reward function:} && r_t &âˆ¼p(r_t|h_t, s_t), \\
\textrm{encoder:} && q_t &âˆ¼q(s_t|h_t, o_t).
\end{aligned}$$

---
# PlaNet â€“ Results

![w=100%,v=middle](planet_results.svgz)

---
# PlaNet â€“ Ablations

![w=77%,h=center](planet_ablations1.svgz)

---
# PlaNet â€“ Ablations

![w=77%,h=center](planet_ablations2.svgz)

Random collection: random actions; random shooting: best action out of 1000 random seqs.

---
section: ST
class: section
# Straight-Through (ST) Estimator

---
# Discrete Latent Variables

Consider that we would like to have discrete neurons on the hidden layer
of a neural network.

~~~
Note that on the output layer, we relaxed discrete prediction (i.e.,
an $\argmax$) with a continuous relaxation â€“ $\softmax$. This way, we can
compute the derivatives and also predict the most probable class.
(It is possible to derive $\softmax$ as an entropy-regularized $\argmax$.)

~~~
However, on a hidden layer, we also need to _sample_ from the predicted
categorical distribution, and then backpropagate the gradients.

---
# Stochastic Gradient Estimators

![w=64%,h=center](stochastic_estimators.svgz)

---
# Stochastic Gradient Estimators

Consider a model with a discrete categorical latent variable $â†’z$ sampled from
$p(â†’z; â†’Î¸)$, with a loss $L(â†’z; â†’Ï‰)$.
~~~
Several gradient estimators have been proposed:

- A REINFORCE-like gradient estimation.

~~~
  Using the identity $âˆ‡_{â†’Î¸} p(â†’z; â†’Î¸) = p(â†’z; â†’Î¸) âˆ‡_{â†’Î¸} \log p(â†’z; â†’Î¸)$, we
  obtain that
~~~
  $$âˆ‡_{â†’Î¸} ğ”¼_{â†’z} \big[L(â†’z; â†’Ï‰)\big] = ğ”¼_{â†’z} \big[L(â†’z; â†’Ï‰) âˆ‡_{â†’Î¸} \log p(â†’z; â†’Î¸)\big].$$

~~~
  Analogously as before, we can also include the baseline for variance
  reduction, resulting in
  $$âˆ‡_{â†’Î¸} ğ”¼_{â†’z} \big[L(â†’z; â†’Ï‰)\big] = ğ”¼_{â†’z} \big[(L(â†’z; â†’Ï‰) - b) âˆ‡_{â†’Î¸} \log p(â†’z; â†’Î¸)\big].$$

~~~
- A **straight-through (ST)** estimator.

  The straight-through estimator has been proposed by Y. Bengio in 2013. It is
  a biased estimator, which assumes that $âˆ‡_{â†’Î¸} â†’z â‰ˆ âˆ‡_{â†’Î¸} p(â†’z; â†’Î¸)$, which implies
  $âˆ‡_{p(â†’z; â†’Î¸)} â†’z â‰ˆ 1$. Even if the bias can be considerable, it seems to work
  quite well in practice.

---
section: Gumbel-Softmax
class: section
# Gumbel-Softmax

---
# Gumbel-Softmax

The **Gumbel-softmax** distribution was proposed independently in two papers
in Nov 2016 (under the name of **Concrete** distribution in the other paper).

~~~
It is a continuous distribution over the simplex (over categorical
distributions) that can approximate _sampling_ from a categorical distribution.

~~~
Let $z$ be a categorical variable with class probabilities $â†’p = (p_1, p_2, â€¦, p_K)$.

~~~
Recall that the Gumbel-Max trick (based on a 1954 theorem from E. J. Gumbel) states that
we can draw samples $z âˆ¼ â†’p$ using
$$z = \operatorname{one-hot}\Big(\argmax_i \big(g_i + \log p_i\big)\Big),$$
where $g_i$ are independent samples drawn from the $\operatorname{Gumbel}(0, 1)$
distribution.

To sample $g$ from the distribution $\operatorname{Gumbel}(0, 1)$, we can sample
$u âˆ¼ U(0, 1)$ and then compute $g = -\log(-\log u)$.

---
# Gumbel-Softmax

To obtain a continuous distribution, we relax the $\argmax$ into a $\softmax$
with temperature $T$ as
$$z_i = \frac{e^{(g_i + \log p_i)/T}}{âˆ‘_j e^{(g_j + \log p_j)/T}}.$$

~~~
As the temperature $T$ goes to zero, the generated samples become one-hot, and
therefore the Gumbel-softmax distribution converges to the categorical
distribution $p(z)$.

![w=74%,h=center](gumbel_temperatures.svgz)

---
# Gumbel-Softmax Estimator

The Gumbel-softmax distribution can be used to reparametrize the sampling of the
discrete variable using a fully differentiable estimator.

![w=54%,f=right](stochastic_estimators.svgz)

~~~
However, the resulting sample is not discrete, it only converges to a discrete
sample as the temperature $T$ goes to zero.

~~~
If it is a problem, we can combine the Gumbel-softmax with a straight-through estimator,
obtaining ST Gumbel-softmax, where we:
- discretize $â†’y$ as $â†’z = \argmax â†’y$,
~~~
- assume $âˆ‡_{â†’Î¸}â†’z â‰ˆ âˆ‡_{â†’Î¸}â†’y$, or in other words, $\frac{âˆ‚â†’z}{âˆ‚â†’y} â‰ˆ 1$.

---
# Gumbel-Softmax Estimator Results

![w=68%,h=center](gumbel_results.svgz)

![w=49%](gumbel_results_sbn.svgz)![w=49%](gumbel_results_vae.svgz)

---
# Applications of Discrete Latent Variables

The discrete latent variables can be used among others to:
- allow the SAC algorithm to be used on **discrete** actions,
  using either Gumbel-softmax relaxation (if the critic takes
  the actions as binary indicators, it is possible to pass not
  just one-hot encoding, but the result of Gumbel-softmax directly),
  or a straight-through estimator;

~~~
- model images using discrete latent variables
  - VQ-VAE, VQ-VAE-2 use â€œcodebook lossâ€ with a straight-through estimator

    ![w=100%](vqvae.png)

---
# Applications of Discrete Latent Variables

- VQ-GAN combines the VQ-VAE and Transformers, where the latter is used
  to generate a sequence of the _discrete_ latents.

![w=100%](vqgan_architecture.png)

---
class: center
# Applications of Discrete Latent Variables â€“ VQ-GAN

<video controls style="width: 90%">
  <source src="https://github.com/CompVis/taming-transformers/raw/9539a92f08ebea816ec6ddecb2dedd6c8664ef08/images/taming.mp4" type="video/mp4">
</video>

---
# Applications of Discrete Latent Variables â€“ DALL-E

- In DALL-E, Transformer is used to model a sequence of words followed by
  a sequence of the discrete image latent variables.

  The Gumbel-softmax relaxation is used to train the discrete latent states,
  with temperature annealed with a cosine decay from 1 to 1/16 over the first
  150k (out of 3M) updates.

![w=100%,h=center](dalle.png)

---
section: <span style="font-family: symbola">ğŸ˜´</span>DreamerV2
class: section
# DreamerV2

---
# DreamerV2

![w=40%,f=right](dreamerv2_performance.svgz)

The PlaNet model was followed by Dreamer (Dec 2019) and DreamerV2 (Oct 2020),
which train an agent using reinforcement learning using the model alone.
After 200M environment steps, it surpasses Rainbow on a collection of 55
Atari games (the authors do not mention why they do not use all 57 games)
when training on a single GPU for 10 days per game.

~~~
During training, a policy is learned from 486B compact states â€œdreamedâ€
by the model, which is 10,000 times more than the 50M observations from
the real environment (with action repeat 4).

~~~
Interestingly, the latent states are represented as a vector of several
**categorical** variables â€“ 32 variables with 32 classes each are utilized in
the paper.

---
# DreamerV2 â€“ Model Learning

The model in DreamerV2 is learned using the RSSM, collecting agent experiences
of observations, actions, rewards, and discount factors (0.995 within episode
and 0 at an episode end). Training is performed on batches of 50 sequences
of length at most 50 each.

~~~
![w=45%,f=right](dreamerv2_model_learning.svgz)

$$\begin{aligned}
\textrm{recurrent model:}      && h_t &= f_Ï†(h_{t-1},s_{t-1},a_{t-1}), \\
\textrm{representation model:} && s_t &âˆ¼ q_Ï†(s_t | h_t,x_t), \\
\textrm{transition predictor:} && sÌ„_t &âˆ¼ p_Ï†(sÌ„_t | h_t), \\
\textrm{image predictor:}      && xÌ„_t &âˆ¼ p_Ï†(xÌ„_t | h_t,s_t), \\
\textrm{reward predictor:}     && rÌ„_t &âˆ¼ p_Ï†(rÌ„_t | h_t,s_t), \\
\textrm{discount predictor:}     && Î³Ì„_t &âˆ¼ p_Ï†(Î³Ì„_t | h_t,s_t).
\end{aligned}$$

~~~
![w=75%,h=center](dreamerv2_st_gradients.svgz)

---
# DreamerV2 â€“ Model Learning

The following loss function is used:

$$\begin{aligned}
ğ“›(Ï†) = ğ”¼_{q_Ï†(s_{1:T} | a_{1:T}, x_{1:T})}\Big[âˆ‘\nolimits_{t=1}^T &
  \underbrace{-\log p_Ï†(x_t | h_t,s_t)}_\textrm{image log loss}
  \underbrace{-\log p_Ï†(r_t | h_t,s_t)}_\textrm{reward log loss}
  \underbrace{-\log p_Ï†(Î³_t | h_t,s_t)}_\textrm{discount log loss} \\
 &\underbrace{+Î² D_\textrm{KL}\big[q_Ï†(s_t | h_t,x_t) \| p_Ï†(s_t | h_t)\big]}_\textrm{KL loss}
\Big].
\end{aligned}$$

~~~
In the KL term, we train both the prior and the encoder. However, regularizing
the encoder towards the prior makes training harder (especially at the
beginning), so the authors propose **KL balancing**, minimizing the KL term
faster for the prior ($Î±=0.8$) than for the posterior.

![w=100%](dreamerv2_kl_balancing.svgz)

---
# DreamerV2 â€“ Policy Learning

![w=50%,f=right](dreamerv2_policy_learning.svgz)

The policy is trained solely from the model, starting from the encountered
posterior states and then considering $H=15$ actions simulated in the compact
latent state.

~~~
We train an actor predicting $Ï€_Ïˆ(a_t | s_t)$ and a critic predicting
$$\textstyle v_Î¾(s_t) = ğ”¼_{p_Ï†, Ï€_Ïˆ} \big[âˆ‘_{r â‰¥ t} (âˆ_{r'=t+1}^r Î³_{r'}) r_t\big].$$

~~~
The critic is trained by estimating the truncated $Î»$-return as
$$V_t^Î» = r_t + \gamma_t\begin{cases}
  (1 - Î») v_Î¾(\hat{z}_{t+1}) + Î» V_{t+1}^Î» & \textrm{if~~}t<H, \\
  v_Î¾(\hat{z}_H) & \textrm{if~~}t=H. \\
\end{cases}$$
and then minimizing the MSE.

---
# DreamerV2 â€“ Policy Learning

The actor is trained using two approaches:
- the REINFORCE-like loss (with a baseline), which is unbiased, but has a high
  variance (even with the baseline);
~~~
- the reparametrization of discrete actions using a straight-through gradient
  estimation, which is biased, but has lower variance.

~~~
$$\begin{aligned}
ğ“›(Ïˆ) = ğ”¼_{p_Ï†,Ï€_Ïˆ}\Big[âˆ‘\nolimits_{t=1}^{H-1} \big(&
    \underbrace{-Ï \log Ï€_Ïˆ(a_t|s_t)\operatorname{stop\char`_gradient}(V_t^Î»-v_Î¾({s_t}))}_\textrm{reinforce} \\
   &\underbrace{-(1-Ï) V_t^Î»}_\textrm{dynamics backprop}\,\,
    \underbrace{-Î· H(a_t|s_t)}_\textrm{entropy regularizer}\big)\Big]
\end{aligned}$$

~~~
For Atari domains, authors use $Ï = 1$ and $Î·=10^{-3}$ (they say it works
â€œsubstantially betterâ€), while for continuous actions, $Ï = 0$ works
â€œsubstantially betterâ€ (presumably because of the bias in case of discrete
actions) and $Î·=10^{-4}$ is used.

---
# DreamerV2 â€“ Results

The authors evaluate on 55 Atari games. They argue that the commonly used
metrics have various flaws:
- **gamer-normalized median** ignores scores on half of the games,
- **gamer-normalized mean** is dominated by several games where the agent
  achieves super-human performance by several orders.

~~~
They therefore propose two additional ones:
- **record-normalized mean** normalizes with respect to any registered human
  world record for each game; however, in some games the agents still achieve
  super-human-record performance;
- **clipped record-normalized mean** additionally clips each score to 1;
  this measure is used as the primary metric in the paper.

---
# DreamerV2 â€“ Results

![w=100%](dreamerv2_results_graph.svgz)

![w=67%,h=center](dreamerv2_results_table.svgz)

Scheduling anneals actor gradient mixing $Ï$ (from 0.1 to 0), entropy loss scale,
KL, lr.

---
# DreamerV2 â€“ Ablations

![w=100%](dreamerv2_ablations.svgz)

![w=67%,h=center](dreamerv2_ablations_table.svgz)

---
# DreamerV2 â€“ Discrete Latent Variables

Categorical latent variables outperform Gaussian latent variables on 42 games,
tie on 5 games and decrease performance on 8 games (where a tie is defined as
being within 5\%).

The authors provide several hypotheses why could the categorical latent
variables be better:
- Categorical prior can perfectly match aggregated posterior, because mixture of
  categoricals is categorical, which is not true for Gaussians.

~~~
- Sparsity achieved by the 32 categorical variables with 32 classes each could
  be beneficial for generalization.

~~~
- Contrary to intuition, optimizing categorical variables might be easier than
  optimizing Gaussians, because the straight-through estimator ignores a term
  which would otherwise scale the gradient, which could reduce
  exploding/vanishing gradient problem.

~~~
- Categorical variables could be a better match for modeling discrete aspect
  of the Atari games (defeating an enemy, collecting reward, entering a room, â€¦).

---
# DreamerV2 â€“ Comparison, Hyperparametres

![w=100%](dreamerv2_comparison.svgz)

~~~
![w=100%](dreamerv2_hyperparameters.svgz)

---
section: <span style="font-family: symbola">ğŸ˜´</span>DreamerV3
class: section
# DreamerV3

---
# DreamerV3

![w=88%,h=center](dreamerv3_summary.svgz)

---
# DreamerV3

![w=100%,v=middle](dreamerv3_tasks.svgz)

---
# DreamerV3

![w=92%,h=center](dreamerv3_overview.svgz)

---
# DreamerV3

![w=86%,h=center](dreamerv3_observation_predictions.svgz)

---
style: .katex-display { margin: .8em 0 }
# DreamerV3

![w=30%,f=right](dreamerv3_symlog.svgz)

To be able to predict rewards and returns in different scales, the authors
propose to use symmetrical and shifted logarithm:

$$\begin{aligned}
  \operatorname{symlog}(x) &â‰ \sign(x)\log\big(|x| + 1\big), \\
  \operatorname{symexp}(x) &â‰ \sign(x)\big(\exp(|x|) - 1\big). \\
\end{aligned}$$

~~~
The loss could then be
$$ğ“›(â†’Î¸) â‰ \tfrac{1}{2} \big(f(â†’x; â†’Î¸) - \operatorname{symlog}(y)\big)^2,$$

~~~
and we can reconstruct the original quantity as
$$yÌ‚ â‰ \operatorname{symexp}\big(f(â†’x; â†’Î¸)\big).$$

~~~
We can use this approach to predict also a whole distribution, trained using twohot loss:
$$B â‰ \operatorname{symexp}\big([-20, â€¦, 20]\big),~~~~~yÌ‚ â‰ \softmax\big(f(â†’x; â†’Î¸)\big)^T B,~~~~~ğ“›(â†’Î¸) â‰ -\operatorname{twohot}^T \log yÌ‚.$$

---
# DreamerV3

The world model is the same as in DreamerV2, here using the original notation:
$$\begin{aligned}
\textrm{RSSM}~~ & \textrm{sequence model:}      && h_t = f_Ï†(h_{t-1},s_{t-1},a_{t-1}), \\
\textrm{RSSM}~~ & \textrm{encoder:}             && s_t âˆ¼ q_Ï†(s_t | h_t,x_t), \\
\textrm{RSSM}~~ & \textrm{dynamics predictor:}  && sÌ„_t âˆ¼ p_Ï†(sÌ„_t | h_t), \\
& \textrm{decoder:}             && xÌ„_t âˆ¼ p_Ï†(xÌ„_t | h_t,s_t), \\
& \textrm{reward predictor:}    && rÌ„_t âˆ¼ p_Ï†(rÌ„_t | h_t,s_t), \\
& \textrm{continue predictor:}  && cÌ„_t âˆ¼ p_Ï†(cÌ„_t | h_t,s_t).
\end{aligned}$$

~~~
The observations are transformed using the $\operatorname{symlog}$ function,
both on the encoder input and decoder targets.

---
# DreamerV3

The overall world model loss is:
$$ğ“›(â†’Ï†) â‰ ğ”¼_{q_Ï†} \Big[âˆ‘_{t=1}^T\big(
  \underbrace{Î²_\textrm{pred}}_{1.0} ğ“›_\textrm{pred}(â†’Ï†)
+ \underbrace{Î²_\textrm{dyn}}_{1.0} ğ“›_\textrm{dyn}(â†’Ï†)
+ \underbrace{Î²_\textrm{rep}}_{0.1} ğ“›_\textrm{rep}(â†’Ï†)\big)\Big],$$

~~~
where the individual components are
$$\begin{aligned}
ğ“›_\textrm{pred}(â†’Ï†) &â‰ -\log p_{â†’Ï†}(x_t | h_t, s_t) -\log p_{â†’Ï†}(r_t | h_t, s_t) -\log p_{â†’Ï†}(c_t | h_t, s_t), \\
ğ“›_\textrm{dyn}(â†’Ï†) &â‰ \max\Big(1, D_\textrm{KL}\big(\operatorname{sg}(q_{â†’Î¸}(s_t | h_t, x_t)) \big\| \hphantom{\operatorname{sg}(\,}p_{â†’Ï†}(s_t | h_t)\hphantom{)}\big)\Big), \\
ğ“›_\textrm{rep}(â†’Ï†) &â‰ \max\Big(1, D_\textrm{KL}\big(\hphantom{\operatorname{sg}(\,}q_{â†’Î¸}(s_t | h_t, x_t)\hphantom{)} \big\| \operatorname{sg}(p_{â†’Ï†}(s_t | h_t))\big)\Big). \\
\end{aligned}$$

~~~
To stabilize training, the authors use _free bits_ (clipping the dynamics and
representation losses below 1 nat â‰ˆ 1.44 bits) and parametrize the categorical
distributions of the encoder, dynamics predictor, and actor distribution
as a mixture of 1% uniform and 99% neural network output.

---
# DreamerV3

The critic is trained to predict from the current state
$z_t=\{h_t, s_t/sÌ„_t\}$ the return $v_t(z_t; â†’Ï†)$ as a categorical distribution
over exponentially spaced bins $B$.

~~~
As target, the boostrapped $Î»$-return is used during training:
$$R_t^Î» â‰ r_t + Î³ cÌ„_t \big((1-Î») v_t + Î» R_{t+1}^Î»\big),$$
where for the imaginary horizon $T=16$, we use just the bootstrapped
return $R_T^Î» â‰ v_T$.

~~~
The critic is trained using a mixture of imaginary trajectories $\{h_t, sÌ„_t\}$
with loss scale $Î²_\mathit{val}=1$ and trajectories sample from the replay
buffer $\{h_t, s_t\}$ with loss scale $Î²_\mathit{repval}=0.3$.

~~~
The training is stabilized by a regularization loss of the critic to its
exponentially moving average of its own parameters, allowing to use the current
network for computing the returns.

---
style: .katex-display { margin: .4em 0 }
# DreamerV3

The actor is trained using entropy-regularized REINFORCE loss, for both discrete
and continuous actions:
$$ğ“›(â†’Î¸) â‰ -âˆ‘_{t=1}^T \operatorname{sg}\big((R_t^Î» - v_t(z_t; â†’Ï†) / \max(1, S)\big) \log Ï€(a_t | z_t; â†’Î¸) + Î·H\big[Ï€(a_t | s)t; â†’Î¸)\big],$$

~~~
where the returns are normalized using
$$S â‰ \operatorname{EMA}\big(\operatorname{Quantile}(R_t^Î», 0.95)-\operatorname{Quantile}(R_t^Î», 0.05), 0.99\big).$$

~~~
During training, uniform sampling is used, even if authors mention that
prioritized replay improved performance.

~~~
Each batch is a combination of online data (from the current interactions)
and data sampled from the replay buffer and then followed by the actor and the
world model.

~~~
The _replay ratio_ is the number of time steps trained for every single step
collected from the environment (without action repeat); e.g., for a replay
ratio of 32, action repeat (frame skip) 4, and batches of 64 sequences of 16
steps, a gradient update is performed every $4 â‹… 64 â‹… 16 / 32 = 128$ environment
steps.

---
# DreamerV3

![w=92%,h=center](dreamerv3_ablations.svgz)

---
# DreamerV3

![w=51.5%,h=center](dreamerv3_ablations_robustness.svgz)

---
# DreamerV3

![w=51%,h=center](dreamerv3_ablations_signals.svgz)

---
# DreamerV3

![w=70%,h=center](dreamerv3_benchmark_parameters.svgz)

![w=70%,h=center](dreamerv3_model_sizes.svgz)

---
# DreamerV3

![w=100%,v=middle](dreamerv3_hyperparameters.svgz)

---
section: MERLIN
class: section
# MERLIN

---
# MERLIN

In a partially-observable environment, keeping all information in the RNN state
is substantially limiting. Therefore, _memory-augmented_ networks can be used to
store suitable information in external memory (in the lines of NTM, DNC, or MANN
models).

We now describe an approach used by Merlin architecture (_Unsupervised
Predictive Memory in a Goal-Directed Agent_ DeepMind Mar 2018 paper).

![w=95.75%,mw=50%,h=center](merlin_rl-lstm.svgz)![w=95%,mw=50%,h=center](merlin_rl-mem.svgz)

---
# MERLIN â€“ Memory Module

![w=30%,f=right](merlin_rl-mem.svgz)

Let $â†’M$ be a memory matrix of size $N_\textit{mem} Ã— 2|â†’e|$.

~~~
Assume we have already encoded observations as $â†’e_t$ and previous action
$a_{t-1}$. We concatenate them with $K$ previously read vectors and process
them by a deep LSTM (two layers are used in the paper) to compute $â†’h_t$.

~~~
Then, we apply a linear layer to $â†’h_t$, computing $K$ key vectors
$â†’k_1, â€¦, â†’k_K$ of length $2|â†’e|$ and $K$ positive scalars $Î²_1, â€¦, Î²_K$.

~~~
**Reading:** For each $i$, we compute cosine similarity of $â†’k_i$ and all memory
rows $â†’M_j$, multiply the similarities by $Î²_i$ and pass them through a $\softmax$
to obtain weights $â†’Ï‰_i$. The read vector is then computed as $â‡‰M â†’Ï‰_i$.

~~~
**Writing:** We find one-hot write index $â†’v_\textit{wr}$ to be the least used
memory row (we keep usage indicators and add read weights to them). We then
compute $â†’v_\textit{ret} â† Î³ â†’v_\textit{ret} + (1 - Î³) â†’v_\textit{wr}$,
and retroactively update the memory matrix using
$â‡‰M â† â‡‰M + â†’v_\textit{wr}[â†’e_t, 0] + â†’v_\textit{ret}[0, â†’e_t]$.

---
# MERLIN â€” Prior and Posterior

However, updating the encoder and memory content purely using RL is inefficient.
Therefore, MERLIN includes a _memory-based predictor (MBP)_ in addition to policy.
The goal of MBP is to compress observations into low-dimensional state
representations $â†’z$ and storing them in memory.

~~~
We want the state variables not only to faithfully represent the data, but also
emphasise rewarding elements of the environment above irrelevant ones. To
accomplish this, the authors follow the hippocampal representation theory of
Gluck and Myers, who proposed that hippocampal representations pass through
a compressive bottleneck and then reconstruct input stimuli together with task
reward.

~~~
In MERLIN, a (Gaussian diagonal) _prior_ distribution over $â†’z_t$ predicts next state variable
conditioned on history of state variables and actions
$p(â†’z_t^\textrm{prior} | â†’z_{t-1}, a_{t-1}, â€¦, â†’z_1, a_1)$,
and _posterior_ corrects the prior using the new observation $â†’o_t$, forming
a better estimate $q(â†’z_t | â†’o_t, â†’z_t^\textrm{prior}, â†’z_{t-1}, a_{t-1}, â€¦, â†’z_1, a_1) + â†’z_t^\textrm{prior}$.

---
# MERLIN â€” Prior and Posterior

To achieve the mentioned goals, we add two terms to the loss.

- We try reconstructing input stimuli, action, reward and return using a sample from
  the state variable posterior, and add the difference of the reconstruction and
  ground truth to the loss.

~~~
- We also add KL divergence of the prior and the posterior to the loss, to ensure
  consistency between the prior and the posterior.

~~~
![w=85%,h=center](merlin_diagram.svgz)

---
# MERLIN â€” Algorithm

![w=100%](merlin_algorithm.svgz)

---
# MERLIN

![w=70%,h=center](merlin_tasks.svgz)

---
# MERLIN

![w=50%,h=center](merlin_analysis.svgz)

---
# MERLIN

![w=90%,h=center](merlin_predictive_model.svgz)

