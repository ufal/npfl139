title: NPFL139, Lecture 11
class: title, langtech, cc-by-sa
# UCB, Monte Carlo Tree Search, AlphaZero

## Milan Straka

### April 30, 2024

---
section: UCB
class: section
# Upper Confidence Bound

---
# Upper Confidence Bound

Revisiting multi-armed bandits with $Îµ$-greedy exploration, we note that using
the same epsilon for all actions in $Îµ$-greedy method seems inefficient.

~~~
One possible improvement is to select action according to _upper confidence bound_
(instead of choosing a random action with probability $Îµ$):
$$A_{t+1} â‰ \argmax_a \Bigg[\underbrace{\vphantom{\sqrt{\frac{\ln t}{N_t(a)}}}Q_t(a)}_\textrm{exploitation} + \underbrace{c\sqrt{\frac{\ln t}{N_t(a)}}}_\textrm{exploration}\Bigg],$$
where:
~~~
- $t$ is the number of times any action has been taken;
~~~
- $N_t(a)$ is the number of times the action $a$ has been taken before step $t$;
~~~
- if $N_t(a) = 0$, the right expression is frequently assumed to have a value of
  $âˆ$.

~~~
The updates are then performed as before (e.g., using averaging or fixed
learning rate $Î±$).

---
# Upper Confidence Bound

![w=100%](ucb_visualization.png)

---
# Upper Confidence Bound Derivation

We want to select the upper confidence bound, so that
- the probability of the real expected value being less or equal to the
  confidence bound gradually converges to 1;

~~~
- it is as small as possible.

~~~
Assuming that random variables $X_i$ are bounded by $[0, 1]$ and denoting
$XÌ„ = \frac{1}{N} âˆ‘_{i=1}^N X_i$, (Chernoff-)Hoeffding's inequality states that
$$P\big(XÌ„ â‰¤ ğ”¼[XÌ„] - Î´\big) â‰¤ e^{-2NÎ´^2},\textrm{~~which we can rearrange into~~}P\big(XÌ„ + Î´ â‰¤ ğ”¼[XÌ„]).$$

~~~
Our goal is to choose $Î´$ such that for every action,
$$P\big(Q_t(a) + Î´ â‰¤ q_*(a)\big) â‰¤ \left(\frac{1}{t}\right)^Î±.$$

~~~
The required inequality will hold if $e^{-2N_t(a)Î´^2} â‰¤ \left(\frac{1}{t}\right)^Î±$, yielding
$Î´ â‰¥ \sqrt{Î±/2} â‹… \sqrt{(\ln t)/N_t(a)}.$

---
# Asymptotic Optimality of UCB

We define **regret** as the difference of maximum of what we could get
(i.e., repeatedly using the action with maximum expectation) and
what a strategy yields, i.e.,
$$\textrm{regret}_N â‰ N \max_a q_*(a) - âˆ‘_{i=1}^N ğ”¼[R_i].$$

~~~
It can be shown that regret of UCB is asymptotically optimal,
see Lai and Robbins (1985), _Asymptotically Efficient Adaptive Allocation
Rules_; or the Chapter 8 of the 2018 _Bandit Algorithms Book_ available
online at https://banditalgs.com/.

---
# Upper Confidence Bound Multi-armed Bandits Results

![w=30%,v=middle](../01/k-armed_bandits.svgz)![w=70%,v=middle](ucb.svgz)

---
# Multi-armed Bandits Comparison

![w=90%,h=center,v=middle](../01/bandits_comparison.svgz)

---
section: MCTS
class: section
# Monte Carlo Tree Search

---
# Monte Carlo Tree Search

Assume we want to estimate $Q(s, â‹…)$ for a given state $s$ to decide how to act
in $s$.

~~~
We might perform a lot of simulations from the state $s$ and â€œpromisingâ€
actions. Many simulations will probably share prefixes of the trajectories;
therefore, we might represent these simulations using a _tree_.

~~~
![w=80%,h=center](mcts.svgz)

---
# Monte Carlo Tree Search â€“ Selection

When selecting a child to visit, we apply the **UCT** (Upper Confidence Bounds
applied for Trees) method, which chooses an action according to

$$a^* = \argmax_a \left[Q(s, a) + c\sqrt{\frac{\ln N(s)}{N(s, a)}}\right],$$

where:
~~~
- $Q(s, a)$ denotes the average result of playing action $a$ in state $s$ in the
  simulations so far,
~~~
- $N(s)$ is the number of times the state $s$ has been visited in previous
  iterations,
~~~
- $N(s, a)$ is the number of times the action $a$ has been sampled in state $s$.

~~~
The $Q(s, a)$ values are often normalized to the $[0, 1]$ range, and the constant
$c$ is game dependent, but some papers propose to try out $\sqrt 2$ first.

---
# Monte Carlo Tree Search

![w=35%,f=right](mcts_frequencies.png)

The MCTS is by nature asymmetrical, visiting promising actions more often.

~~~
## Expansion

Once a chosen action does not correspond to a node in the search tree,
a new leaf is added to the game tree.

~~~
## Simulation

The simulation phase is the â€œMonte Carloâ€ part of the algorithm, and is heavily
game-specific. It reaches a terminal state and produces final game outcome.

~~~
## Backpropagation

The obtained outcome updates the $Q(s, a)$ values in the nodes along the path
from the expanded node to the root of the search tree.

---
section: Alpha*
# Alpha* Gradual Development

- AlphaGo

  - Mar 2016 â€“ beat 9-dan professional player Lee Sedol

~~~
- AlphaGo Master â€“ Dec 2016
  - beat 60 professionals, beat Ke Jie in May 2017
~~~
- AlphaGo Zero â€“ 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero â€“ Dec 2017 (Dec 2018 in Nature)
  - self-play only, defeated AlphaGo Zero after 30 hours of training
  - impressive chess and shogi performance after 9h and 12h, respectively
~~~
- MuZero â€“ Dec 2020 in Nature
  - extends AlphaZero by a _trained model_
~~~
  - later extended into Sampled MuZero capable of handling continuous actions
~~~
- AlphaTensor â€“ Oct 2022 in Nature
  - automatic discovery of faster algorithms for matrix multiplication

---
section: AlphaZero
class: section
# AlphaZero

---
# AlphaZero

On 7 December 2018, the AlphaZero paper came out in Science journal. It
demonstrates learning chess, shogi and go, _tabula rasa_ â€“ without any
domain-specific human knowledge or data, only using self-play. The evaluation
is performed against strongest programs available.

![w=80%,h=center](a0_results.svgz)

---
# AlphaZero â€“ Overview

AlphaZero uses a neural network predicting $(â†’p(s), v(s)) â† f(s; â†’Î¸)$ for
a given state $s$, where:
- policy $â†’p(s)$ is a vector of move probabilities, and
~~~
- value function $v(s)$ is the expected outcome of the game in range $[-1, 1]$.

~~~
Instead of the usual alpha-beta search used by classical game playing programs,
AlphaZero uses Monte Carlo Tree Search (MCTS).

~~~
By a sequence of simulated self-play games, the search can improve the estimate
of $â†’p$ and $v$, and can be considered a powerful policy improvement operator
â€“ given a network $f$ predicting policy $â†’p$ and value estimate $v$, MCTS
produces a more accurate policy $â†’Ï€$ and better value estimate $w$ for a given
state $s$:
$$\begin{aligned}
  (â†’p(s), v(s)) &â† f(s; â†’Î¸), \\
  (â†’Ï€(s), w(s)) &â† \textrm{MCTS}(s, f)
\end{aligned}$$

---
# AlphaZero â€“ Overview

The network is trained from self-play games.

~~~
A game is played by repeatedly running MCTS from a state $s_t$ and choosing
a move $a_t âˆ¼ â†’Ï€_t$, until a terminal position $s_T$ is encountered, which is
then scored according to game rules as $zâˆˆ\{-1, 0, 1\}$. (Note that the range
$[0, 1]$ is also often used, for example in MuZero.)

~~~
Finally, the network parameters are trained to minimize the error between the
predicted outcome $v$ and the simulated outcome $z$, and maximize the similarity of
the policy vector $â†’p$ and the search probabilities $â†’Ï€$ (in other words, we
want to find a fixed point of the MCTS):
$$ğ“› â‰ (z - v)^2 - â†’Ï€^T \log â†’p + c\|â†’Î¸\|^2.$$

~~~
The loss is a combination of:
- a mean squared error for the value functions;
- a crossentropy for the action distribution;
- $L^2$-regularization.

---
section: A0-MCTS
class: section
# AlphaZero â€“ Monte Carlo Tree Search

---
# AlphaZero â€“ Monte Carlo Tree Search

MCTS keeps a tree of currently explored states from a fixed root state.
~~~
Each node corresponds to a game state and to every non-root node we got
by performing an action $a$ from the parent state.
~~~
Each state-action pair $(s, a)$ stores the following set of statistics:
- visit count $N(s, a)$,
~~~
- total action-value $W(s, a)$,
~~~
- mean action value $Q(s, a) â‰ W(s, a) / N(s, a)$, which is usually not stored
  explicitly,
~~~
- prior probability $P(s, a)$ of selecting the action $a$ in the state $s$.

~~~
![w=87%,h=center](ag0_mcts.svgz)

---
# AlphaZero â€“ Monte Carlo Tree Search

Each simulation starts in the root node and finishes in a leaf node $s_L$.
In a state $s_t$, an action is selected using a variant of PUCT algorithm as
$$a_t = \argmax\nolimits_a \big(Q(s_t, a) + U(s_t, a)\big),$$
where
~~~
$$U(s, a) â‰ C(s) P(s, a) \frac{\sqrt{N(s)}}{1 + N(s, a)},$$
~~~
with $C(s) = \log\left(\frac{1+N(s)+c_\textrm{base}}{c_\textrm{base}}\right) + c_\textrm{init}$
being slightly time-increasing exploration rate.

~~~
The paper uses $c_\textrm{init} = 1.25$, $c_\textrm{base} = 19652$ without
any (public) supporting experiments.

~~~
The reason for the modification of the UCB formula was never discussed in any
of the AlphaGo/AlphaZero/MuZero papers and is not obvious. However, in June 2020
a paper discussing the properties of this modified formula was published.

---
# AlphaZero â€“ Monte Carlo Tree Search

Additionally, exploration in the root state $s_\textrm{root}$ is supported by
including a random sample from Dirichlet distribution,
$$P(s_\textrm{root}, a) = (1-Îµ)p_a + Îµ\operatorname{Dir}(Î±),$$
with $Îµ=0.25$ and $Î±=0.3, 0.15, 0.03$ for chess, shogi and go, respectively.

~~~
![w=24%,f=right](dirichlet_03.png)

Note that using $Î±<1$ makes the Dirichlet noise non-uniform, with a smaller
number of actions with high probability.

~~~
The Dirichlet distribution can be seen as a limit of the PÃ³lyaâ€™s urn scheme,
where in each step we sample from a bowl of balls (with initial counts
$Î±$) and return an additional ball of the same color to the bowl.

~~~
```python
sorted(np.random.dirichlet([0.3] * 9), reverse=True) ->
0.496 0.139 0.124 0.113 0.066 0.035 0.027 0.000 0.000
0.324 0.250 0.151 0.133 0.068 0.044 0.024 0.005 0.001
0.526 0.222 0.111 0.061 0.035 0.026 0.014 0.005 0.000
```

---
# AlphaZero â€“ Monte Carlo Tree Search

![w=97%,h=center](ag0_mcts.svgz)

When reaching a leaf node $s_L$, we:
~~~
- evaluate it by the network, generating $(â†’p, v)$,
~~~
- add all its children with $N=W=0$ and the prior probability $â†’p$,
~~~
- in the backward pass for all $t â‰¤ L$, we update the statistics in nodes by performing
  - $N(s_t, a_t) â† N(s_t, a_t) + 1$, and
  - $W(s_t, a_t) â† W(s_t, a_t) Â± v$, depending on the player on turn.

---
# AlphaZero â€“ Monte Carlo Tree Search

The Monte Carlo Tree Search runs usually several hundreds simulations in
a single tree. The result is a distribution proportional to exponentiated visit
counts $N(s_\mathrm{root}, a)^\frac{1}{Ï„}$ using a temperature $Ï„$ ($Ï„=1$ is
mostly used), together with the predicted value function.

~~~
The next move is chosen as either:
- proportional to visit counts $N(s_\textrm{root}, â‹…)^\frac{1}{Ï„}$:
  $$â†’Ï€_\textrm{root}(a) âˆ N(s_\textrm{root}, a)^\frac{1}{Ï„},$$
~~~
- deterministically as the most visited action
  $$â†’Ï€_\textrm{root} = \argmax_a N(s_\textrm{root}, a).$$

~~~
During self-play, the stochastic policy is used for the first 30
moves of the game, while the deterministic is used for the rest of the moves.
(This does not affect the internal MCTS search, there we always sample
according to PUCT rule.)


---
# AlphaZero â€“ Monte Carlo Tree Search Example

![w=95%,mw=78%,h=center,f=right](a0_mcts_example.svgz)

Visualization of the 10 most visited states in a MCTS with a given number of
simulations. The displayed numbers are predicted value functions from the
white's perspective, scaled to $[0, 100]$ range. The border thickness is
proportional to a node visit count.

---
section: A0-Network
class: section
# AlphaZero â€“ Network Architecture

---
# AlphaZero â€“ Network Architecture

The network processes game-specific input, which consists of a history of
8 board positions encoded by several $N Ã— N$ planes, and some number of
constant-valued inputs.

~~~
Output is considered to be a categorical distribution of possible moves. For
chess and shogi, for each piece we consider all possible moves (56 queen
moves, 8 knight moves, and 9 underpromotions for chess).

~~~
The input is processed by:
- initial convolution block with CNN with 256 $3Ã—3$ kernels with stride 1, batch
  normalization, and ReLU activation,
~~~
- 19 residual blocks, each consisting of two CNN with 256 $3Ã—3$ kernels with stride 1,
  batch normalization, ReLU activation, and a residual connection around
  them,
~~~
- _policy head_, which applies another CNN with batch normalization, followed by
  a convolution with 73/139 filters for chess/shogi, or a linear layer of size
  362 for go,
~~~
- _value head_, which applies another CNN with one $1Ã—1$ kernel with stride 1,
  followed by a ReLU layer of size 256, and a final $\tanh$ layer of size 1.

---
# AlphaZero â€“ Network Inputs

![w=80%,h=center,v=middle](a0_input.svgz)

---
# AlphaZero â€“ Network Outputs

![w=80%,h=center,v=middle](a0_output.svgz)

---
section: A0-Training
class: section
# AlphaZero â€“ Training

---
# AlphaZero â€“ Training

Training is performed by running self-play games of the network with itself.
Each MCTS uses 800 simulations. A replay buffer of one million most recent games
is kept.

~~~
During training, 5000 first-generation TPUs are used to generate self-play games.
Simultaneously, network is trained using SGD with momentum of 0.9 on batches
of size 4096, utilizing 16 second-generation TPUs. Training takes approximately
9 hours for chess, 12 hours for shogi and 13 days for go.

---
# AlphaZero â€“ Training

![w=100%](a0_results_learning.svgz)

![w=85%,h=center](a0_training_stats.svgz)

---
# AlphaZero â€“ Training

According to the authors, training is highly repeatable.

![w=50%,h=center,mh=90%,v=middle](a0_repeatability.svgz)

---
# AlphaZero â€“ Symmetries

In the original AlphaGo Zero, symmetries (8 in total, using rotations and reflections) were explicitly utilized, by
- randomly sampling a symmetry during training,
~~~
- randomly sampling a symmetry during MCTS evaluation.

~~~
However, AlphaZero does not utilize symmetries in any way (because chess and
shogi do not have them), so it generates only approximately 1/8 positions per training step.

![w=100%,h=center](a0_symmetries.svgz)

---
section: A0-Evaluation
# AlphaZero â€“ Inference

During inference, AlphaZero utilizes much less evaluations than classical game
playing programs.

![w=80%,h=center,mh=80%,v=middle](a0_inference.svgz)

---
# AlphaZero â€“ Main Results

Evaluation against strongest program available.

![w=80%,h=center](a0_results.svgz)

---
# AlphaZero â€“ Ablations

![w=65%,h=center](a0_match_chess.svgz)
![w=65%,h=center](a0_match_shogi.svgz)

---
# AlphaZero â€“ Ablations

![w=80%,h=center](a0_results_ablations.svgz)

---
# AlphaZero â€“ Ablations

![w=100%](ag0_architectures_ablation.svgz)

---
# AlphaZero â€“ Preferred Chess Openings

![w=100%,v=middle](a0_openings.svgz)

---
section: A0-PT
class: section
# AlphaZero as Regularized Policy Optimization

---
# AlphaZero as Regularized Policy Optimization

Recall that in AlphaZero, actions are selected according to a variant of PUCT
algorithm:
$$a^* = \argmax\nolimits_a \bigg(Q(s, a) + C(s) P(s, a) \frac{\sqrt{N(s)}}{1 + N(s, a)}\bigg),$$
with a slightly time-increasing exploration rate
$C(s) = \log\left(\frac{1+N(s)+19625}{19625}\right) + 1.25 â‰ˆ 1.25$.

~~~
The paper _Jean-Bastien Grill et al.: Monte-Carlo Tree Search as Regularized
Policy Optimization_, the authors have shown how to interpret this algorithm
as a regularized policy optimization.


---
# AlphaZero as Regularized Policy Optimization

Policy optimization is usually an iterative procedure, which in every step
improves a current policy $Ï€_{â†’Î¸_0}$ according to
$$Ï€_{â†’Î¸'} â‰ \argmax_{â†’y âˆˆ ğ“¢} â†’q_{Ï€_{â†’Î¸_0}}^T â†’y - ğ“¡(â†’y, Ï€_{â†’Î¸_0}),$$
where $ğ“¢$ is a $|ğ“|$-dimensional simplex and $ğ“¡: ğ“¢^2 â†’ â„$ is an optional
(usually convex) regularization term.

~~~
- with $ğ“¡ = 0$, the above reduces to policy iteration (used for example in DQN);
~~~
- with $ğ“¡ = 0$, if the policy is updated using a single gradient step, the
  algorithm reduces to policy gradient;
~~~
- when $ğ“¡(â†’y, Ï€_{â†’Î¸_0}) = -H(â†’y)$, we recover the Soft Actor Critic objective;
~~~
- for $ğ“¡(â†’y, Ï€_{â†’Î¸_0}) = D_\textrm{KL}(Ï€_{â†’Î¸_0} \| â†’y)$ we get an analogue of
  the TRPO objective, which motivated PPO;
~~~
- the MPO algorithm (which we did not discuss) employs $ğ“¡(â†’y, Ï€_{â†’Î¸_0}) = D_\textrm{KL}(â†’y \| Ï€_{â†’Î¸_0})$.

---
# AlphaZero as Regularized Policy Optimization

Let us define the **empirical visit distribution** $Ï€Ì‚$ as
$$Ï€Ì‚(a|s) â‰ \frac{1 + N(s, a)}{|ğ“| + âˆ‘_b N(s, b)}.$$

~~~
The added plus ones makes the following analysis easier, but are not strictly
necessary.

~~~
We also define the **multiplier** $Î»_N$ as
$$Î»_N(s) â‰ C(s) â‹… \frac{\sqrt{âˆ‘_b N(s, b)}}{|ğ“| + âˆ‘_b N(s, b)}.$$

~~~
With these definitions, we can rewrite the AlphaZero action selection to
$$a^* = \argmax\nolimits_a \bigg(Q(s, a) + Î»_N â‹… \frac{Ï€_{â†’Î¸}(a|s)}{Ï€Ì‚(a|s)}\bigg).$$

---
# AlphaZero as Regularized Policy Optimization

$$a^* = \argmax\nolimits_a \bigg(Q(s, a) + Î»_N â‹… \frac{Ï€_{â†’Î¸}(a|s)}{Ï€Ì‚(a|s)}\bigg)$$

For notational simplicity, we will represent $Q(s, a)$ as a vector $â†’q$, where
$q_a = Q(s, a)$, and similarly the policies as $â†’Ï€_{â†’Î¸}$, $â†’Ï€Ì‚$.

~~~
Furthermore, for two vectors $â†’a, â†’b$, let $\frac{â†’a}{â†’b}$ denote element-wise
division with $(\frac{â†’a}{â†’b})_i â‰ \frac{a_i}{b_i}.$ 

~~~
With this notation, the action selection can be succinctly written as
$$a^* = \argmax\nolimits_a \Big(â†’q + Î»_N \frac{â†’Ï€_{â†’Î¸}}{â†’Ï€Ì‚}\Big).$$

---
# AlphaZero as Regularized Policy Optimization

Let $â†’Ï€Ì„$ be the solution of the following objective:
$$â†’Ï€Ì„ â‰ \argmax\nolimits_{â†’y âˆˆ ğ“¢} \Big(â†’q^T â†’y - Î»_N D_\textrm{KL}(â†’Ï€_{â†’Î¸} \| â†’y)\Big).$$

~~~
It can be shown that this solution can be computed explicitly as
$$â†’Ï€Ì„ = Î»_N \frac{â†’Ï€_{â†’Î¸}}{Î± - â†’q},$$
where $Î± âˆˆ â„$ is set such that the result is a proper distribution.
~~~
- Note that $Î± â‰¥ \max_{bâˆˆğ“} \big(q_b + Î»_N Ï€_{â†’Î¸}(b)\big)$, because $Ï€Ì„(a)$ must
  be at most 1.
~~~
- Furthermore, $Î± â‰¤ \max_{bâˆˆğ“} (q_b) + Î»_N$, because we need $âˆ‘_a Ï€Ì„(a) = 1$
  and we combine $âˆ‘_a \frac{Î»_N Ï€_{â†’Î¸}(a)}{\max_b (q_b) + Î»_N - q_a} â‰¤ âˆ‘_a \frac{Î»_N Ï€_{â†’Î¸}(a)}{Î»_N} = 1$
  with the fact that $âˆ‘_a \frac{Î»_N Ï€_{â†’Î¸}(a)}{Î± - q_a}$ is a decreasing
  function of $Î± â‰¥ \max_b q_b$.

~~~
Note the $Î»_N â‰ˆ 1/\sqrt N$ decreasing the regularization for increasing number
of simulations.

---
# AlphaZero as Regularized Policy Optimization

In the paper, it is proven that the action $a^*$ selected by the AlphaZero algorithm fulfills
$$a^* = \argmax\nolimits_a \bigg(\frac{âˆ‚}{âˆ‚ N(s, a)} \Big(â†’q^T â†’Ï€Ì‚ - Î»_N D_\textrm{KL}(â†’Ï€_{â†’Î¸} \| â†’Ï€Ì‚)\Big)\bigg).$$

~~~
In other words, $Ï€Ì‚$ â€œtracksâ€ $Ï€Ì„$.

~~~
Furthermore, it can be also shown that for the selected action $a^*$,
$$Ï€Ì‚(a^*|s) â‰¤ Ï€Ì„(a^* | s),$$
until in the limit, the two distributions coincide.

---
# AlphaZero as Regularized Policy Optimization

The $Ï€Ì„$ can be used in the AlphaZero algorithm in several ways:

- **Act**: the action in self-play games could be sampled according to
  $Ï€Ì„(â‹…|s_\textrm{root})$ instead of $Ï€Ì‚$;

~~~
- **Search**: during search, we could sample the actions stochastically
  according to $Ï€Ì„$ instead of the PUCT rule;
~~~
- **Learn**: we could use $Ï€Ì„$ as the target policy during training instead
  of $Ï€Ì‚$;
~~~
- **All**: all of the above.

---
# AlphaZero as Regularized Policy Optimization

![w=50%](muzero_regpolopt_cheetah.svgz)![w=50%](muzero_regpolopt_cheetah_curves.svgz)

---
# AlphaZero as Regularized Policy Optimization

![w=53%](muzero_regpolopt_mcpacman.svgz)![w=46%](muzero_regpolopt_atari.svgz)

---
# AlphaZero as Regularized Policy Optimization

![w=100%,v=middle](muzero_regpolopt_components.svgz)

