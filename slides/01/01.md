title: NPFL139, Lecture 1
class: title, langtech, cc-by-sa
# Introduction to Reinforcement Learning

## Milan Straka

### February 19, 2025

---
# Reinforcement Learning

**Reinforcement learning** is a machine learning paradigm, different from
_supervised_ and _unsupervised learning_.

~~~
The essence of reinforcement learning is to learn from _interactions_ with the
environment to maximize a numeric _reward_ signal.
~~~
The learner is not told which actions to take, and the actions may affect not
just the immediate reward, but also all following rewards.

~~~
![w=50%,h=center](robots.png)

---
# Deep Reinforcement Learning

In the last decade, reinforcement learning has been successfully combined with
_deep NNs_.

~~~
![w=30%](atari_games.png)
~~~
![w=37%](a0_results.svgz)
~~~
![w=31%](alphastar.png)

~~~
![w=78%,mw=17%,h=center,v=top,mh=50%](robot_navigation.jpg)
~~~
![w=97%,mw=17%,h=center,v=top,mh=50%](data_center.jpg)
~~~
![w=95%,mw=21%,h=center,v=top,mh=50%](tpu_trillium.jpg)
~~~
![w=82.5%,mw=17%,h=center,v=top,mh=50%](nas_block.svgz)
~~~
![w=24%,v=top,mh=50%](muzero_rc.png)

---
section: Organization
class: section
# Organization

---
# Organization

![w=37%,f=right](screenshot_page.png)

**Course Website:** https://ufal.mff.cuni.cz/courses/npfl139
~~~
  - Slides, recordings, assignments, exam questions
~~~

**Course Repository:** https://github.com/ufal/npfl139
- Templates for the assignments, slide sources.

~~~

![w=37%,f=right](screenshot_piazza.png)

## Piazza

- Piazza will be used as a communication platform.

  You can post questions or notes,
  - **privately** to the instructors,
~~~
  - **publicly** to everyone (signed or anonymously).
~~~
    - Other students can answer these too, which allows you to get faster
      response.
~~~
    - However, **do not include even parts of your source code** in public
      questions.
~~~

- Please use Piazza for **all communication** with the instructors.
~~~
- You will get the invite link after the first lecture.

---
# ReCodEx

https://recodex.mff.cuni.cz

- The assignments will be evaluated automatically in ReCodEx.
~~~
- If you have a MFF SIS account, you should be able to create an account
  using your CAS credentials and should automatically see the right group.
~~~
- Otherwise, there will be **instructions** on **Piazza** how to get
  ReCodEx account (generally you will need to send me a message with several
  pieces of information and I will send it to ReCodEx administrators in
  batches).

---
# Course Requirements

## Practicals
~~~

- There will be about 2-3 assignments a week, each with a 2-week deadline.
~~~
  - There is also another week-long second deadline, but for fewer points.
~~~
- After solving the assignment, you get non-bonus points, and sometimes also
  bonus points.
~~~
- To pass the practicals, you need to get **80 non-bonus points**. There will be
  assignments for at least 120 non-bonus points.
~~~
- If you get more than 80 points (be it bonus or non-bonus), they will be
  all transferred to the exam. Additionally, if you solve **all the
  assignments**, you pass the exam with grade 1.

~~~
## Lecture

You need to pass a written exam (or solve all the assignments).
~~~
- All questions are publicly listed on the course website.
~~~
- There are questions for 100 points in every exam, plus the surplus
  points from the practicals and plus at most 10 surplus points for **community
  work** (improving slides, ‚Ä¶).
~~~
- You need 60/75/90 points to pass with grade 3/2/1.

---
# Organization

- Both the lectures and the practicals are recorded.

~~~

## Consultations

- Regular consultations are part of the course schedule.

  - Wednesday, 14:00, S9
~~~
  - However, the consultations are **completely voluntary**.

~~~
- The consultations take place on the last day of assignment deadlines.

~~~
- The consultations are not recorded and have no predefined content.

~~~
- The consultations start on the **second week** of the semester.

---
section: History
class: section
# History of Reinforcement Learning

---
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s ‚Äì Richard Bellman

~~~
- Trial and error learning ‚Äì since 1850s
  - Law and effect ‚Äì Edward Thorndike, 1911
    - Responses that produce a satisfying effect in a particular situation become
      more likely to occur again in that situation, and responses that produce
      a discomforting effect become less likely to occur again in that situation
  - Shannon, Minsky, Clark&Farley, ‚Ä¶ ‚Äì 1950s and 1960s
  - Tsetlin, Holland, Klopf ‚Äì 1970s
  - Sutton, Barto ‚Äì since 1980s

~~~
- Arthur Samuel ‚Äì first implementation of temporal difference methods
  for playing checkers

~~~
## Notable successes
- Gerry Tesauro ‚Äì 1992, human-level Backgammon program trained solely by self-play

~~~
- IBM Watson in Jeopardy ‚Äì 2011

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì Atari Games

- Human-level video game playing (DQN) ‚Äì 2013 (2015 Nature), Mnih. et al, Deepmind

  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games

~~~
- A3C ‚Äì 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games

~~~
- Rainbow ‚Äì 2017
  - human-normalized median: 153%; ~39 days of game play experience

~~~
- Impala ‚Äì Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

~~~
- PopArt-Impala ‚Äì Sep 2018
  - human-normalized median: 110.7% on 57 games; 57*38.6 days of experience

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì Atari Games

![w=22%,f=right](r2d2_results.svgz)

- R2D2 ‚Äì Jan 2019

  - human-normalized mean: 4024.9%, median: 1920.6% on 57 games
  - processes ~5.7B frames during a day of training
~~~
- Agent57 - Mar 2020
  - super-human performance on all 57 Atari games
~~~
- Data-efficient approaches learning from ~2 hours of game experience

![w=85%,mw=25%,f=left](der-progress.svgz)

  - SimPLE ‚Äì Mar 2019, 14.4% median
~~~
  - data-efficient Rainbox ‚Äì Jun 2019, 20.4% median
~~~
  - EfficientMuZero ‚Äì Nov 2021, 109% median

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì Board Games

- AlphaGo

  - Mar 2016 ‚Äì beat 9-dan professional player Lee Sedol

~~~
- AlphaGo Master ‚Äì Dec 2016
  - beat 60 professionals, beat Ke Jie in May 2017
~~~
- AlphaGo Zero ‚Äì 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero ‚Äì Dec 2017 (Dec 2018 in Nature)
  - self-play only, defeated AlphaGo Zero after 30 hours of training
  - impressive chess and shogi performance after 9h and 12h, respectively
![w=24%,h=center](a0_results.svgz)

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì 3D Games

- Dota2 ‚Äì Aug 2017

  - OpenAI bot won Dota2 1v1 matches against a professional player

~~~
- MERLIN ‚Äì Mar 2018
  - unsupervised representation of states using external memory
  - beat human in unknown maze navigation

~~~
- FTW ‚Äì Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - solely by self-play, trained on 450k games

~~~
- OpenAI Five ‚Äì Aug 2018
  - won Dota2 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs, 180 years of experience per day

~~~
- AlphaStar
  - Jan 2019: won 10 out of 11 StarCraft II games against two professional players
  - Oct 2019: ranked 99.8% on `Battle.net`, playing with full game rules

---
# History of Deep Reinforcement Learning
## Deep Reinforcement Learning ‚Äì Other Applications

- Optimize non-differentiable loss

  - improved translation quality in 2016
  - better summarization performance

~~~
- Neural architecture search (since Nov 2016)
  - SoTA CNN architecture generated by another network
  - can search also for suitable RL architectures, new activation functions,
    optimizers‚Ä¶

~~~
- Controlling cooling in Google datacenters directly by AI (2018)
  - reaching 30% cost reduction

~~~
- Designing the layout of TPU chips (AlphaChip; since 2021, opensourced)

~~~
- Improving efficiency of VP9 codec (2022; 4% in bandwith with no loss in
  quality)

~~~
- Discovering faster algorithms for matrix multiplication (AlphaTensor, Oct 2022),
  sorting (AlphaDev, June 2023)

~~~
- Searching for solutions of mathematical problems (FunSearch, Dec 2023)

---
# History of Deep Reinforcement Learning

- Reinforcement learning from human feedback (RLHF) is used to train chatbots

![w=65%,h=center](rlhf_overview.svgz)

~~~
- Improving reasoning of LLMs (DeepSeek R1), proving math theorems (AlphaGeometry 2)

---
# History of Deep Reinforcement Learning

Note that the machines learn just to obtain a reward we have defined,
they do not learn what we want them to.

- [Hide and seek](https://openai.com/blog/emergent-tool-use/#surprisingbehaviors)

~~~
![w=49%,mh=70%,v=bottom](driving.gif)
~~~
![w=49%,mh=70%,v=bottom](human_evaluation.gif)

---
section: Bandits
class: section
# Multi-armed Bandits

---
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.svgz)

---
# Multi-armed Bandits

We start by selecting an action $A_1$ (the index of the arm to use), and we
obtain a reward $R_1$. We then repeat the process by selecting an action $A_2$,
obtaining $R_2$, selecting $A_3$, ‚Ä¶, with the indices denoting the time step
when the actions and rewards occurred.

~~~
Let $q_*(a)$ be the real **value** of an action $a$:
$$q_*(a) = ùîº[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) ‚âù \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a **greedy** action $A_t$ as
$$A_t ‚âù \argmax_a Q_t(a).$$

---
section: $Œµ$-greedy
# $Œµ$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is **exploitation** of current estimates. We however also
need to **explore** the space of actions to improve our estimates.

~~~

An _$Œµ$-greedy_ method follows the greedy action with probability $1-Œµ$, and
chooses a uniformly random action with probability $Œµ$.

---
# $Œµ$-greedy Method

![w=87%,mw=60%,h=center,f=right](e_greedy.svgz)

Considering the 10-armed bandit problem:

- we generate 2000 random instances

  - each $q_*(a)$ is sampled from $ùìù(0, 1)$

~~~
- for every instance, we run 1000 steps of the $Œµ$-greedy method
  - we consider $Œµ$ of 0, 0.01, 0.1

~~~
- we plot the averaged results over the 2000 instances

---
# $Œµ$-greedy Method

## Incremental Implementation

Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

~~~
$\displaystyle\kern14em{}\mathllap{Q_{n+1}} = \frac{1}{n} ‚àë_{i=1}^n R_i$

~~~
$\displaystyle\kern14em{} = \frac{1}{n} \Bigg(‚àë_{i=1}^{n-1} R_i + R_n\Bigg)$

~~~
$\displaystyle\kern14em{} = \frac{1}{n} \Bigg(\frac{n-1}{\textcolor{blue}{n-1}} \textcolor{blue}{‚àë_{i=1}^{n-1} R_i} + R_n\Bigg)$

~~~
$\displaystyle\kern14em{} = \frac{n-1}{n} \textcolor{blue}{Q_n} + \frac{1}{n}R_n$

~~~
$\displaystyle\kern14em{} = Q_n + \frac{1}{n}\Big(R_n - Q_n\Big)$

---
# $Œµ$-greedy Method Algorithm

![w=100%,v=middle](bandits_algorithm.svgz)

---
# Fixed Learning Rate

Analogously to the solution obtained for a stationary problem, we consider
$$Q_{n+1} = Q_n + Œ±(R_n - Q_n).$$

~~~
Converges to the true action values if
$$‚àë_{n=1}^‚àû Œ±_n = ‚àû \textrm{~~~~and~~~~}‚àë_{n=1}^‚àû Œ±_n^2 < ‚àû.$$

~~~
Biased method, because
$$Q_{n+1} = (1 - Œ±)^n Q_1 + ‚àë_{i=1}^n Œ±(1-Œ±)^{n-i} R_i.$$

~~~
The bias can be utilized to support exploration at the start of the episode by
setting the initial values to more than the expected value of the optimal
solution.

---
# Optimistic Initial Values and Fixed Learning Rate

![w=85%,h=center,v=middle](optimistic_values.svgz)

---
# Method Comparison

![w=85%,h=center,v=middle](bandits_comparison.svgz)

---
section: MDP
class: section
# Markov Decision Process

---
# Markov Decision Process

![w=85%,h=center,v=middle](mdp.svgz)

~~~~
# Markov Decision Process

![w=47%,h=center](mdp.svgz)

A **Markov decision process** (MDP) is a quadruple $(ùì¢, ùìê, p, Œ≥)$,
where:
- $ùì¢$ is a set of states,
~~~
- $ùìê$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ‚àà ùìê$ will lead from state $s ‚àà ùì¢$ to $s' ‚àà ùì¢$, producing a **reward** $r ‚àà ‚Ñù$,
~~~
- $Œ≥ ‚àà [0, 1]$ is a **discount factor**.

~~~
Let a **return** $G_t$ be $G_t ‚âù ‚àë_{k=0}^‚àû Œ≥^k R_{t + 1 + k}$. The goal is to optimize $ùîº[G_0]$.

---
# Markov Decision Process

We cannot replace
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ‚àà ùìê$ will lead from state $s ‚àà ùì¢$ to $s' ‚àà ùì¢$, producing a **reward** $r ‚àà ‚Ñù$,

by
- $p(S_{t+1} = s' | S_t = s, A_t = a)$, a transition probability,
- $r(R_{t+1} = r | S_t = s, A_t = a)$, a reward probability.

~~~
because the reward might depend on $S_{t+1}$.

~~~
However, we could use
- $p(S_{t+1} = s' | S_t = s, A_t = a)$, a transition probability,
- $r(R_{t+1} = r | S_{t+1} = s', S_t = s, A_t = a)$, a reward probability.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $ùì¢=\{S\}$;
~~~
- an action for every arm, $ùìê=\{a_1, a_2, ‚Ä¶, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $ùìù(Œº_i, œÉ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = ùìù(r | Œº_i, œÉ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to
consider a separate reward distribution for every state. Such generalization is
called **Contextualized Bandits** problem. Assuming state transitions are
independent on rewards and given by a distribution $\textit{next}(s)$, the MDP
dynamics function for contextualized bandits problem is given by
$$p(s', r | s, a_i) = ùìù(r | Œº_{i,s}, œÉ_{i,s}^2) ‚ãÖ \textit{next}(s'|s).$$

---
section: POMDP
# Partially Observable MDPs

Recall that the Markov decision process is a quadruple $(ùì¢, ùìê, p, Œ≥)$,
where:
- $ùì¢$ is a set of states,
- $ùìê$ is a set of actions,
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ‚àà ùìê$ will lead from state $s ‚àà ùì¢$ to $s' ‚àà ùì¢$, producing a reward $r ‚àà ‚Ñù$,
- $Œ≥ ‚àà [0, 1]$ is a discount factor.

~~~
![w=46%,f=right](pomdp.svgz)

**Partially observable Markov decision process** extends the Markov decision
process to a sextuple $(ùì¢, ùìê, p, Œ≥, ùìû, o)$, where in addition to an MDP,
- $ùìû$ is a set of observations,
- $o(O_{t+1} | S_{t+1}, A_t)$ is an observation model, where observation $O_t$ is used as agent input
  instead of the state $S_t$.

---
section: POMDP
# Partially Observable MDPs

Planning in a general POMDP is in theory undecidable.
~~~
- Nevertheless, several approaches are used to handle POMDPs in robotics
  - to model uncertainty, imprecise mechanisms and inaccurate sensors, ‚Ä¶
  - consider for example robotic vacuum cleaners

~~~

Partially observable MDPs are needed to model many environments
(maze navigation, FPS games, ‚Ä¶).
~~~
- We will initially assume all environments are fully observable, even if some
  of them will not.
~~~
- Later we will mention solutions, where partially observable MDPs are handled
  using recurrent networks (or networks with external memory), which model the
  latent states $S_t$.

---
section: MDP
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizon tasks** then can use discount factor $Œ≥=1$,
because the return $G ‚âù ‚àë_{t=0}^H Œ≥^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $Œ≥$ needs
to be sharply smaller than 1.

---
section: Value Functions
class: section
# Value Function and Action-Value Function

---
# (State-)Value Function and Action-Value Function

A **policy** $œÄ$ computes a distribution of actions in a given state, i.e.,
$œÄ(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_œÄ(s)$, or
**state-value function**, as
$$\begin{aligned}
  v_œÄ(s) & ‚âù ùîº_œÄ\big[G_t \big| S_t = s\big] = ùîº_œÄ\left[‚àë\nolimits_{k=0}^‚àû Œ≥^k R_{t+k+1} \middle| S_t=s\right] \\
         & = ùîº_{A_t ‚àº œÄ(s)} ùîº_{S_{t+1},R_{t+1} ‚àº p(s,A_t)} \big[R_{t+1}
           + Œ≥ ùîº_{A_{t+1} ‚àº œÄ(S_{t+1})} ùîº_{S_{t+2},R_{t+2} ‚àº p(S_{t+1},A_{t+1})} \big[R_{t+2} + ‚Ä¶ \big]\big]
\end{aligned}$$

~~~
An **action-value function** for a policy $œÄ$ is defined analogously as
$$q_œÄ(s, a) ‚âù ùîº_œÄ\big[G_t \big| S_t = s, A_t = a\big] = ùîº_œÄ\left[‚àë\nolimits_{k=0}^‚àû Œ≥^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
The value function and action-value function can be of course expressed using one another:
$$v_œÄ(s) = ùîº_{a‚àºœÄ}\big[q_œÄ(s, a)\big],~~~~~~~q_œÄ(s, a) = ùîº_{s', r ‚àº p}\big[r + Œ≥v_œÄ(s')\big].$$

---
# (State-)Value Function and Action-Value Function

Consider the following simple gridworld environment:
- the cells corresponding to states
- four actions _north_, _east_, _south_, _west_ available from all states
- reward -1 when action would take an agent off the grid (no movement in that
  case)
- reward +10 and +5 in the two special cells below, with the next state
  always the indicated one independently on the action; reward 0 otherwise
- discount $Œ≥ = 0.9$

![w=88%,h=center](gridworld_uniform_policy.svgz)

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) ‚âù \max_œÄ v_œÄ(s),$$
~~~
analogously
$$q_*(s, a) ‚âù \max_œÄ q_œÄ(s, a).$$

~~~
Any policy $œÄ_*$ with $v_{œÄ_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $œÄ_*(s) ‚âù \argmax_a q_*(s, a) = \argmax_a ùîº\big[R_{t+1} + Œ≥v_*(S_{t+1}) | S_t = s, A_t = a\big]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizon tasks or if $Œ≥ < 1$, there always exists a unique optimal
state-value function, a¬†unique optimal action-value function, and a (not necessarily
unique) optimal policy.

---
# Optimal Value Functions

Here are the unique optimal value function and non-unique optimal policy
of the gridworld environment from before.

![w=80%,h=center,v=middle,mh=90%](gridworld_optimal_policy.svgz)

---
section: Monte Carlo Methods
class: section
# Monte Carlo Methods

---
# Monte Carlo Prediction

Assuming we have a fixed policy $œÄ$ and that we want to estimate $v_œÄ(s)$.

~~~
A Monte Carlo method to estimate this value function would be to simulate
many episodes, and then compute an average _return_ for all visited states:
$$V(s) ‚âà ùîº[G_t | S_t = s].$$

~~~
Some states might be visited multiple times; in that case, we could use
~~~
- **first-visit** Monte Carlo method, where only the first occurrence of
  the state is considered;
~~~
- **every-visit** Monte Carlo method, where all occurrences of the state are
  considered.

~~~
By the law of large numbers, the Monte Carlo estimate converges to the
real value function for all visited states.
~~~
- Actually, for every-visit MC, it is more complicated, because multiple returns
  from a single state in a single episode are not independent; but it can be
  proven that even every-visit Monte Carlo converges.

---
# Monte Carlo Prediction of Blackjack

![w=82%,h=center](blackjack_estimation.svgz)

---
# Monte Carlo Control

We now present the first algorithm for computing optimal behavior without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $ùì¢$ and we will store
estimates for each of them (a _tabular_ method).

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Specifically, they try to estimate
$$Q(s, a) ‚âà ùîº[G_t | S_t = s, A_t = a].$$

~~~
With such estimates, a greedy action in state $S_t$ can be computed as
$$A_t = \argmax_a Q(S_t, a).$$

~~~
To hope for convergence, we need to visit each state-action pair infinitely many times.
One of the simplest way to achieve that is to assume **exploring starts**, where
we randomly select the first state and first action, and behave greedily
afterwards.

---
# Monte Carlo with Exploring Starts

![w=90%,h=center](monte_carlo_exploring_starts.svgz)

---
# Monte Carlo Predicting Optimal Policy on Blackjack

![w=81%,h=center](blackjack_optimal.svgz)

---
# Monte Carlo and $Œµ$-soft Behavior

The problem with exploring starts is that in many situations, we either cannot
start in an arbitrary state, or it is impractical.

~~~
Instead of choosing random state at the beginning, we can consider adding
‚Äúrandomness‚Äù gradually ‚Äì for a given $Œµ$, we set the probability of choosing any
action to be at least
$$\frac{Œµ}{|ùìê(s)|}$$
in each step. Such behavior is called _$Œµ$-soft_.

~~~
In an $Œµ$-soft behaviour, selecting and action greedily (the $Œµ$-greedy
behavior) means one action has a maximum probability of
$$1-Œµ+\frac{Œµ}{|A(s)|}.$$

~~~
We now present Monte Carlo algorithm with $Œµ$-greedy action selection.

---
# Monte Carlo for $Œµ$-soft Behavior

### On-policy every-visit Monte Carlo for $Œµ$-soft Policies
Algorithm parameter: small $Œµ>0$

Initialize $Q(s, a) ‚àà ‚Ñù$ arbitrarily (usually to 0), for all $s ‚àà ùì¢, a ‚àà ùìê$<br>
Initialize $C(s, a) ‚àà ‚Ñ§$ to 0, for all $s ‚àà ùì¢, a ‚àà ùìê$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, ‚Ä¶, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $Œµ$, generate a random uniform action
  - Otherwise, set $A_t ‚âù \argmax\nolimits_a Q(S_t, a)$
- $G ‚Üê 0$
- For each $t=T-1, T-2, ‚Ä¶, 0$:
  - $G ‚Üê Œ≥G + R_{t+1}$
  - $C(S_t, A_t) ‚Üê C(S_t, A_t) + 1$
  - $Q(S_t, A_t) ‚Üê Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$
