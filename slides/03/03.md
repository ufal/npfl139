title: NPFL139, Lecture 3
class: title, langtech, cc-by-sa
# Off-Policy Methods, N-step, Function Approximation, DQN

## Milan Straka

### March 4, 2024

---
section: Refresh
# Sarsa

A straightforward application to the temporal-difference policy evaluation
is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$
computes
$$q(S_t, A_t) â† q(S_t, A_t) + Î±\big(R_{t+1} + [Â¬\textrm{done}]â‹…Î³ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\big).$$

~~~
![w=75%,h=center](../02/sarsa.svgz)

---
# Sarsa

![w=65%,h=center](../02/sarsa_example.svgz)

~~~
MC methods cannot be easily used, because an episode might not terminate if
the current policy causes the agent to stay in the same state.

---
# Q-learning

Q-learning was an important early breakthrough in reinforcement learning (Watkins, 1989).

$$q(S_t, A_t) â† q(S_t, A_t) + Î±\Big(R_{t+1} +  [Â¬\textrm{done}]â‹…Î³ \max_a q(S_{t+1}, a) -q(S_t, A_t)\Big).$$

~~~
![w=80%,h=center](../02/q_learning.svgz)

---
# Q-learning versus Sarsa

![w=100%,h=center](../02/cliff_walking.svgz)

~~~ ~
# Q-learning versus Sarsa
![w=43%,h=center](../02/cliff_walking.svgz)
![w=45%,h=center](../02/cliff_walking_learning.svgz)

---
section: Off-policy
# On-policy and Off-policy Methods

So far, most methods were **on-policy**. The same policy was used both for
generating episodes and as a target of value function.

~~~
However, while the policy for generating episodes needs to be more exploratory,
the target policy should capture optimal behaviour.

~~~
Generally, we can consider two policies:
- **behaviour** policy, usually $b$, is used to generate behaviour and can be more
  exploratory;
~~~
- **target** policy, usually $Ï€$, is the policy being learned (ideally the optimal
  one).

~~~
When the behaviour and target policies differ, we talk about **off-policy**
learning.

---
# On-policy and Off-policy Methods

The off-policy methods are usually more complicated and slower to converge, but
are able to process data generated by different policy than the target one.

~~~
The advantages are:
- can compute optimal non-stochastic (non-exploratory) policies;

~~~
- more exploratory behaviour;

~~~
- ability to process _expert trajectories_.

---
# Off-policy Prediction

Consider prediction problem for off-policy case.

~~~
In order to use episodes from $b$ to estimate values for $Ï€$, we require that
every action taken by $Ï€$ is also taken by $b$, i.e.,
$$Ï€(a|s) > 0 â‡’ b(a|s) > 0.$$

~~~
Many off-policy methods utilize **importance sampling**, a general technique for
estimating expected values of one distribution given samples from another
distribution.

---
# Importance Sampling

Assume that $p$ and $q$ are two distributions and let $x_i$ be $N$ samples of $p$.
We can then estimate $ğ”¼_{â‡xâˆ¼p}\big[f(x)\big]$ as $\frac{1}{N} âˆ‘_i f(x_i)$.

~~~
In order to estimate $ğ”¼_{â‡xâˆ¼q}\big[f(x)\big]$ using the samples $x_i$, we need to account
for different probabilities of $x_i$ under the two distributions. It is
straightforward to verify that
$$ğ”¼_{â‡xâˆ¼q}\big[f(x)\big] = ğ”¼_{â‡xâˆ¼p}\left[\frac{q(x)}{p(x)}f(x)\right].$$

~~~
Therefore, we can estimate
$$ğ”¼_{â‡xâˆ¼q}\big[f(x)\big] â‰ˆ \frac{1}{N} âˆ‘_i \frac{q(x_i)}{p(x_i)} f(x_i),$$
with $q(x)/p(x)$ being the **relative probability** of $x$ under the two
distributions.

~~~
Both estimates mentioned on this slide are _unbiased_.

---
# Off-policy Prediction

Given an initial state $S_t$ and an episode $A_t, S_{t+1}, A_{t+1}, â€¦, S_T$,
the probability of this episode under a policy $Ï€$ is
$$âˆ_{k=t}^{T-1} Ï€(A_k | S_k) p(S_{k+1} | S_k, A_k).$$

~~~
Therefore, the relative probability of a trajectory under the target and
behaviour policies is
$$Ï_t â‰ \frac{âˆ_{k=t}^{T-1} Ï€(A_k | S_k) p(S_{k+1} | S_k, A_k)}{âˆ_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)}
      = âˆ_{k=t}^{T-1} \frac{Ï€(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
The $Ï_t$ is usually called the **importance sampling ratio** or _relative
probability_.

~~~
Therefore, if $G_t$ is a return of episode generated according to $b$, we can
estimate
$$v_Ï€(S_t) = ğ”¼_b[Ï_t G_t].$$

---
# Off-policy Monte Carlo Prediction

Let $ğ“£(s)$ be a set of times when we visited state $s$. Given episodes sampled
according to $b$, we can estimate
$$v_Ï€(s) = \frac{âˆ‘_{tâˆˆğ“£(s)} Ï_t G_t}{|ğ“£(s)|}.$$

~~~
Such simple average is called **ordinary importance sampling**. It is unbiased, but
can have very high variance.

~~~
An alternative is **weighted importance sampling**, where we compute weighted
average as
$$v_Ï€(s) = \frac{âˆ‘_{tâˆˆğ“£(s)} Ï_t G_t}{âˆ‘_{tâˆˆğ“£(s)} Ï_t}.$$

~~~
Weighted importance sampling is biased (with bias asymptotically converging to
zero, i.e., a _consistent_ estimate), but has smaller variance.

---
# Off-policy Multi-armed Bandits

![w=30%,f=right](../01/k-armed_bandits.svgz)

As a simple example, consider the 10-armed bandits from the first lecture, with
single-step episodes.

~~~
Let the _behaviour policy_ be a uniform policy, so the return is a reward of
a randomly selected arm.

~~~
Let the _target policy_ be a greedy policy always using action 3.

~~~
Assume that the first sample from the behaviour policy produced action 3 with
reward $R$. Then
- Ordinary importance sampling estimate the return for the target policy as
  $$\frac{Ï€(a)}{b(a)} R = \frac{1}{1/10} R = 10â‹…R.$$

  The factor $10$ is present because the behaviour policy returns action 3
  in 10% cases.

~~~
- Weighted importance sampling estimate the return for target policy as average
  of rewards for action 3.

---
# Off-policy Monte Carlo Policy Evaluation

![w=80%,h=center](importance_sampling.svgz)

Comparison of ordinary and weighted importance sampling on Blackjack. Given
a state with dealer showing a deuce and sum of player's cards 13 with a usable
ace, we estimate target policy of sticking only with a sum of 20 and 21, using
uniform behaviour policy.

---
# Off-policy Monte Carlo Policy Evaluation

We can compute weighted importance sampling similarly to the incremental
implementation of Monte Carlo averaging.

![w=75%,h=center](off_policy_mc_prediction.svgz)

---
# Off-policy Monte Carlo

![w=80%,h=center](off_policy_mc.svgz)

---
# Expected Sarsa

The action $A_{t+1}$ is a source of variance, providing correct estimate only _in expectation_.

~~~
We could improve the algorithm by considering all actions proportionally to their
policy probability, obtaining Expected Sarsa algorithm:
$$\begin{aligned}
  q(S_t, A_t) &â† q(S_t, A_t) + Î±\Big(R_{t+1} + [Â¬\textrm{done}]â‹…Î³ ğ”¼_Ï€ q(S_{t+1}, a) - q(S_t, A_t)\Big)\\
              &â† q(S_t, A_t) + Î±\Big(R_{t+1} + [Â¬\textrm{done}]â‹…Î³ âˆ‘\nolimits_a Ï€(a|S_{t+1}) q(S_{t+1}, a) - q(S_t, A_t)\Big).
\end{aligned}$$

~~~
Compared to Sarsa, the expectation removes a source of variance and therefore
usually performs better. However, the complexity of the algorithm increases and
becomes dependent on number of actions $|ğ“|$.

---
# Expected Sarsa as an Off-policy Algorithm

Note that Expected Sarsa is also an off-policy algorithm, allowing the behaviour
policy $b$ and target policy $Ï€$ to differ.

~~~
Especially, if $Ï€$ is a greedy policy with respect to current value function,
Expected Sarsa simplifies to Q-learning.

---
# Expected Sarsa Example

![w=25%](../02/cliff_walking.svgz)![w=90%,mw=75%,h=center](expected_sarsa.svgz)

Asymptotic performance is an average over 100k episodes (10 runs), interim
performance over the first 100 episodes (50k runs); $Îµ$-greedy policy with
$Îµ=0.1%$ is used.

---
# Q-learning and Maximization Bias

Because behaviour policy in Q-learning is $Îµ$-greedy variant of the target
policy, the same samples (up to $Îµ$-greedy) determine both the maximizing action
and its value estimate.

~~~
![w=75%,h=center](double_q_learning_example.svgz)

---
# Double Q-learning

![w=80%,h=center](double_q_learning.svgz)

---
section: $n$-step
# $n$-step Methods

![w=40%,f=right](nstep_td.svgz)

Full return is
$$G_t = âˆ‘_{k=t}^âˆ Î³^{k-t} R_{k+1},$$
one-step return is
$$G_{t:t+1} = R_{t+1} + [Â¬\textrm{done}]â‹…Î³ V(S_{t+1}).$$

~~~
We can generalize both into $n$-step returns:
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + Î³^n V(S_{t+n}).$$
with $G_{t:t+n} â‰ G_t$ if $t+n â‰¥ T$ (episode length).

---
# $n$-step Methods

A natural update rule is
$$V(S_t) â† V(S_t) + Î±\big(G_{t:t+n} - V(S_t)\big).$$

~~~
![w=58%,h=center](nstep_td_prediction.svgz)

---
# $n$-step Methods Example

Using the random walk example, but with 19 states instead of 5,
![w=50%,h=center](../02/td_mc_comparison_example.svgz)

we obtain the following comparison of different values of $n$:
![w=53%,h=center](nstep_td_performance.svgz)

---
# $n$-step Sarsa

Defining the $n$-step return to utilize action-value function as
$$G_{t:t+n} â‰ \left(âˆ‘_{k=t}^{t+n-1} Î³^{k-t} R_{k+1}\right) + Î³^n Q(S_{t+n}, A_{t+n})$$
with $G_{t:t+n} â‰ G_t$ if $t+n â‰¥ T$,
~~~
we get the following straightforward
algorithm:
$$Q(S_t, A_t) â† Q(S_t, A_t) + Î±\big(G_{t:t+n} - Q(S_t, A_t)\big).$$

~~~
![w=70%,h=center](nstep_sarsa_example.svgz)

---
# $n$-step Sarsa Algorithm

![w=60%,h=center](nstep_sarsa_algorithm.svgz)

---
# Off-policy $n$-step Sarsa

Recall the relative probability of a trajectory under the target and behaviour policies,
which we now generalize as
$$Ï_{t:t+n} â‰ âˆ_{k=t}^{\min(t+n, T-1)} \frac{Ï€(A_k | S_k)}{b(A_k | S_k)}.$$

~~~
Then a simple off-policy $n$-step TD policy evaluation can be computed as
$$V(S_t) â† V(S_t) + Î±Ï_{t:t+n-1}\big(G_{t:t+n} - V(S_t)\big).$$

~~~
Similarly, $n$-step Sarsa becomes
$$Q(S_t, A_t) â† Q(S_t, A_t) + Î±Ï_{\boldsymbol{t+1}:\boldsymbol{t+n}}\big(G_{t:t+n} - Q(S_t, A_t)\big).$$

---
# Off-policy $n$-step Sarsa

![w=60%,h=center](off_policy_nstep_sarsa.svgz)

---
section: TB
# Off-policy $n$-step Without Importance Sampling

![w=30%,h=center](off_policy_nstep_algorithms.svgz)

Q-learning and Expected Sarsa can learn off-policy without importance sampling.

~~~
To generalize to $n$-step off-policy method, we must compute expectations
over actions in each step of $n$-step update. However, we have not obtained
a return for the non-sampled actions.

~~~
Luckily, we can estimate their values by using the current action-value
function.

---
# Off-policy $n$-step Without Importance Sampling

![w=10%,f=right](tree_backup_example.svgz)

~~~
We now derive the $n$-step reward, starting from one-step:
$$G_{t:t+1} â‰ R_{t+1} + [Â¬\textrm{done}]â‹…Î³âˆ‘\nolimits_a Ï€(a|S_{t+1}) Q(S_{t+1}, a).$$

~~~
For two-step, we get:
$$G_{t:t+2} â‰ R_{t+1} + Î³âˆ‘\nolimits_{aâ‰ A_{t+1}} Ï€(a|S_{t+1}) Q(S_{t+1}, a) + Î³Ï€(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$

~~~
Therefore, we can generalize to:
$$G_{t:t+n} â‰ R_{t+1} + Î³âˆ‘\nolimits_{aâ‰ A_{t+1}} Ï€(a|S_{t+1}) Q(S_{t+1}, a) + Î³Ï€(A_{t+1}|S_{t+1})G_{t+1:t+n},$$
with $G_{t:t+n} â‰ G_{t:T}$ if $t+n â‰¥ T$ (episode length).

~~~
The resulting algorithm is $n$-step **Tree backup** and it is an off-policy
$n$-step temporal difference method not requiring importance sampling.

---
# Off-policy $n$-step Without Importance Sampling

![w=55%,h=center](tree_backup_algorithm.svgz)

---
section: Refresh
# Where Are We

- Until now, we have solved the tasks by explicitly calculating expected return,
either as $v(s)$ or as $q(s, a)$.
~~~

  - Finite number of states and actions.
~~~
  - We do not share information between different states or actions.
~~~
  - We use $q(s, a)$ if we do not have the environment model
    (a _model-free_ method); if we do, it is usually better to
    estimate $v(s)$ and choose actions as $\argmax\nolimits_a ğ”¼[R + v(s')]$.
~~~
- The methods we know differ in several aspects:

  - Whether they compute return by simulating a whole episode (Monte Carlo
    methods), or by bootstrapping (temporal difference, i.e.,
    $G_t â‰ˆ R_t + v(S_t)$, possibly $n$-step).
~~~
    - TD methods more noisy and unstable, but can learn immediately and
      explicitly assume Markovian property of value function.
~~~
  - Whether they estimate the value function of the same policy they use to
    generate the episodes (on-policy) or not (off-policy).
~~~
    - The off-policy methods are more noisy and unstable, but more flexible.

---
section: FuncApprox
# Function Approximation

We now approximate the value function $v$ and/or the state-value function $q$,
selecting it from a family of functions parametrized by a weight vector $â†’w âˆˆ â„^d$.

~~~
We denote the approximations as
$$\begin{gathered}
  vÌ‚(s; â†’w),\\
  qÌ‚(s, a; â†’w).
\end{gathered}$$

~~~
Weights are usually shared among states. Therefore, we need to define state
distribution $Î¼(s)$ to obtain an objective for finding the best function approximation
(if we give preference to some states, improving their estimates might worsen
estimates in other states).

~~~
The state distribution $Î¼(s)$ gives rise to a natural objective function called
**Mean Squared Value Error**, denoted $\overline{VE}$:
$$\overline{VE}(â†’w) â‰ âˆ‘_{sâˆˆğ“¢} Î¼(s) \big(v_Ï€(s) - vÌ‚(s; â†’w)\big)^2.$$

---
# Function Approximation

For on-policy algorithms, $Î¼(s)$ is often the on-policy distribution (fraction of
time spent in $s$).

~~~
- For **episodic tasks**, let $h(s)$ be the probability that an episodes starts in state $s$,
  and let $Î·(s)$ denote the number of time steps spent, on average, in state $s$
  in a single episode:
  $$Î·(s) = h(s) + âˆ‘\nolimits_{s'}Î·(s')âˆ‘\nolimits_a Ï€(a|s') p(s|s', a).$$

~~~
  The on-policy distribution is then obtained by normalizing: $Î¼(s) â‰ \frac{Î·(s)}{âˆ‘_{s'} Î·(s')}.$

~~~
  ![w=30%,f=right](discounting_as_termination.svgz)

  If there is discounting ($Î³<1$), it should be treated as a form of
  termination, by including a factor $Î³$ to the second term of the $Î·(s)$ equation.

~~~
- For **continuing tasks**, we require $Î³<1$, and employ the same definition as
  in the episodic case.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $â†’w$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  â†’w_{t+1} &â† â†’w_t - \tfrac{1}{2} Î± âˆ‡_{â†’w_t} \big(v_Ï€(S_t) - vÌ‚(S_t; â†’w_t)\big)^2\\
           &â† â†’w_t + Î±\big(v_Ï€(S_t) - vÌ‚(S_t; â†’w_t)\big) âˆ‡_{â†’w_t} vÌ‚(S_t; â†’w_t).\\
\end{aligned}$$

~~~
As usual, the $v_Ï€(S_t)$ is estimated by a suitable sample of a return:
~~~
- in Monte Carlo methods, we use episodic return $G_t$,
~~~
- in temporal difference methods, we employ bootstrapping and use
  one-step return
  $$R_{t+1} + [Â¬\textrm{done}]â‹…Î³vÌ‚(S_{t+1}; â†’w)$$
  or an $n$-step return.

---
# Monte Carlo Gradient Policy Evaluation
![w=100%,v=middle](grad_mc_estimation.svgz)

---
# Linear Methods

A simple special case of function approximation are linear methods, where
$$vÌ‚\big(â†’x(s); â†’w\big) â‰ â†’x(s)^T â†’w = âˆ‘x(s)_i w_i.$$

~~~
The $â†’x(s)$ is a representation of state $s$, which is a vector of the same size
as $â†’w$. It is sometimes called a _feature vector_.

~~~
The SGD update rule then becomes
$$â†’w_{t+1} â† â†’w_t + Î±\big(v_Ï€(S_t) - vÌ‚(â†’x(S_t); â†’w_t)\big) â†’x(S_t).$$

~~~
This rule is the same as in the tabular methods if $â†’x(s)$ is the one-hot
representation of the state $s$.

---
# State Aggregation

Simple way of generating a feature vector is **state aggregation**, where several
neighboring states are grouped together.

~~~
For example, consider a 1000-state random walk, where transitions lead uniformly
randomly to any of 100 neighboring states on the left or on the right. Using
state aggregation, we can partition the 1000 states into 10 groups of 100
states. Monte Carlo policy evaluation then computes the following:

![w=60%,h=center](grad_mc_estimation_example.svgz)

---
# Feature Construction for Linear Methods

Many methods for construction features for linear methods have been developed in the past:
~~~
- polynomials,

~~~
- Fourier bases,
~~~
- radial basis functions,
~~~
- tile coding,
~~~
- â€¦

~~~
But of course, nowadays we use deep neural networks, which construct a suitable
feature vector automatically as a latent variable (the last hidden layer).

---
section: TileCoding
# Tile Coding

![w=100%,mh=90%,v=middle](tile_coding.svgz)

~~~
If $t$ overlapping tiles are used, the learning rate is usually normalized as $Î±/t$.

---
# Tile Coding

For example, on the 1000-state random walk example, the performance of the tile
coding surpasses state aggregation:

![w=60%,h=center](tile_coding_performance.svgz)
Each tile covers 200 states, and when multiple tiles are used, they are offset
by 4 states.

---
# Asymmetrical Tile Coding

In higher dimensions, the tiles should have asymmetrical offsets, with
a sequence of $(1, 3, 5, â€¦, 2d-1)$ proposed as a good choice.

![w=50%,h=center](tile_coding_asymmetrical.svgz)

---
section: Semi-GradTD
# Temporal Difference Semi-Gradient Policy Evaluation

In TD methods, we again bootstrap the estimate $v_Ï€(S_t)$ as
$R_{t+1} + [Â¬\textrm{done}]â‹…Î³vÌ‚(S_{t+1}; â†’w)$.

~~~
![w=70%,h=center](grad_td_estimation.svgz)

---
# Why Semi-Gradient TD

Note that the above algorithm is called **semi-gradient**, because it does not
backpropagate through $vÌ‚(S_{t+1}; â†’w)$:
$$â†’w â† â†’w + Î±\big(R_{t+1} + [Â¬\textrm{done}]â‹…Î³vÌ‚(S_{t+1}; â†’w) - vÌ‚(S_t; â†’w)\big) âˆ‡_{â†’w} vÌ‚(S_t; â†’w).$$

~~~
In other words, the above rule is in fact not an SGD update, because there does
not exist a function $J(â†’w)$, for which we would get the above update.

~~~
To sketch a proof, consider a linear $vÌ‚(S_t; â†’w) = âˆ‘_i x(S_t)_i w_i$ and assume such a $J(â†’w)$ exists.
Then
$$\tfrac{âˆ‚}{âˆ‚w_i}J(â†’w) = \big(R_{t+1} + Î³vÌ‚(S_{t+1}; â†’w) - vÌ‚(S_t; â†’w)\big) x(S_t)_i.$$

~~~
Now considering second derivatives, we see they are not equal, which is a contradiction:
$$\begin{aligned}
  \tfrac{âˆ‚}{âˆ‚w_i}\tfrac{âˆ‚}{âˆ‚w_j}J(â†’w) &= \big(Î³x(S_{t+1})_i - x(S_t)_i\big) x(S_t)_j = Î³x(S_{t+1})_i x(S_t)_j - x(S_t)_i x(S_t)_j \\
  \tfrac{âˆ‚}{âˆ‚w_j}\tfrac{âˆ‚}{âˆ‚w_i}J(â†’w) &= \big(Î³x(S_{t+1})_j - x(S_t)_j\big) x(S_t)_i = Î³x(S_{t+1})_j x(S_t)_i - x(S_t)_i x(S_t)_j
\end{aligned}$$

---
# Temporal Difference Semi-Gradient Convergence

It can be proven (by using separate theory than for SGD) that the linear
semi-gradient TD methods do converge.

~~~
However, they do not converge to the optimum of $\overline{VE}$. Instead, they
converge to a different **TD fixed point** $â†’w_\mathrm{TD}$.

~~~
It can be proven that
$$\overline{VE}(â†’w_\mathrm{TD}) â‰¤ \frac{1}{1-Î³} \min_{â†’w} \overline{VE}(â†’w).$$

~~~
However, when $Î³$ is close to one, the multiplication factor in the above bound
is quite large.

---
# Temporal Difference Semi-Gradient Policy Evaluation

As before, we can utilize $n$-step TD methods.

![w=65%,h=center](grad_td_nstep_estimation.svgz)

---
# Temporal Difference Semi-Gradient Policy Evaluation

On the left, the results of one-step TD(0) algorithm is presented.
The effect of increasing $n$ in an $n$-step variant is displayed on the right.

![w=100%](grad_td_estimation_example.svgz)

---
# Sarsa with Function Approximation

Until now, we talked only about policy evaluation. Naturally, we can extend it
to a full Sarsa algorithm:

![w=80%,h=center](grad_sarsa.svgz)

---
# Sarsa with Function Approximation

Additionally, we can incorporate $n$-step returns:

![w=55%,h=center](grad_sarsa_nstep.svgz)

---
# Mountain Car Example

![w=65%,h=center](mountain_car.png)

The performances are for semi-gradient Sarsa($Î»$) algorithm (which we did not
talked about yet) with tile coding of 8 overlapping tiles covering position and
velocity, with offsets of $(1, 3)$.

---
# Mountain Car Example

![w=50%,h=center](mountain_car_performance_1and8_step.svgz)
![w=50%,h=center](mountain_car_performance_nstep.svgz)

---
section: Off-policyDiver
# Off-policy Divergence With Function Approximation

Consider a deterministic transition between two states whose values are computed
using the same weight:

![w=20%,h=center](off_policy_divergence_idea.svgz)

~~~
- If initially $w=10$, the TD error will be also 10 (or nearly 10 if $Î³<1$).
~~~
- If for example $Î±=0.1$, $w$ will be increased to 11 (by 10%).
~~~
- This process can continue indefinitely.

~~~
However, the problem arises only in off-policy setting, where we do not decrease
value of the second state from further observation.

---
# Off-policy Divergence With Function Approximation

The previous idea can be implemented for instance by the following **Baird's
counterexample**:

![w=77%,h=center](off_policy_divergence_example.svgz)

The rewards are zero everywhere, so the value function is also zero everywhere.
We assume the initial values of weights are 1, except for $w_7=10$, and that the
learning rate $Î±=0.01$.

---
# Off-policy Divergence With Function Approximation

For off-policy semi-gradient Sarsa, or even for off-policy
dynamic-programming update (where we compute expectation over all following
states and actions), the weights diverge to $+âˆ$.
Using on-policy distribution converges fine.

$$â†’w â† â†’w + \frac{Î±}{|ğ“¢|} âˆ‘_s \Big(ğ”¼_Ï€ \big[R_{t+1} + Î³vÌ‚(S_{t+1}; â†’w) | S_t=s\big] - vÌ‚(s; â†’w)\Big) âˆ‡vÌ‚(s; â†’w)$$

![w=47%](off_policy_divergence_example.svgz)![w=53%](off_policy_divergence_results.svgz)

---
# Off-policy Divergence With Function Approximation

The divergence can happen when all following elements are combined:

- functional approximation;

~~~
- bootstrapping;

~~~
- off-policy training.

In the Sutton's and Barto's book, these are called **the deadly triad**.

---
section: DQN
# Deep Q Networks

Volodymyr Mnih et al.: _Playing Atari with Deep Reinforcement Learning_ (Dec 2013 on arXiv),

~~~
in Feb 2015 accepted in Nature as _Human-level control through deep reinforcement learning_.

~~~
Off-policy Q-learning algorithm with a convolutional neural network function
approximation of action-value function.

~~~
Training can be extremely brittle (and can even diverge as shown earlier).

---
# Deep Q Network

![w=85%,h=center](dqn_architecture.svgz)

---
# Deep Q Networks

- Preprocessing: $210Ã—160$ 128-color images are converted to grayscale and
  then resized to $84Ã—84$.
~~~
- Frame skipping technique is used, i.e., only every $4^\textrm{th}$ frame
  (out of 60 per second) is considered, and the selected action is repeated on
  the other frames.
~~~
- Input to the network are last $4$ frames (considering only the frames kept by
  frame skipping), i.e., an image with $4$ channels.
~~~
- The network is fairly standard, performing
  - 32 filters of size $8Ã—8$ with stride 4 and ReLU,
  - 64 filters of size $4Ã—4$ with stride 2 and ReLU,
  - 64 filters of size $3Ã—3$ with stride 1 and ReLU,
  - fully connected layer with 512 units and ReLU,
  - output layer with 18 output units (one for each action)

---
# Deep Q Networks

- Network is trained with RMSProp to minimize the following loss:
  $$ğ“› â‰ ğ”¼_{(s, a, r, s')âˆ¼\mathrm{data}}\left[(r + \left[Â¬\textrm{done}\right] â‹… Î³ \max\nolimits_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸))^2\right].$$
~~~
- An $Îµ$-greedy behavior policy is utilized (starts at $Îµ=1$ and gradually decreases to $0.1$).

Important improvements:
~~~
- **experience replay**: the generated episodes are stored in a buffer as $(s, a, r,
  s')$ quadruples, and for training a transition is sampled uniformly
  (off-policy training);
~~~
- separate **target network** $â†’Î¸Ì„$: to prevent instabilities, a separate _target
  network_ is used to estimate one-step returns. The weights are not trained,
  but copied from the trained network after a fixed number of gradient updates;
~~~
- reward clipping: because rewards have wildly different scale in different
  games, all positive rewards are replaced by $+1$ and negative by $-1$;
  life loss is used as an end of episode.
~~~
  - furthermore, $(r + \left[Â¬\textrm{done}\right] â‹… Î³ \max_{a'} Q(s', a'; â†’Î¸Ì„) - Q(s, a; â†’Î¸))$ is
    also clipped to $[-1, 1]$ (i.e., a $\textrm{smooth}_{L_1}$ loss or Huber loss).

---
# Deep Q Networks

![w=60%,h=center](dqn_algorithm.svgz)

---
# Deep Q Network

![w=40%,h=center](dqn_results.svgz)

---
# Deep Q Network

![w=80%,h=center](dqn_visualization_breakout.svgz)

---
# Deep Q Network

![w=100%,v=middle](dqn_visualization_pong.svgz)


---
class: tablewide
style: td:nth-of-type(1) {width: 75%}
# Deep Q Networks Hyperparameters

| Hyperparameter | Value |
|----------------|-------|
| minibatch size | 32 |
~~~
| replay buffer size | 1M |
~~~
| target network update frequency | 10k |
~~~
| discount factor | 0.99 |
~~~
| training frames | 50M |
~~~
| RMSProp learning rate and both momentums | 0.00025, 0.95 |
~~~
| initial $Îµ$, final $Îµ$ (linear decay) and frame of final $Îµ$ | 1.0, 0.1, 1M |
~~~
| replay start size | 50k |
~~~
| no-op max | 30 |
