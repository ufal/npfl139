title: NPFL139, Lecture 6
class: title, langtech, cc-by-sa
# Distributional RL II

## Milan Straka

### March 26, 2025

---
section: Distributional RL
class: section
# Distributional RL

---
# Distributional RL

Instead of an expected return $Q(s, a)$, we could estimate the distribution of
expected returns $Z(s, a)$ – the _value distribution_.

~~~
The authors define the distributional Bellman operator $𝓣^π$ as:
$$𝓣^π Z(s, a) ≝ R(s, a) + γ Z(S', A')~~~\textrm{for}~~~S'∼p(s, a), A'∼π(S').$$

~~~
The authors of the paper prove similar properties of the distributional Bellman
operator compared to the regular Bellman operator, mainly being a contraction
under a suitable metric.
~~~
- For Wasserstein metric $W_p$, the authors define  
  $$W̄_p(Z_1, Z_2)≝\sup\nolimits_{s, a} W_p\big(Z_1(s, a), Z_2(s, a)\big)$$
  and prove that $𝓣^π$ is a γ-contraction in $W̄_p$.
~~~
- However, $𝓣^π$ is not a contraction in KL divergence nor in total variation
  distance.

---
style: .katex-display { margin: .2em 0 }
class: dbend
# Wasserstein Metric

For two probability distributions $μ, ν$ on a metric space with metric $d$,
Wasserstein metric $W_p$ is defined as
$$W_p(μ, ν) ≝ \inf_{γ∈Γ(μ,ν)} \Big(𝔼_{(x, y)∼γ} d\big(x, y\big)^p\Big)^{1/p},$$
~~~
where $Γ(μ,ν)$ is a set of all _couplings_, each being a joint probability
distribution whose marginals are $μ$ and $ν$, respectively.
~~~
A possible intuition is the optimal transport of probability mass from $μ$ to
$ν$.

~~~
For distributions over reals with CDFs $F, G$, the optimal transport has an
analytic solution:

![w=27.5%,f=right](../05/wasserstein-1.svgz)

$$W_p(μ, ν) = \bigg(∫\nolimits_0^1 |F^{-1}(q) - G^{-1}(q)|^p \d q\bigg)^{1/p},$$
where $F^{-1}$ and $G^{-1}$ are _quantile functions_, i.e., inverse CDFs.

~~~
For $p=1$, the 1-Wasserstein metric correspond to area “between” F and G, and
in that case we can compute it also as $W_1(μ, ν) = ∫\nolimits_x \big|F(x)- G(x)\big| \d x.$

---
class: middle
# Wasserstein Metric

![w=50%](../05/wasserstein_1d.png)

![w=49%,f=right](../05/wasserstein_vs_ks_categorical.svgz)

![w=50%](../05/wasserstein_vs_kl.png)


---
# Distributional RL: C51 Refresh

The distribution of returns is modeled as a discrete distribution parametrized
by the number of atoms $N ∈ ℕ$ and by $V_\textrm{MIN}, V_\textrm{MAX} ∈ ℝ$.
Support of the distribution are atoms
$$\{z_i ≝ V_\textrm{MIN} + i Δz : 0 ≤ i < N\}\textrm{~~~for~}Δz ≝ \frac{V_\textrm{MAX} - V_\textrm{MIN}}{N-1}.$$

~~~
The atom probabilities are predicted using a $\softmax$ distribution as
$$Z_{→θ}(s, a) = \left\{z_i\textrm{ with probability }p_i = \frac{e^{f_i(s, a; →θ)}}{∑_j e^{f_j(s, a; →θ)}}\right\}.$$

---
# Distributional RL: C51 Refresh

![w=30%,f=right](../05/dqn_distributional_operator.svgz)

After the Bellman update, the support of the distribution $R(s, a) + γZ(s', a')$
is not the same as the original support. We therefore project it to the original
support by proportionally mapping each atom of the Bellman update to immediate
neighbors in the original support.

~~~
$$Φ\big(R(s, a) + γZ(s', a')\big)_i ≝
  ∑_{j=1}^N \left[ 1 - \frac{\left|[r + γz_j]_{V_\textrm{MIN}}^{V_\textrm{MAX}}-z_i\right|}{Δz} \right]_0^1 p_j(s', a').$$

~~~
The network is trained to minimize the Kullbeck-Leibler divergence between the
current distribution and the (mapped) distribution of the one-step update
$$D_\textrm{KL}\Big(Φ\big(R + γZ_{→θ̄}\big(s', \argmax_{a'} 𝔼Z_{→θ̄}(s', a')\big)\big) \Big\| Z_{→θ}\big(s, a\big)\Big).$$

---
# Distributional RL: C51 Refresh

![w=63%,mw=78%,f=left,h=center](../05/dqn_distributional_algorithm.svgz)

_Beware that there is a small bug in the original algorithm (on the left, taken
from the paper), improperly handling one special case._<br clear="both">

Note that by minimizing the $D_\textrm{KL}$ instead of the Wasserstein metric
$W_p$, the algorithm has no guarantee of convergence of any kind. However, the
authors did not know how to minimize it.

---
section: Quantile Regression
class: section
# Quantile Regression

---
# Distributional RL with Quantile Regression

Although the authors of C51 proved that the distributional Bellman operator
is a contraction with respect to Wasserstein metric $W_p$, they were not able
to actually minimize it during training; instead, they minimize the KL
divergence between the current value distribution and one-step estimate.

![w=60%,h=center](qr_dqn_c51projection.svgz)

---
# Distributional RL with Quantile Regression

The same authors later proposed a different approach, which actually manages to minimize
the 1-Wasserstein distance.

~~~
In contrast to C51, where $Z(s, a)$ is represented using a discrete distribution
on a fixed “comb” support of uniformly spaces locations, we now represent it
as a _quantile distribution_ – as quantiles $θ_i(s, a)$ for a fixed
probabilities $τ_1, …, τ_N$ with $τ_i = \frac{i}{N}$.

~~~
![w=37%,f=right](qr_dqn_1wasserstein.svgz)

Formally, we can define the quantile distribution as a uniform combination of
$N$ Diracs:
$$Z_θ(s, a) ≝ \frac{1}{N} ∑_{i=1}^N δ_{θ_i(s, a)},$$
~~~
so that the cumulative density function is a step function increasing by
$\frac{1}{N}$ on every quantile $θ_i$.

---
# Distributional RL with Quantile Regression

The quantile distribution offers several advantages:

~~~
- a fixed support is no longer required;

~~~
- the projection step $Φ$ is not longer needed;

~~~
- this parametrization enables direct minimization of the Wasserstein loss.

---
# Distributional RL with Quantile Regression

Recall that 1-Wasserstein distance between two distributions $μ, ν$ can be computed as
$$W_1(μ, ν) = ∫\nolimits_0^1 \big|F_μ^{-1}(q) - F_ν^{-1}(q)\big| \d q,$$
where $F_μ$, $F_ν$ are their cumulative density functions.

~~~
For arbitrary distribution $Z$, the we denote the most accurate quantile
distribution as
$$Π_{W_1} Z ≝ \argmin_{Z_θ} W_1(Z, Z_θ).$$

~~~
In this case, the 1-Wasserstein distance can be written as
$$W_1(Z, Z_θ) = ∑_{i=1}^N ∫\nolimits_{τ_{i-1}}^{τ_i} \big|F_Z^{-1}(q) - θ_i\big| \d q.$$

---
# Distributional RL with Quantile Regression

It can be proven that for continuous $F_Z^{-1}$, $W_1(Z, Z_θ)$ is minimized by
(for proof, see Lemma 2 of Dabney et al.: Distributional Reinforcement Learning
with Quantile Regression, or consider how the 1-Wasserstein distance changes in
the range $[τ_{i-1}, τ_i]$ when you move $θ_i$):

![w=46%,f=right](qr_dqn_1wasserstein.svgz)

$$\bigg\{θ_i ∈ ℝ \bigg| F_Z(θ_i) = \frac{τ_{i-1} + τ_i}{2}\bigg\}.$$

~~~
We denote the _quantile midpoints_ as
$$τ̂_i ≝ \frac{τ_{i-1} + τ_i}{2}.$$

~~~
In the paper, the authors prove that the composition
$Π_{W_1} 𝓣^π$ is γ-contraction in $W̄_∞$, so repeated
application of $Π_{W_1} 𝓣^π$ converges to a unique fixed
point.

---
# Quantile Regression

Our goal is now to show that it is possible to estimate a quantile $τ ∈ [0, 1]$
by minimizing a loss suitable for SGD.

~~~
Assume we have samples from a distribution $P$.

~~~
- Minimizing the MSE of $x̂$ and the samples of $P$,
  $$x̃ = \argmin\nolimits_x̂\, 𝔼_{x∼P} \big[(x - x̂)^2\big],$$
  yields the _mean_ of the distribution, $x̃ = 𝔼_{x∼P}[x]$.

~~~
  To show that this holds, we compute the derivative of the loss with respect to
  $x̂$ and set it to 0, arriving at
  $$0 = 𝔼_x [2(x̂ - x)] = 2 𝔼_x[x̂] - 2𝔼_x[x] = 2\big(x̂ - 𝔼_x[x]\big).$$

---
# Quantile Regression

Assume we have samples from a distribution $P$ with cumulative density function
$F_P$.

- Minimizing the mean absolute error (MAE) of $x̂$ and the samples of $P$,
  $$x̃ = \argmin\nolimits_x̂\, 𝔼_{x∼P} \big[|x - x̂|\big],$$
~~~
  yields the _median_ of the distribution, $x̃ = F_P^{-1}(0.5)$.

~~~
  We prove this again by computing the derivative with respect to $x̂$, assuming
  the functions are nice enough that the Leibniz integral rule can be used:

~~~
  $\displaystyle \frac{∂}{∂x̂} ∫_{-∞}^{∞} P(x) |x - x̂| \d x = \frac{∂}{∂x̂} \bigg[∫_{-∞}^{x̂} P(x) (x̂ - x) \d x  + ∫_x̂^∞ P(x) (x - x̂) \d x \bigg]$

~~~
  $\displaystyle \hphantom{\frac{∂}{∂x̂} ∫_{-∞}^{∞} P(x) |x̂ - x| \d x} = ∫_{-∞}^{x̂} P(x) \d x - ∫_x̂^∞ P(x) \d x$

~~~
  $\displaystyle \hphantom{\frac{∂}{∂x̂} ∫_{-∞}^{∞} P(x) |x̂ - x| \d x} = 2 ∫_{-∞}^{x̂} P(x) \d x - 1 = 2 F_P(x̂) - 1 = 2 \big(F_P(x̂) - \tfrac{1}{2}\big).$

---
class: dbend
# Leibniz integral rule

The Leibniz integral rule for differentiation under the integral sign states that
for $-∞ < a(x), b(x) < ∞$,

$$\frac{∂}{∂ x} \bigg[∫_{a(x)}^{b(x)} f(x, t) \d t \bigg] =$$

~~~
$$ = ∫_{a(x)}^{b(x)} \frac{∂}{∂ x} f(x, t) \d t
   + \bigg(\frac{∂}{∂ x} b(x)\bigg) f\big(x, b(x)\big)
   - \bigg(\frac{∂}{∂ x} a(x)\bigg) f\big(x, a(x)\big).$$

~~~
_Sufficient condition for the Leibniz integral rule to hold is that the $f(x,
y)$ and its partial derivative $\frac{∂}{∂x}f(x, y)$ are continuous in both $x$
and $t$, and $a(x)$ and $b(x)$ are continuous and have continuous derivatives._

~~~
_If any of the bounds is improper, additional conditions must hold, notably that
the integral of the partial derivatives of $f$ must converge._

---
# Quantile Regression

Assume we have samples from a distribution $P$ with cumulative density function
$F_P$.

- By generalizing the previous result, we can show that for a quantile $τ ∈ [0,
  1]$, if
  $$x̃ = \argmin\nolimits_x̂\, 𝔼_{x∼P} \big[(x - x̂)(τ - [x ≤ x̂])\big],$$
  then $x̃ = F_P^{-1}(τ)$.
~~~
  Let $ρ_τ(x - x̂) ≝ (x - x̂)(τ - [x ≤ x̂]) = |x - x̂| ⋅ |τ - [x ≤ x̂]|$.
~~~
  This loss penalizes overestimation errors with weight $1-τ$, underestimation
  errors with $τ$.

~~~
  $\displaystyle \frac{∂}{∂x̂} ∫_{-∞}^{∞} P(x) (x - x̂)(τ - [x ≤ x̂]) \d x =$

~~~
  $\displaystyle \kern2em = \frac{∂}{∂x̂} \bigg[(τ-1) ∫_{-∞}^{x̂} P(x) (x - x̂) \d x + τ ∫_x̂^∞ P(x) (x - x̂) \d x \bigg]$

~~~
  $\displaystyle \kern2em = (\textcolor{blue}{1} - \textcolor{magenta}{τ}) ∫_{-∞}^{x̂} P(x) \d x - \textcolor{magenta}{τ} ∫_x̂^∞ P(x) \d x = \textcolor{blue}{∫_{-∞}^{x̂} P(x) \d x} - \textcolor{magenta}{τ} = F_P(x̂) - τ.$


---
style: .katex-display { margin: .7em 0pt }
# Quantile Regression

Using the quantile regression, when we have a value distribution $Z$, we can
find the most accurate quantile distribution by minimizing
$$∑_{i=1}^N 𝔼_{z ∼ Z} \big[ρ_{τ̂_i}(z - θ_i)\big].$$

~~~
However, the quantile loss is not smooth around zero, which could limit
performance when training a model. The authors therefore propose the
**quantile Huber loss**, which acts as an asymmetric squared loss
in interval $[-κ, κ]$ and fall backs to the standard quantile loss outside this
range.

~~~
Specifically, let
$$ρ_τ^κ(z - θ) ≝ \begin{cases}
  \big|τ - [z ≤ θ]\big| ⋅ \tfrac{1}{2}\big(z - θ\big)^2 &~~\textrm{if}~~ |z - θ| ≤ κ,\\
  \big|τ - [z ≤ θ]\big| ⋅ κ\big(|z - θ| - \tfrac{1}{2}κ\big) &~~\textrm{otherwise},\\
  \end{cases}$$
with a special case of not using Huber loss at all for $κ=0$, i.e., $ρ_τ^0(z - θ) ≝ |z - θ|$.

---
section: QR-DQN
# Distributional RL with Quantile Regression

To conclude, in DR-DQN-$κ$, the network for a given state predicts $ℝ^{|𝓐|×N}$,
so $N$ quantiles for every action. The authors evaluate Huber loss with $κ=1$
and also pure quantile loss ($κ=0$).

~~~
The following loss is used:

![w=65%,h=center](qr_dqn_loss.svgz)

The $q_j$ is just $\frac{1}{N}$.

---
# Distributional RL with Quantile Regression

![w=100%](qr_dqn_approximation_errors.svgz)

Each state transition has probability of 0.1 of moving in a random direction.

---
style: table { line-height: 0.8 }
# Distributional RL with Quantile Regression

![w=90%,h=center](qr_dqn_atari_graphs.svgz)

![w=62%,mw=60%,f=left,h=center](qr_dqn_atari_results.svgz)

| Hyperparameter | Value |
|:---------------|------:|
| learning rate | 0.00005 |
| quantiles N | 200 |

$N$ chosen from $(10, 50, 100, 200)$ on 5 training games.

---
section: Implicit Quantile Networks
class: section
# Implicit Quantile Regression

---
# Implicit Quantile Networks for Distributional RL

In IQN (implicit quantile regression), the authors (again the same team as in
C51 and DR-DQN) generalize the value distribution representation to predict
_any given quantile $τ$_.

![w=69%,f=right](iqn_architecture_comparison.svgz)

~~~
- The $ψ(s)$ is a convolutional stack from DQN, composed of

  - CNN $8×8$, stride 4, 32 filters, ReLU;
  - CNN $4×4$, stride 2, 64 filters, ReLU;
  - CNN $3×3$, stride 1, 64 filters, ReLU.

~~~
- The $f$ is an MLP:
  - fully connected layer with 512 units, ReLU;
  - output layer, 1 unit.

---
style: .katex-display { margin: .7em 0pt }
# Implicit Quantile Networks for Distributional RL

The quantile $τ$ of the value distribution, $Z_τ(s, a)$, is modeled as
$$Z_τ(s, a) ≈ f\big(ψ(s) ⊙ φ(τ)\big)_a.$$

~~~
- Other ways than multiplicative combinations were tried (concatenation, or
  residual computation $ψ(s)⊙(1+φ(τ))$), but the multiplicative form delivered
  the best results.

~~~
- The quantile $τ$ is represented using trainable cosine embeddings with
  dimension $n=64$:
  $$φ_j(τ) ≝ \operatorname{ReLU}\Big(∑\nolimits_{i=0}^{n-1} \cos(π i τ) w_{i,j} + b_j\Big).$$

~~~
- The target policy is greedy with respect to action-value approximation
  computed using $K$ samples $τ̃_k ∼ U[0, 1]$:
  $$π(x) ≝ \argmax_a \frac{1}{K} ∑_{k=1}^K Z_{τ̃_k}(x, a).$$
~~~
  - As in DQN, the exploration is still performed by using the $ε$-greedy
    policy.

---
# Implicit Quantile Networks for Distributional RL

The overall loss is:

![w=60%,h=center](iqn_loss.svgz)

~~~

Note the different roles of $N$ and $N'$.

---
# Implicit Quantile Networks for Distributional RL

![w=95%,mw=67%,f=right,h=right](iqn_n_ablations.svgz)

The authors speculate that:
- large $N$ may increase sample complexity (faster
learning because we have more loss terms),
- larger $N'$ could reduce variance (like a minibatch size).

---
# Implicit Quantile Networks for Distributional RL

![w=75%,h=center](iqn_atari_graphs.svgz)
![w=52%,h=center,mw=65%](iqn_atari_results.svgz)![w=100%,mw=35%](iqn_atari_results_2.svgz)

---
# Implicit Quantile Networks for Distributional RL

The ablation experiments of the quantile representation. A full grid search with
two seeds for every configuration was performed, with the black dots
corresponding to the hyperparameters of IQN; six Atari games took part in the
evaluation.

![w=92%,h=center](iqn_hyperparameters.svgz)

~~~
- the gray horizontal line is the QR-DQN baseline;
~~~
- “learn” is a learnt MLP embedding with a single hidden layer of size $n$;
~~~
- “concat” combines the state and quantile representations by concatenation, not
  $⊙$.

---
# TrackMania using Implicit Quantile Networks

![w=89.5%,h=center](iqn_trackmania.jpg)

---
section: Policy Gradient Methods
class: section
# Policy Gradient Methods

---
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its _gradient_ $∇_{→θ} v_π(s)$.

---
# Policy Gradient Methods

In addition to discarding $ε$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,f=right](stochastic_policy_example.svgz)

In the example, the reward is -1 per step, and we assume the three states appear
identical under the function approximation.

---
# Policy Gradient Theorem

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let also $J(→θ) ≝ 𝔼_{s∼h} v_π(s)$.

~~~
Then
$$∇_{→θ} v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ)$$
and
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ),$$

~~~
where $P(s → … → s'|π)$ is the probability of getting to state $s'$ when starting
from state $s$, after any number of 0, 1, … steps. The $γ$ parameter should
be treated as a form of termination, i.e., $P(s → … → s'|π) ∝ ∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π)$.
