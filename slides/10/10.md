title: NPFL139, Lecture 10
class: title, langtech, cc-by-sa
# PPO, R2D2, Agent57

## Milan Straka

### April 23, 2025

---
section: Second-Order Methods
class: section
# First-order and Second-order Methods

---
# First-order and Second-order Methods

Neural networks usually rely on SGD for finding a minimum, by performing
$$â†’Î¸ â† â†’Î¸ - Î± âˆ‡_{â†’Î¸} L(â†’Î¸).$$

~~~
A disadvantage of this approach (so-called **first-order method**) is that we
need to specify the learning rates by ourselves, usually using quite a small
one, and perform the update many times.

~~~
However, in some situations, we can do better.

---
# Newtonâ€™s Root-Finding Method

Assume we have a function $f: â„ â†’ â„$ and we want to find its root. An SGD-like
algorithm would always move â€œtowardsâ€ zero by taking small steps.

~~~
![w=40%,f=right](newton_iteration.svgz)

Instead, we could consider the linear local approximation
(i.e., consider a line â€œtouchingâ€ the function in a given point)
and perform a step so that our linear local approximation has
a value 0:
$$x' â† x - \frac{f(x)}{f'(x)}.$$

~~~
## Finding Minima

The same method can be used to find minima, because a minimum
is just a root of a derivative, resulting in:
$$x' â† x - \frac{f'(x)}{f''(x)}.$$

---
# Newtonâ€™s Method

The following update is the Newtonâ€™s method of searching for extremes:
$$x' â† x - \frac{f'(x)}{f''(x)}.$$

It is a so-called **second-order** method, but it is just an SGD update with
a learning rate $\frac{1}{f''(x)}$.

~~~
## Derivation from Taylorâ€™s Expansion

The same update can be derived also from the Taylorâ€™s expansion
$$f(x + Îµ) â‰ˆ f(x) + Îµ f'(x) + \frac{1}{2} Îµ^2 f''(x) \textcolor{gray}{+ ğ“(Îµ^3)},$$

~~~
which we can minimize for $Îµ$ by
$$0 = \frac{âˆ‚f(x + Îµ)}{âˆ‚Îµ} â‰ˆ f'(x) + Îµ f''(x),\textrm{ ~obtaining~ }x + Îµ = x - \frac{f'(x)}{f''(x)}.$$

---
style: .katex-display { margin: .8em 0 }
# Training MLPs with the Newtonâ€™s Method

Note that the second-order methods (methods utilizing second derivatives) are
impractical when training MLPs with many parameters.
~~~
The problem is that there are too many second derivatives â€“ if we consider
weights $â†’Î¸ âˆˆ â„^D$,
- the gradient $âˆ‡_{â†’Î¸} L(â†’Î¸)$ has $D$ elements;
~~~
- however, we have a $DÃ—D$ matrix with all second derivatives, called the
  **Hessian** $H$:
  $$H_{i,j} â‰ \frac{âˆ‚^2 L(â†’Î¸)}{âˆ‚Î¸_i âˆ‚Î¸_j}.$$

~~~
The Taylor expansion of a multivariate function then has the following form:
$$f(â†’x + â†’Îµ) = f(â†’x) + â†’Îµ^T âˆ‡f(â†’x) + \frac{1}{2} â†’Îµ^T â‡‰H â†’Îµ,$$
from which we obtain the following second-order method update:
$$â†’x â† â†’x - â‡‰H^{-1} âˆ‡f(â†’x).$$

---
# Fisher Information Matrix

Assume we have a model computing a distribution $p(y | â†’x; â†’Î¸)$.

~~~
We define **score** $s(â†’Î¸; â†’x, y)$ as
$$s(â†’Î¸; â†’x, y) â‰ âˆ‡_{â†’Î¸} \log p(y|â†’x; â†’Î¸).$$

~~~
Given the formula for a derivative of a logarithm, the score can also be written
as
$$s(â†’Î¸; â†’x, y) â‰ \frac{âˆ‡_{â†’Î¸} p(y|â†’x; â†’Î¸)}{p(y|â†’x; â†’Î¸)}.$$

~~~
Note that the expectation of the score with respect to the model output $y$ is
zero:
$$ğ”¼_{yâˆ¼p(â†’x; â†’Î¸)} \big[s(â†’Î¸; â†’x, y)\big] = âˆ‘_y p(y | â†’x; â†’Î¸) \frac{âˆ‡_{â†’Î¸} p(y|â†’x; â†’Î¸)}{p(y|â†’x; â†’Î¸)} = âˆ‡_{â†’Î¸} âˆ‘_y p(y|â†’x; â†’Î¸) = âˆ‡_{â†’Î¸} 1 = 0.$$

---
# Fisher Information Matrix

Let $ğ““$ be the data generating distribution, and $ğ”»$ a dataset sampled from $ğ““$.

~~~
Assuming $â†’Î¸ âˆˆ â„^D$, we define the **Fisher Information Matrix**
$â‡‰F(â†’Î¸) âˆˆ â„^{DÃ—D}$ as the covariance matrix of the score:
$$â‡‰F(â†’Î¸) â‰ ğ”¼_{â†’xâˆ¼ğ““} ğ”¼_{yâˆ¼p(y|â†’x; â†’Î¸)} \Big[\big(s(â†’Î¸; â†’x, y) - ğ”¼[s(â†’Î¸; â†’x, y)]\big)\big(s(â†’Î¸; â†’x, y) - ğ”¼[s(â†’Î¸; â†’x, y)]\big)^T\Big].$$

~~~
Because the expectation of the score is zero, the definition simplifies to
$$â‡‰F(â†’Î¸) â‰ ğ”¼_{â†’xâˆ¼ğ““} ğ”¼_{yâˆ¼p(y|â†’x; â†’Î¸)} \Big[s(â†’Î¸; â†’x, y) s(â†’Î¸; â†’x, y)^T\Big].$$

~~~
The first expectation is usually computed over $ğ”»$. When the $y|â†’x$ is taken
from the data distribution $ğ““$, the matrix is called **Empirical Fisher**, and
it can be computed with a single forward and backward pass through the model;
however, empirical Fisher is not an unbiased sample of the Fisher.

---
style: .katex-display { margin: .4em 0 }
# Fisher Information Matrix

Now consider the usual NLL loss $ğ“›(y|â†’x; â†’Î¸) = -\log p(y|â†’x; â†’Î¸)$. Its Hessian is

$$H_{ğ“›(y|â†’x;â†’Î¸)} = H_{-\log p(y|â†’x;â†’Î¸)} = âˆ‡_{â†’Î¸} âˆ‡_{â†’Î¸} -\log p(y|â†’x; â†’Î¸).$$

~~~
We get that

$\displaystyle \mkern{6em}\mathllap{H_{ğ“›(y|â†’x;â†’Î¸)}} = âˆ‡_{â†’Î¸} âˆ‡_{â†’Î¸} -\log p(y|â†’x; â†’Î¸) = âˆ‡_{â†’Î¸} \bigg(\frac{-âˆ‡_{â†’Î¸} p(y|â†’x; â†’Î¸)}{p(y|â†’x; â†’Î¸)}\bigg)$
~~~
$\mkern{3em}\textcolor{gray}{\Big(\frac{f}{g}\Big)' = \frac{f'g - fg'}{g^2}}$

~~~
$\displaystyle \mkern{6em}{} = \frac{\big(-âˆ‡_{â†’Î¸}âˆ‡_{â†’Î¸} p(y|â†’x; â†’Î¸)\big)p(y|â†’x; â†’Î¸) + \big(âˆ‡_{â†’Î¸} p(y|â†’x; â†’Î¸)\big)\big(âˆ‡_{â†’Î¸} p(y|â†’x; â†’Î¸)\big)^T}{p(y|â†’x; â†’Î¸)p(y|â†’x; â†’Î¸)}$

~~~
$\displaystyle \mkern{6em}{} = \frac{-H_{p(y|â†’x; â†’Î¸)}}{p(y|â†’x; â†’Î¸)} + \underbrace{\bigg(\frac{âˆ‡_{â†’Î¸} p(y|â†’x; â†’Î¸)}{p(y|â†’x; â†’Î¸)}\bigg)\bigg(\frac{âˆ‡_{â†’Î¸} p(y|â†’x; â†’Î¸)}{p(y|â†’x; â†’Î¸)}\bigg)^T}_{s(â†’Î¸; â†’x, y) s(â†’Î¸; â†’x, y)^T}.$

~~~
Therefore,
$\displaystyle ğ”¼_{â†’xâˆ¼ğ““} ğ”¼_{yâˆ¼p(y|â†’x; â†’Î¸)} H_{ğ“›(y|â†’x;â†’Î¸)} = ğ”¼_{â†’xâˆ¼ğ““}\Big[\underbrace{\Big(âˆ« -H_{p(y|â†’x; â†’Î¸)} \d y\Big)}_{0} + â‡‰F(â†’Î¸) = â‡‰F(â†’Î¸)$.

---
style: .katex-display { margin: .7em 0 }
# Fisher Information Matrix

Lets consider

$$D_\textrm{KL} \big(p(y|â†’x; â†’Î¸) \| p(y|â†’x; â†’Î¸ + â†’d)\big).$$

~~~
Denoting $D_\textrm{KL}(â†’d) = D_\textrm{KL} \big(p(y|â†’x; â†’Î¸) \| p(y|â†’x; â†’Î¸ + â†’d)\big)$,
the expectation over $â†’x âˆ¼ ğ““$ of the Taylor expansion to the second order gives
~~~
$$ğ”¼_{â†’x âˆ¼ ğ““} D_\textrm{KL}(â†’d) = ğ”¼_{â†’x âˆ¼ ğ““}\Big[\underbrace{D_\textrm{KL}(â†’0)}_{0} + â†’d^T âˆ‡_{â†’d}D_\textrm{KL}(â†’d)\big\vert_{â†’d=0} + \tfrac{1}{2}â†’d^T â†’H_{D_\textrm{KL}(â†’d)} â†’d\big\vert_{â†’d=0} \Big] + ğ“\big(\|â†’d\|^3\big).$$

~~~
Given that at point $â†’d=0$
$$âˆ‡_{â†’d} D_\textrm{KL}(â†’d) = âˆ‡_{â†’d} ğ”¼_{yâˆ¼p(y|â†’x; â†’Î¸)}\bigg[-\log\frac{p(y|â†’x; â†’Î¸ + â†’d)}{p(y|â†’x; â†’Î¸)}\bigg] = ğ”¼_{yâˆ¼p(y|â†’x; â†’Î¸)}\big[âˆ‡_{â†’Î¸} -\log p(y|â†’x; â†’Î¸)\big],$$

~~~
we get that
$$ğ”¼_{â†’x âˆ¼ ğ““} D_\textrm{KL}(â†’d) â‰ˆ ğ”¼_{â†’x âˆ¼ ğ““} ğ”¼_{yâˆ¼p(y|â†’x; â†’Î¸)} \Big[â†’d^T s(â†’Î¸; â†’x, y) + \tfrac{1}{2}â†’d^T â†’H_{-\log p(y|â†’x; â†’Î¸)} â†’d \Big] = \tfrac{1}{2}â†’d^T â‡‰F(â†’Î¸) â†’d.$$

---
style: .katex-display { margin: .7em 0 }
# The Direction of the SGD Gradient Update

Consider the
$$\lim_{Îµ â†’ 0} \frac{1}{Îµ} \argmin_{â†’d, \|â†’d\| â‰¤ Îµ} ğ“›(ğ”»; â†’Î¸ + â†’d).$$

~~~
For a given $Îµ$, the Taylor expansion of order one gives
$$\argmin_{â†’d, \|â†’d\| â‰¤ Îµ} ğ“›(ğ”»; â†’Î¸) + â†’d^T âˆ‡_{â†’Î¸} ğ“›(ğ”»; â†’Î¸).$$

We solve the constrained optimum by forming a Lagrangian and computing
a derivative with respect to $â†’d$:
$$âˆ‡_{â†’d} \big(ğ“›(ğ”»; â†’Î¸) + â†’d^T âˆ‡_{â†’Î¸} ğ“›(ğ”»; â†’Î¸) + Î»(â†’d^T â†’d - Îµ)\big) = âˆ‡_{â†’Î¸} ğ“›(ğ”»; â†’Î¸) + 2 Î» â†’d,$$
~~~
resulting in
$$â†’d âˆ -âˆ‡_{â†’Î¸} ğ“›(ğ”»; â†’Î¸),\textrm{~~and therefore,~~} â†’d = Îµ \frac{-âˆ‡_{â†’Î¸} ğ“›(ğ”»; â†’Î¸)}{\|âˆ‡_{â†’Î¸} ğ“›(ğ”»; â†’Î¸)\|}.$$

---
style: .katex-display { margin: .45em 0 }
# The Direction of the FIM Gradient Update

Consider now
$$\argmin_{â†’d, D_\textrm{KL}(p(y|â†’x; â†’Î¸) \| p(y|â†’x; â†’Î¸ + â†’d)) â‰¤ Îµ} ğ“›(ğ”»; â†’Î¸ + â†’d).$$

~~~
Again applying the Taylor expansion to the loss term, forming a Lagrangian, and
utilizing the fact that $ğ”¼_{â†’x âˆ¼ ğ““} D_\textrm{KL}(p(y|â†’x; â†’Î¸) \| p(y|â†’x; â†’Î¸ + â†’d)) â‰ˆ \tfrac{1}{2}â†’d^T â‡‰F(â†’Î¸) â†’d,$
~~~
we get
$$âˆ‡_{â†’d} \big(ğ“›(ğ”»; â†’Î¸) + â†’d^T âˆ‡_{â†’Î¸} ğ“›(ğ”»; â†’Î¸) + Î»(\tfrac{1}{2} â†’d^T â‡‰F(â†’Î¸) â†’d - Îµ)\big),$$

~~~
resulting in
$$âˆ‡_{â†’Î¸} ğ“›(ğ”»; â†’Î¸) + Î» â‡‰F(â†’Î¸) â†’d,$$

~~~
obtaining
$$â†’d âˆ - â‡‰F(â†’Î¸)^{-1} âˆ‡_{â†’Î¸} ğ“›(ğ”»; â†’Î¸).$$

~~~
Note that if we consider just the diagonal of $â‡‰F(â†’Î¸)^{-1}$, the resulting
algorithm is similar to Adam.
~~~
Adam also computes a square root and adds $Îµ$ to the diagonal; both can be
considered dampening (limiting the size of an update when the FIM entries
are very close to zero).

---
section: NPG
class: section
# Natural Policy Gradient

---
# Natural Policy Gradient

Kakade (2002) introduced natural policy gradient, a second-order method
utilizing the Fisher Information Matrix.

~~~
Using policy gradient theorem, we are able to compute $âˆ‡ v_Ï€$. Normally, we
update the parameters by using directly this gradient. This choice is justified
by the fact that a vector $â†’d$ which maximizes $v_Ï€(s; â†’Î¸ + â†’d)$ under
the constraint that $\|â†’d\|^2$ is bounded by a small constant is exactly
the gradient $âˆ‡ v_Ï€$.

~~~
However, for the Fisher information matrix
$$â‡‰F(â†’Î¸) â‰ ğ”¼_{s} ğ”¼_{Ï€(a | s; â†’Î¸)} \Big[\big(âˆ‡_{â†’Î¸} \log Ï€(a|s; â†’Î¸)\big) \big(âˆ‡_{â†’Î¸} \log Ï€(a|s; â†’Î¸)\big)^T \Big],$$
we might update the parameters using $â†’d_F â‰ F(â†’Î¸)^{-1} âˆ‡ v_Ï€$.

~~~
It can be shown that the Fisher information metric is the only Riemannian metric
(up to rescaling) invariant to change of parameters under sufficient statistic.

---
# Natural Policy Gradient

![w=82%,h=center](npg.svgz)

~~~
An interesting property of using the $â†’d_F$ to update the parameters is that
- updating $â†’Î¸$ using $âˆ‡ v_Ï€$ will choose an arbitrary _better_ action in state
  $s$;
~~~
- updating $â†’Î¸$ using $â‡‰F(â†’Î¸)^{-1} âˆ‡ v_Ï€$ chooses the _best_ action (maximizing
  expected return), similarly to tabular greedy policy improvement.

~~~
However, computing $â†’d_F$ in a straightforward way is too costly.

---
# Truncated Natural Policy Gradient

Duan et al. (2016) in paper _Benchmarking Deep Reinforcement Learning for
Continuous Control_ propose a modification to the NPG to efficiently compute
$â†’d_F$.

~~~
Following Schulman et al. (2015), they suggest to use _conjugate gradient
algorithm_, which can solve a system of linear equations $â‡‰Aâ†’x = â†’b$
in an iterative manner, by using $â‡‰A$ only to compute products $â‡‰Aâ†’v$ for
a suitable $â†’v$.

~~~
Therefore, $â†’d_F$ is found as a solution of
$$â‡‰F(â†’Î¸)â†’d_F = âˆ‡ v_Ï€$$
and using only 10 iterations of the algorithm seem to suffice according to the
experiments.

~~~
Furthermore, Duan et al. suggest to use a specific learning rate suggested by
Peters et al (2008) of
$$\frac{Î±}{\sqrt{(âˆ‡ v_Ï€)^T â‡‰F(â†’Î¸)^{-1} âˆ‡ v_Ï€}}.$$

---
section: TRPO
class: section
# Trust Region Policy Optimization

---
# Trust Region Policy Optimization

Schulman et al. in 2015 wrote an influential paper introducing TRPO as an
improved variant of NPG.

~~~
Considering two policies $Ï€, Ï€Ìƒ$, we can write
$$v_Ï€Ìƒ = v_Ï€ + ğ”¼_{s âˆ¼ Î¼(Ï€Ìƒ)} ğ”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(s, a),$$
where $a_Ï€(s, a)$ is the advantage function $q_Ï€(s, a) - v_Ï€(s)$ and
$Î¼(Ï€Ìƒ)$ is the on-policy distribution of the policy $Ï€Ìƒ$.

~~~
Analogously to policy improvement, we see that if $ğ”¼_{aâˆ¼Ï€Ìƒ} a_Ï€(s, a) â‰¥0$, policy
$Ï€Ìƒ$ performance increases (or stays the same if the advantages are zero
everywhere).

~~~
However, sampling states $s âˆ¼ Î¼(Ï€Ìƒ)$ is costly. Therefore, we instead
consider
$$L_Ï€(Ï€Ìƒ) = v_Ï€ + ğ”¼_{s âˆ¼ Î¼(Ï€)} ğ”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(s, a).$$

---
# Trust Region Policy Optimization
$$L_Ï€(Ï€Ìƒ) = v_Ï€ + ğ”¼_{s âˆ¼ Î¼(Ï€)} ğ”¼_{a âˆ¼ Ï€Ìƒ(a | s)} a_Ï€(s, a)$$

Using $L_Ï€(Ï€Ìƒ)$ is usually justified by $L_Ï€(Ï€) = v_Ï€$ and $âˆ‡_Ï€Ìƒ L_Ï€(Ï€Ìƒ) |_{Ï€Ìƒ = Ï€} = âˆ‡_Ï€Ìƒ v_Ï€Ìƒ |_{Ï€Ìƒ = Ï€}$.

~~~
Schulman et al. additionally proves that if we denote
$Î± = D_\textrm{KL}^\textrm{max}(Ï€_\textrm{old} \| Ï€_\textrm{new})
   = \max_s D_\textrm{KL}\big(Ï€_\textrm{old}(â‹…|s) \| Ï€_\textrm{new}(â‹…|s)\big)$, then
$$v_{Ï€_\textrm{new}} â‰¥ L_{Ï€_\textrm{old}}(Ï€_\textrm{new}) - \frac{4ÎµÎ³}{(1-Î³)^2}Î±\textrm{~~~where~~~}Îµ = \max_{s, a} |a_Ï€(s, a)|.$$

~~~
Therefore, TRPO maximizes $L_{Ï€_{â†’Î¸_0}}(Ï€_{â†’Î¸})$ subject to
$D_\textrm{KL}(Ï€_{â†’Î¸_0} \| Ï€_{â†’Î¸}) < Î´$, where
- $D_\textrm{KL}^{â†’Î¸_0}(Ï€_{â†’Î¸_0} \| Ï€_{â†’Î¸}) = ğ”¼_{s âˆ¼ Î¼(Ï€_{â†’Î¸_0})} [D_\textrm{KL}\big(Ï€_\textrm{old}(â‹…|s) \| Ï€_\textrm{new}(â‹…|s)\big)]$
  is used instead of $D_\textrm{KL}^\textrm{max}$ for performance reasons;
~~~
- $Î´$ is a constant found empirically, as the one implied by the above equation
  is too small;
~~~
- importance sampling is used to account for sampling actions from $Ï€$.

---
# Trust Region Policy Optimization

$$\textrm{maximize}~~L_{Ï€_{â†’Î¸_0}}(Ï€_{â†’Î¸})
 = ğ”¼_{s âˆ¼ Î¼(Ï€_{â†’Î¸_0}), a âˆ¼ Ï€_{â†’Î¸_0}(a | s)} \Big[\tfrac{Ï€_{â†’Î¸}(a|s)}{Ï€_{â†’Î¸_0}(a|s)}a_{Ï€_{â†’Î¸_0}}(s, a)\Big]
 ~~\textrm{subject to}~~D_\textrm{KL}(Ï€_{â†’Î¸_0} \| Ï€_{â†’Î¸}) < Î´$$

The parameters are updated using $â†’d_F = â‡‰F(â†’Î¸)^{-1} âˆ‡ L_{Ï€_{â†’Î¸_0}}(Ï€_{â†’Î¸})$, utilizing the
conjugate gradient algorithm as described earlier for TNPG (note that the
algorithm was designed originally for TRPO and only later employed for TNPG).

~~~
To guarantee improvement and respect the $D_\textrm{KL}$ constraint, a line
search is in fact performed. We start by the learning rate of
$\sqrt{Î´/(â†’d_F^T â‡‰F(â†’Î¸)^{-1} â†’d_F)}$ and shrink it exponentially until
the constraint is satistifed and the objective improves.

---
# Trust Region Policy Optimization

![w=30%,h=center](rllib_tasks.svgz)

![w=100%](rllib_results.svgz)

---
section: PPO
class: section
# Proximal Policy Optimization

---
# Proximal Policy Optimization

PPO is a simplification of TRPO which can be implemented using a few lines of code.

~~~
Let $r_t(â†’Î¸) â‰ \frac{Ï€(A_t|S_t; â†’Î¸)}{Ï€(A_t|S_t; â†’Î¸_\textrm{old})}$.
~~~
PPO maximizes the objective (i.e., you should minimize its negation)
$$L^\textrm{CLIP}(â†’Î¸) â‰ ğ”¼_t\Big[\min\big(r_t(â†’Î¸) AÌ‚_t, \operatorname{clip}(r_t(â†’Î¸), 1-Îµ, 1+Îµ) AÌ‚_t)\big)\Big].$$

~~~
Such a $L^\textrm{CLIP}(â†’Î¸)$ is a lower (pessimistic) bound.

![w=60%,h=center](ppo_clipping.svgz)

---
# Proximal Policy Optimization

The advantages $AÌ‚_t$ are additionally estimated using the so-called
_generalized advantage estimation_, which is just an analogue
of the truncated n-step lambda return:
$$AÌ‚_t = âˆ‘_{i=0}^{n-1} Î³^i Î»^i \big(R_{t+1+i} + Î³ V(S_{t+i+1}) - V(S_{t + i})\big).$$

---
# Proximal Policy Optimization â€“ The Algorithm

![w=95%,h=center](ppo_algorithm.svgz)

~~~
- The rollout phase should be usually performed using vectorized environments.

~~~
- It is important to correctly handle episodes that did not finish in a rollout,
  using bootstrapping to estimate the return from the rest of the episode.
~~~
  That way, PPO can learn in long-horizont games with $T$ much smaller than
  episode length.

~~~
- Increasing $N$ increases parallelism, while increasing $T$ increase the number
  of steps that must be performed sequentially.

---
# Proximal Policy Optimization

![w=100%,v=middle](ppo_results.svgz)

---
# Proximal Policy Optimization

Results from the SAC paper:

![w=79%,h=center](../08/sac_results.svgz)

---
# Proximal Policy Optimization

- There are a few tricks that influence the peformance of PPO significantly;
  see the following nice blogpost about many of them:

  https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/

---
# Proximal Policy Optimization

- The paper _What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale
  Study_ https://openreview.net/forum?id=nIAxjsniDzg performs a evaluation of
  many hyperparameters of the PPO algorithm.

~~~
  Main takeaways:
~~~
  - Start with clipping threshold 0.25, but try increasing/decreasing it.
~~~
  - Initialization of the last policy layer influences the results considerably;
    recommendation is to use 100 times smaller weights.
~~~
  - Use softplus to parametrize standard deviation of actions, use a negative
    offset to decrease initial standard deviation of actions, tune it if
    possible.
~~~
  - Use $\tanh$ do transform the action distribution instead of clipping.
~~~
  - Do not share weights between the policy and value network; use a wide value
    network.
~~~
  - Always normalize observations; check if normalizing value function helps.
~~~
  - Use GAE with $Î»=0.9$, do not use Huber loss. Adam with 3e-4 is a safe choice.
~~~
  - Perform multiple passes over the data, recompute advantages at the beginning
    of every one of them.
~~~
  - The discount factor $Î³$ is important, tune it per environment starting with
    $Î³=0.99$.

---
section: TransRews
class: section
# Transformed Rewards

---
# Transformed Rewards

So far, we have clipped the rewards in DQN on Atari environments.

~~~
Consider a Bellman operator $ğ“£$
$$(ğ“£q)(s, a) â‰ ğ”¼_{s',r âˆ¼ p} \Big[r + Î³ \max_{a'} q(s', a')\Big].$$

~~~
Instead of clipping the magnitude of rewards, we might use a function
$h: â„ â†’ â„$ to reduce their scale. We define a transformed Bellman operator
$ğ“£_h$ as
$$(ğ“£_hq)(s, a) â‰ ğ”¼_{s',r âˆ¼ p} \Big[h\Big(r + Î³ \max_{a'} h^{-1} \big(q(s', a')\big)\Big)\Big].$$

---
# Transformed Rewards

It is easy to prove the following two propositions from a 2018 paper
_Observe and Look Further: Achieving Consistent Performance on Atari_ by Tobias
Pohlen et al.

~~~
1. If $h(z) = Î± z$ for $Î± > 0$, then $ğ“£_h^k q \xrightarrow{k â†’ âˆ} h \circ q_* = Î± q_*$.

~~~
   The statement follows from the fact that it is equivalent to scaling the
   rewards by a constant $Î±$.

~~~
2. When $h$ is strictly monotonically increasing and the MDP is deterministic,
   then $ğ“£_h^k q \xrightarrow{k â†’ âˆ} h \circ q_*$.

~~~
   This second proposition follows from
   $$h \circ q_* = h \circ ğ“£ q_* = h \circ ğ“£(h^{-1} \circ h \circ q_*) = ğ“£_h(h \circ q_*),$$
   where the last equality only holds if the MDP is deterministic.

---
# Transformed Rewards

For stochastic MDP, the authors prove that if $h$ is strictly monotonically
increasing, Lipschitz continuous with Lipschitz constant $L_h$, and has a
Lipschitz continuous inverse with Lipschitz constant $L_{h^{-1}}$, then
for $Î³ < \frac{1}{L_h L_{h^{-1}}}$, $ğ“£_h$ is again a contraction. (Proof
in Proposition A.1.)

~~~
For the Atari environments, the authors propose the transformation
$$h(x) â‰ \sign(x)\left(\sqrt{|x| + 1} - 1\right) + Îµx$$
with $Îµ = 10^{-2}$. The additive regularization term ensures that
$h^{-1}$ is Lipschitz continuous.

~~~
It is straightforward to verify that
$$h^{-1}(x) = \sign(x)\left(\left(\frac{\sqrt{1 + 4Îµ (|x| + 1 + Îµ)} - 1}{2Îµ} \right)^2 - 1\right).$$

~~~
In practice, discount factor larger than $\frac{1}{L_h L_{h^{-1}}}$ is being
used; however, it seems to work.

---
section: R2D2
class: section
# Recurrent Replay Distributed DQN (R2D2)

---
# Recurrent Replay Distributed DQN (R2D2)

Proposed in 2019, to study the effects of recurrent state, experience replay and
distributed training.

~~~
R2D2 utilizes prioritized replay, $n$-step double Q-learning with $n=5$,
convolutional layers followed by a 512-dimensional LSTM passed to duelling
architecture, generating experience by a large number of actors (256; each
performing approximately 260 steps per second) and learning from batches in
a single learner (achieving 5 updates per second using mini-batches of 64
sequences of length 80).

~~~
Rewards are transformed instead of clipped, and no loss-of-life-as-episode-end
heuristic is used.

~~~
Instead of individual transitions, the replay buffer consists of fixed-length
($m=80$) sequences of $(s, a, r)$, with adjacent sequences overlapping by 40
time steps.

~~~
The prioritized replay employs a combination of the maximum and the average
absolute 5-step TD errors $Î´_i$ over the sequence: $p = Î· \max_i Î´_i + (1 - Î·)
Î´Ì„$, for both $Î·$ and the priority exponent set to 0.9.

~~~
Several R2D2 agent videos are available at https://bit.ly/r2d2600.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=95%,mw=65%,h=right,f=right](r2d2_recurrent_staleness.svgz)

When running the recurrent network on a sequence from the replay buffer, two
strategies of initializing the hidden state are considered:
~~~
- **stored-state** uses the hidden state from the training;
- **zero-state** uses 0.

~~~
Furthermore, an optional burn-in of length 0, 20, and 40 (before the 80 states
used during training; only used for obtaining better hidden state) is considered.

~~~
The stored-state and burn-in of length 40 is used during evaluation.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=35%](../01/r2d2_results.svgz)![w=65%](r2d2_result_table.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=100%,v=middle](r2d2_hyperparameters.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=70%,h=center](r2d2_training_progress.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

Ablations comparing the reward clipping instead of value rescaling
(**Clipped**), smaller discount factor $Î³ = 0.99$ (**Discount**)
and **Feed-Forward** variant of R2D2. Furthermore, life-loss
**reset** evaluates resetting an episode on life loss, with
**roll** preventing value bootstrapping (but not LSTM unrolling).

![w=85%,h=center](r2d2_ablations.svgz)
![w=85%,h=center](r2d2_life_loss.svgz)

---
# Utilization of LSTM Memory During Inference

![w=100%](r2d2_memory_size.svgz)

The effect of restricting the policy to $k$ steps only (using either
the zero-state or stored-state initialization).

---
class: section
# Agent57

---
# Agent57

The Agent57 is an agent (from Mar 2020) capable of outperforming the standard
human benchmark on all 57 games.

~~~
Its most important components are:
- Retrace; from _Safe and Efficient Off-Policy Reinforcement Learning_ by Munos
  et al., https://arxiv.org/abs/1606.02647,
~~~
- Never give up strategy; from _Never Give Up: Learning Directed Exploration Strategies_
  by Badia et al., https://arxiv.org/abs/2002.06038,
~~~
- Agent57 itself; from _Agent57: Outperforming the Atari Human Benchmark_ by
  Badia et al., https://arxiv.org/abs/2003.13350.

---
section: Retrace
class: section
# Retrace

---
# Retrace

$\displaystyle \mathrlap{ğ“¡q(s, a) â‰ q(s, a) + ğ”¼_b \bigg[âˆ‘_{tâ‰¥0} Î³^t \left(âˆ\nolimits_{j=1}^t c_t\right)
  \Big(R_{t+1} + Î³ğ”¼_{A_{t+1} âˆ¼ Ï€} q(S_{t+1}, A_{t+1}) - q(S_t, A_t)\Big)\bigg],}$

where there are several possibilities for defining the traces $c_t$:
~~~
- **importance sampling**, $c_t = Ï_t = \frac{Ï€(A_t|S_t)}{b(A_t|S_t)}$,
  - the usual off-policy correction, but with possibly very high variance,
  - note that $c_t = 1$ in the on-policy case;
~~~
- **Tree-backup TB(Î»)**, $c_t = Î» Ï€(A_t|S_t)$,
  - the Tree-backup algorithm extended with traces,
  - however, $c_t$ can be much smaller than 1 in the on-policy case;
~~~
- **Retrace(Î»)**, $c_t = Î» \min\big(1, \frac{Ï€(A_t|S_t)}{b(A_t|S_t)}\big)$,
  - off-policy correction with limited variance, with $c_t = 1$ in the on-policy case.

~~~
The authors prove that $ğ“¡$ has a unique fixed point $q_Ï€$ for any
$0 â‰¤ c_t â‰¤ \frac{Ï€(A_t|S_t)}{b(A_t|S_t)}$.

---
section: NGU
class: section
# Never Give Up

---
# Never Give Up

The NGU (Never Give Up) agent performs _curiosity-driver exploration_, and
augment the extrinsic (MDP) rewards with an intrinsic reward. The augmented
reward at time $t$ is then $r_t^Î² â‰ r_t^e + Î² r_t^i$, with $Î²$ a scalar
weight of the intrinsic reward.

~~~
The intrinsic reward fulfills three goals:

~~~
1. quickly discourage visits of the same state in the same episode;

~~~
2. slowly discourage visits of the states visited many times in all episodes;

~~~
3. ignore the parts of the state not influenced by the agent's actions.

~~~
The intrinsic rewards is a combination of the episodic novelty $r_t^\textrm{episodic}$
and life-long novelty $Î±_t$:
$$r_t^i â‰ r_t^\textrm{episodic} â‹… \operatorname{clip}\Big(1 â‰¤ Î±_t â‰¤ L=5\Big).$$

---
style: .katex-display { margin: .5em 0 }
# Never Give Up

![w=70%,f=right](ngu_novelty.png)

The episodic novelty works by storing the embedded states $f(S_t)$ visited
during the episode in episodic memory $M$.

~~~
The $r_t^\textrm{episodic}$ is then estimated as

$$r_t^\textrm{episodic} = \frac{1}{\sqrt{\textrm{visit~count~of~}f(S_t)}}.$$

~~~
The visit count is estimated using similarities of $k$-nearest neighbors of $f(S_t)$
measured via an inverse kernel $K(x, z) = \frac{Îµ}{\frac{d(x, z)^2}{d_m^2} + Îµ}$ for
$d_m$ a running mean of the $k$-nearest neighbor distance:

$$r_t^\textrm{episodic} = \frac{1}{\sqrt{âˆ‘\nolimits_{f_i âˆˆ N_k} K(f(S_t), f_i)}+c}\textrm{,~~with~pseudo-count~c=0.001}.$$

---
# Never Give Up

![w=70%,f=right](ngu_novelty.png)

The state embeddings are trained to ignore the parts not influenced by the actions of the agent.

~~~

To this end, Siamese network $f$ is trained to predict $p(A_t|S_t, S_{t+1})$,
i.e., the action $A_t$ taken by the agent in state $S_t$ when the resulting
state is $S_{t+1}$.

~~~
The life-long novelty $Î±_t=1 + \tfrac{\|gÌ‚ - g\|^2 - Î¼_\textrm{err}}{Ïƒ_\textrm{err}}$
is trained using random network distillation (RND),
where a predictor network $gÌ‚$ tries to predict the output of an untrained
convolutional network $g$ by minimizing the mean squared error; the
$Î¼_\textrm{err}$ and $Ïƒ_\textrm{err}$ are the running mean and standard
deviation of the error $\|gÌ‚-g\|^2$.

---
# Never Give Up

![w=18%,f=right](ngu_architecture.svgz)

The NGU agent is based on R2D2 with transformed Retrace loss and augmented reward
$$r_t^i â‰ r_t^\textrm{episodic} â‹… \operatorname{clip}\Big(1 â‰¤ Î±_t â‰¤ L=5\Big).$$

~~~
![w=23%,f=left](ngu_betas_gammas.svgz)

To support multiple policies concentrating either on the extrinsic or the
intrinsic reward, the NGU agent trains a parametrized action-value function $q(s, a, Î²_i)$
which corresponds to reward $r_t^{Î²_i}$ for $Î²_0=0$ and $Î³_0=0.997$, â€¦, $Î²_{N-1}=Î²$
and $Î³_{N-1}=0.99$.

For evaluation, $q(s, a, 0)$ is employed.

---
# Never Give Up

![w=73%,h=center](ngu_results_table.svgz)
![w=75%,h=center](ngu_results.svgz)

---
# Never Give Up Ablations

![w=73%,h=center](ngu_ablations_embeddings.svgz)
![w=64%,h=center](ngu_ablations.svgz)

---
section: Agent57
class: section
# Agent57

---
# Agent57

The Agent57 is an agent (from Mar 2020) capable of outperforming the standard
human benchmark on all 57 games.

~~~
Its most important components are:
- Retrace; from _Safe and Efficient Off-Policy Reinforcement Learning_ by Munos
  et al., https://arxiv.org/abs/1606.02647,
~~~
- Never give up strategy; from _Never Give Up: Learning Directed Exploration Strategies_
  by Badia et al., https://arxiv.org/abs/2002.06038,
~~~
- Agent57 itself; from _Agent57: Outperforming the Atari Human Benchmark_ by
  Badia et al., https://arxiv.org/abs/2003.13350.

---
# Agent57

![w=32%,f=right](agent57_architecture.png)

Then Agent57 improves NGU with:
~~~
- splitting the action-value as $q(s, a, j; â†’Î¸) â‰ q(s, a, j; â†’Î¸^e) + Î²_j q(s, a, j; â†’Î¸^i)$, where

  - $q(s, a, j; â†’Î¸^e)$ is trained with $r_e$ as targets, and
  - $q(s, a, j; â†’Î¸^i)$ is trained with $r_i$ as targets.

~~~
- instead of considering all $(Î²_j, Î³_j)$ equal, we train a meta-controller
  using a non-stationary multi-arm bandit algorithm, where arms correspond
  to the choice of $j$ for a whole episode (so an actor first samples a $j$
  using multi-arm bandit problem and then updates it according to the observed
  return), and the reward signal is the undiscounted extrinsic episode return;
  each actor uses a different level of $Îµ_l$-greedy behavior;

~~~
- $Î³_{N-1}$ is increased from $0.997$ to $0.9999$.

---
# Agent57 â€“ Results

![w=35%,h=center](agent57_results.svgz)
![w=89%,h=center](agent57_results_table.svgz)

---
# Agent57 â€“ Ablations

![w=56%](agent57_ablations.svgz)![w=44%](agent57_ablations_arm.svgz)

