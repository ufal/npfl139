title: NPFL139, Lecture 10
class: title, langtech, cc-by-sa
# R2D2, Agent57, PPO

## Milan Straka

### April 22, 2024

---
section: TransRews
# Transformed Rewards

So far, we have clipped the rewards in DQN on Atari environments.

~~~
Consider a Bellman operator $ğ“£$
$$(ğ“£q)(s, a) â‰ ğ”¼_{s',r âˆ¼ p} \Big[r + Î³ \max_{a'} q(s', a')\Big].$$

~~~
Instead of clipping the magnitude of rewards, we might use a function
$h: â„ â†’ â„$ to reduce their scale. We define a transformed Bellman operator
$ğ“£_h$ as
$$(ğ“£_hq)(s, a) â‰ ğ”¼_{s',r âˆ¼ p} \Big[h\Big(r + Î³ \max_{a'} h^{-1} \big(q(s', a')\big)\Big)\Big].$$

---
# Transformed Rewards

It is easy to prove the following two propositions from a 2018 paper
_Observe and Look Further: Achieving Consistent Performance on Atari_ by Tobias
Pohlen et al.

~~~
1. If $h(z) = Î± z$ for $Î± > 0$, then $ğ“£_h^k q \xrightarrow{k â†’ âˆ} h \circ q_* = Î± q_*$.

~~~
   The statement follows from the fact that it is equivalent to scaling the
   rewards by a constant $Î±$.

~~~
2. When $h$ is strictly monotonically increasing and the MDP is deterministic,
   then $ğ“£_h^k q \xrightarrow{k â†’ âˆ} h \circ q_*$.

~~~
   This second proposition follows from
   $$h \circ q_* = h \circ ğ“£ q_* = h \circ ğ“£(h^{-1} \circ h \circ q_*) = ğ“£_h(h \circ q_*),$$
   where the last equality only holds if the MDP is deterministic.

---
# Transformed Rewards

For stochastic MDP, the authors prove that if $h$ is strictly monotonically
increasing, Lipschitz continuous with Lipschitz constant $L_h$, and has a
Lipschitz continuous inverse with Lipschitz constant $L_{h^{-1}}$, then
for $Î³ < \frac{1}{L_h L_{h^{-1}}}$, $ğ“£_h$ is again a contraction. (Proof
in Proposition A.1.)

~~~
For the Atari environments, the authors propose the transformation
$$h(x) â‰ \sign(x)\left(\sqrt{|x| + 1} - 1\right) + Îµx$$
with $Îµ = 10^{-2}$. The additive regularization term ensures that
$h^{-1}$ is Lipschitz continuous.

~~~
It is straightforward to verify that
$$h^{-1}(x) = \sign(x)\left(\left(\frac{\sqrt{1 + 4Îµ (|x| + 1 + Îµ)} - 1}{2Îµ} \right)^2 - 1\right).$$

~~~
In practice, discount factor larger than $\frac{1}{L_h L_{h^{-1}}}$ is being
used â€“ however, it seems to work.

---
section: R2D2
# Recurrent Replay Distributed DQN (R2D2)

Proposed in 2019, to study the effects of recurrent state, experience replay and
distributed training.

~~~
R2D2 utilizes prioritized replay, $n$-step double Q-learning with $n=5$,
convolutional layers followed by a 512-dimensional LSTM passed to duelling
architecture, generating experience by a large number of actors (256; each
performing approximately 260 steps per second) and learning from batches by
a single learner (achieving 5 updates per second using mini-batches of 64
sequences of length 80).

~~~
Rewards are transformed instead of clipped, and no loss-of-life-as-episode-end
heuristic is used.

~~~
Instead of individual transitions, the replay buffer consists of fixed-length
($m=80$) sequences of $(s, a, r)$, with adjacent sequences overlapping by 40
time steps.

~~~
The prioritized replay employs a combination of the maximum and the average
absolute 5-step TD errors $Î´_i$ over the sequence: $p = Î· \max_i Î´_i + (1 - Î·)
Î´Ì„$, for both $Î·$ and the priority exponent set to 0.9.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=75%,h=center](r2d2_recurrent_staleness.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=35%](../01/r2d2_results.svgz)![w=65%](r2d2_result_table.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=100%,v=middle](r2d2_hyperparameters.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=70%,h=center](r2d2_training_progress.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

Ablations comparing the reward clipping instead of value rescaling
(**Clipped**), smaller discount factor $Î³ = 0.99$ (**Discount**)
and **Feed-Forward** variant of R2D2. Furthermore, life-loss
**reset** evaluates resetting an episode on life loss, with
**roll** preventing value bootstrapping (but not LSTM unrolling).

![w=85%,h=center](r2d2_ablations.svgz)
![w=85%,h=center](r2d2_life_loss.svgz)

---
# Utilization of LSTM Memory During Inference

![w=100%,v=middle](r2d2_memory_size.svgz)

---
section: Agent57
# Agent57

The Agent57 is an agent (from Mar 2020) capable of outperforming the standard
human benchmark on all 57 games.

~~~
Its most important components are:
- Retrace; from _Safe and Efficient Off-Policy Reinforcement Learning_ by Munos
  et al., https://arxiv.org/abs/1606.02647,
~~~
- Never give up strategy; from _Never Give Up: Learning Directed Exploration Strategies_
  by Badia et al., https://arxiv.org/abs/2002.06038,
~~~
- Agent57 itself; from _Agent57: Outperforming the Atari Human Benchmark_ by
  Badia et al., https://arxiv.org/abs/2003.13350.

---
# Retrace

$\displaystyle \mathrlap{ğ“¡q(s, a) â‰ q(s, a) + ğ”¼_b \bigg[âˆ‘_{tâ‰¥0} Î³^t \left(âˆ\nolimits_{j=1}^t c_t\right)
  \Big(R_{t+1} + Î³ğ”¼_{A_{t+1} âˆ¼ Ï€} q(S_{t+1}, A_{t+1}) - q(S_t, A_t)\Big)\bigg],}$

where there are several possibilities for defining the traces $c_t$:
~~~
- **importance sampling**, $c_t = Ï_t = \frac{Ï€(A_t|S_t)}{b(A_t|S_t)}$,
  - the usual off-policy correction, but with possibly very high variance,
  - note that $c_t = 1$ in the on-policy case;
~~~
- **Tree-backup TB(Î»)**, $c_t = Î» Ï€(A_t|S_t)$,
  - the Tree-backup algorithm extended with traces,
  - however, $c_t$ can be much smaller than 1 in the on-policy case;
~~~
- **Retrace(Î»)**, $c_t = Î» \min\big(1, \frac{Ï€(A_t|S_t)}{b(A_t|S_t)}\big)$,
  - off-policy correction with limited variance, with $c_t = 1$ in the on-policy case.

~~~
The authors prove that $ğ“¡$ has a unique fixed point $q_Ï€$ for any
$0 â‰¤ c_t â‰¤ \frac{Ï€(A_t|S_t)}{b(A_t|S_t)}$.

---
# Never Give Up

The NGU (Never Give Up) agent performs _curiosity-driver exploration_, and
augment the extrinsic (MDP) rewards with an intrinsic reward. The augmented
reward at time $t$ is then $r_t^Î² â‰ r_t^e + Î² r_t^i$, with $Î²$ a scalar
weight of the intrinsic reward.

~~~
The intrinsic reward fulfills three goals:

~~~
1. quickly discourage visits of the same state in the same episode;

~~~
2. slowly discourage visits of the states visited many times in all episodes;

~~~
3. ignore the parts of the state not influenced by the agent's actions.

~~~
The intrinsic rewards is a combination of the episodic novelty $r_t^\textrm{episodic}$
and life-long novelty $Î±_t$:
$$r_t^i â‰ r_t^\textrm{episodic} â‹… \operatorname{clip}\Big(1 â‰¤ Î±_t â‰¤ L=5\Big).$$

---
style: .katex-display { margin: .5em 0 }
# Never Give Up

![w=70%,f=right](ngu_novelty.png)

The episodic novelty works by storing the embedded states $f(S_t)$ visited
during the episode in episodic memory $M$.

~~~
The $r_t^\textrm{episodic}$ is then estimated as

$$r_t^\textrm{episodic} = \frac{1}{\sqrt{\textrm{visit~count~of~}f(S_t)}}.$$

~~~
The visit count is estimated using similarities of $k$-nearest neighbors of $f(S_t)$
measured via an inverse kernel $K(x, z) = \frac{Îµ}{\frac{d(x, z)^2}{d_m^2} + Îµ}$ for
$d_m$ a running mean of the $k$-nearest neighbor distance:

$$r_t^\textrm{episodic} = \frac{1}{\sqrt{âˆ‘\nolimits_{f_i âˆˆ N_k} K(f(S_t), f_i)}+c}\textrm{,~~with~pseudo-count~c=0.001}.$$

---
# Never Give Up

![w=70%,f=right](ngu_novelty.png)

The state embeddings are trained to ignore the parts not influenced by the actions of the agent.

~~~

To this end, Siamese network $f$ is trained to predict $p(A_t|S_t, S_{t+1})$,
i.e., the action $A_t$ taken by the agent in state $S_t$ when the resulting
state is $S_{t+1}$.

~~~
The life-long novelty $Î±_t=1 + \tfrac{\|gÌ‚ - g\|^2 - Î¼_\textrm{err}}{Ïƒ_\textrm{err}}$
is trained using random network distillation (RND),
where a predictor network $gÌ‚$ tries to predict the output of an untrained
convolutional network $g$ by minimizing the mean squared error; the
$Î¼_\textrm{err}$ and $Ïƒ_\textrm{err}$ are the running mean and standard
deviation of the error $\|gÌ‚-g\|^2$.

---
# Never Give Up

![w=18%,f=right](ngu_architecture.svgz)

The NGU agent uses transformed Retrace loss with the augmented reward
$$r_t^i â‰ r_t^\textrm{episodic} â‹… \operatorname{clip}\Big(1 â‰¤ Î±_t â‰¤ L=5\Big).$$

~~~
![w=23%,f=left](ngu_betas_gammas.svgz)

To support multiple policies concentrating either on the extrinsic or the
intrinsic reward, the NGU agent trains a parametrized action-value function $q(s, a, Î²_i)$
which corresponds to reward $r_t^{Î²_i}$ for $Î²_0=0$ and $Î³_0=0.997$, â€¦, $Î²_{N-1}=Î²$
and $Î³_{N-1}=0.99$.

For evaluation, $q(s, a, 0)$ is employed.

---
# Never Give Up

![w=73%,h=center](ngu_results_table.svgz)
![w=75%,h=center](ngu_results.svgz)

---
# Never Give Up Ablations

![w=73%,h=center](ngu_ablations_embeddings.svgz)
![w=64%,h=center](ngu_ablations.svgz)

---
# Agent57

![w=32%,f=right](agent57_architecture.png)

Then Agent57 improves NGU with:
~~~
- splitting the action-value as $q(s, a, j; â†’Î¸) â‰ q(s, a, j; â†’Î¸^e) + Î²_j q(s, a, j; â†’Î¸^i)$, where

  - $q(s, a, j; â†’Î¸^e)$ is trained with $r_e$ as targets, and
  - $q(s, a, j; â†’Î¸^i)$ is trained with $r_i$ as targets.

~~~
- instead of considering all $(Î²_j, Î³_j)$ equal, we train a meta-controller
  using a non-stationary multi-arm bandit algorithm, where arms correspond
  to the choice of $j$ for a whole episode (so an actor first samples a $j$
  using multi-arm bandit problem and then updates it according to the observed
  return), and the reward signal is the undiscounted extrinsic episode return;
  each actor uses a different level of $Îµ_l$-greedy behavior;

~~~
- $Î³_{N-1}$ is increased from $0.997$ to $0.9999$.

---
# Agent57 â€“ Results

![w=35%,h=center](agent57_results.svgz)
![w=89%,h=center](agent57_results_table.svgz)

---
# Agent57 â€“ Ablations

![w=56%](agent57_ablations.svgz)![w=44%](agent57_ablations_arm.svgz)
