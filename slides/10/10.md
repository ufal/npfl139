title: NPFL139, Lecture 10
class: title, langtech, cc-by-sa
# PPO, R2D2, Agent57

## Milan Straka

### April 23, 2025

---
section: Second-Order Methods
class: section
# First-order and Second-order Methods

---
# First-order and Second-order Methods

Neural networks usually rely on SGD for finding a minimum, by performing
$$→θ ← →θ - α ∇_{→θ} L(→θ).$$

~~~
A disadvantage of this approach (so-called **first-order method**) is that we
need to specify the learning rates by ourselves, usually using quite a small
one, and perform the update many times.

~~~
However, in some situations, we can do better.

---
# Newton’s Root-Finding Method

Assume we have a function $f: ℝ → ℝ$ and we want to find its root. An SGD-like
algorithm would always move “towards” zero by taking small steps.

~~~
![w=40%,f=right](newton_iteration.svgz)

Instead, we could consider the linear local approximation
(i.e., consider a line “touching” the function in a given point)
and perform a step so that our linear local approximation has
a value 0:
$$x' ← x - \frac{f(x)}{f'(x)}.$$

~~~
## Finding Minima

The same method can be used to find minima, because a minimum
is just a root of a derivative, resulting in:
$$x' ← x - \frac{f'(x)}{f''(x)}.$$

---
# Newton’s Method

The following update is the Newton’s method of searching for extremes:
$$x' ← x - \frac{f'(x)}{f''(x)}.$$

It is a so-called **second-order** method, but it is just an SGD update with
a learning rate $\frac{1}{f''(x)}$.

~~~
## Derivation from Taylor’s Expansion

The same update can be derived also from the Taylor’s expansion
$$f(x + ε) ≈ f(x) + ε f'(x) + \frac{1}{2} ε^2 f''(x) \textcolor{gray}{+ 𝓞(ε^3)},$$

~~~
which we can minimize for $ε$ by
$$0 = \frac{∂f(x + ε)}{∂ε} ≈ f'(x) + ε f''(x),\textrm{ ~obtaining~ }x + ε = x - \frac{f'(x)}{f''(x)}.$$

---
style: .katex-display { margin: .8em 0 }
# Training MLPs with the Newton’s Method

Note that the second-order methods (methods utilizing second derivatives) are
impractical when training MLPs with many parameters.
~~~
The problem is that there are too many second derivatives – if we consider
weights $→θ ∈ ℝ^D$,
- the gradient $∇_{→θ} L(→θ)$ has $D$ elements;
~~~
- however, we have a $D×D$ matrix with all second derivatives, called the
  **Hessian** $H$:
  $$H_{i,j} ≝ \frac{∂^2 L(→θ)}{∂θ_i ∂θ_j}.$$

~~~
The Taylor expansion of a multivariate function then has the following form:
$$f(→x + →ε) = f(→x) + →ε^T ∇f(→x) + \frac{1}{2} →ε^T ⇉H →ε,$$
from which we obtain the following second-order method update:
$$→x ← →x - ⇉H^{-1} ∇f(→x).$$

---
# Fisher Information Matrix

Assume we have a model computing a distribution $p(y | →x; →θ)$.

~~~
We define **score** $s(→θ; →x, y)$ as
$$s(→θ; →x, y) ≝ ∇_{→θ} \log p(y|→x; →θ).$$

~~~
Given the formula for a derivative of a logarithm, the score can also be written
as
$$s(→θ; →x, y) ≝ \frac{∇_{→θ} p(y|→x; →θ)}{p(y|→x; →θ)}.$$

~~~
Note that the expectation of the score with respect to the model output $y$ is
zero:
$$𝔼_{y∼p(→x; →θ)} \big[s(→θ; →x, y)\big] = ∑_y p(y | →x; →θ) \frac{∇_{→θ} p(y|→x; →θ)}{p(y|→x; →θ)} = ∇_{→θ} ∑_y p(y|→x; →θ) = ∇_{→θ} 1 = 0.$$

---
# Fisher Information Matrix

Let $𝓓$ be the data generating distribution, and $𝔻$ a dataset sampled from $𝓓$.

~~~
Assuming $→θ ∈ ℝ^D$, we define the **Fisher Information Matrix**
$⇉F(→θ) ∈ ℝ^{D×D}$ as the covariance matrix of the score:
$$⇉F(→θ) ≝ 𝔼_{→x∼𝓓} 𝔼_{y∼p(y|→x; →θ)} \Big[\big(s(→θ; →x, y) - 𝔼[s(→θ; →x, y)]\big)\big(s(→θ; →x, y) - 𝔼[s(→θ; →x, y)]\big)^T\Big].$$

~~~
Because the expectation of the score is zero, the definition simplifies to
$$⇉F(→θ) ≝ 𝔼_{→x∼𝓓} 𝔼_{y∼p(y|→x; →θ)} \Big[s(→θ; →x, y) s(→θ; →x, y)^T\Big].$$

~~~
The first expectation is usually computed over $𝔻$. When the $y|→x$ is taken
from the data distribution $𝓓$, the matrix is called **Empirical Fisher**, and
it can be computed with a single forward and backward pass through the model;
however, empirical Fisher is not an unbiased sample of the Fisher.

---
style: .katex-display { margin: .4em 0 }
# Fisher Information Matrix

Now consider the usual NLL loss $𝓛(y|→x; →θ) = -\log p(y|→x; →θ)$. Its Hessian is

$$H_{𝓛(y|→x;→θ)} = H_{-\log p(y|→x;→θ)} = ∇_{→θ} ∇_{→θ} -\log p(y|→x; →θ).$$

~~~
We get that

$\displaystyle \mkern{6em}\mathllap{H_{𝓛(y|→x;→θ)}} = ∇_{→θ} ∇_{→θ} -\log p(y|→x; →θ) = ∇_{→θ} \bigg(\frac{-∇_{→θ} p(y|→x; →θ)}{p(y|→x; →θ)}\bigg)$
~~~
$\mkern{3em}\textcolor{gray}{\Big(\frac{f}{g}\Big)' = \frac{f'g - fg'}{g^2}}$

~~~
$\displaystyle \mkern{6em}{} = \frac{\big(-∇_{→θ}∇_{→θ} p(y|→x; →θ)\big)p(y|→x; →θ) + \big(∇_{→θ} p(y|→x; →θ)\big)\big(∇_{→θ} p(y|→x; →θ)\big)^T}{p(y|→x; →θ)p(y|→x; →θ)}$

~~~
$\displaystyle \mkern{6em}{} = \frac{-H_{p(y|→x; →θ)}}{p(y|→x; →θ)} + \underbrace{\bigg(\frac{∇_{→θ} p(y|→x; →θ)}{p(y|→x; →θ)}\bigg)\bigg(\frac{∇_{→θ} p(y|→x; →θ)}{p(y|→x; →θ)}\bigg)^T}_{s(→θ; →x, y) s(→θ; →x, y)^T}.$

~~~
Therefore,
$\displaystyle 𝔼_{→x∼𝓓} 𝔼_{y∼p(y|→x; →θ)} H_{𝓛(y|→x;→θ)} = 𝔼_{→x∼𝓓}\Big[\underbrace{\Big(∫ -H_{p(y|→x; →θ)} \d y\Big)}_{0} \Big] + ⇉F(→θ) = ⇉F(→θ)$.

---
style: .katex-display { margin: .7em 0 }
# Fisher Information Matrix

Lets consider

$$D_\textrm{KL} \big(p(y|→x; →θ) \| p(y|→x; →θ + →d)\big).$$

~~~
Denoting $D_\textrm{KL}(→d) = D_\textrm{KL} \big(p(y|→x; →θ) \| p(y|→x; →θ + →d)\big)$,
the expectation over $→x ∼ 𝓓$ of the Taylor expansion to the second order gives
~~~
$$𝔼_{→x ∼ 𝓓} D_\textrm{KL}(→d) = 𝔼_{→x ∼ 𝓓}\Big[\underbrace{D_\textrm{KL}(→0)}_{0} + →d^T ∇_{→d}D_\textrm{KL}(→d)\big\vert_{→d=0} + \tfrac{1}{2}→d^T →H_{D_\textrm{KL}(→d)} →d\big\vert_{→d=0} \Big] + 𝓞\big(\|→d\|^3\big).$$

~~~
Given that at point $→d=0$
$$∇_{→d} D_\textrm{KL}(→d) = ∇_{→d} 𝔼_{y∼p(y|→x; →θ)}\bigg[-\log\frac{p(y|→x; →θ + →d)}{p(y|→x; →θ)}\bigg] = 𝔼_{y∼p(y|→x; →θ)}\big[∇_{→θ} -\log p(y|→x; →θ)\big],$$

~~~
we get that
$$𝔼_{→x ∼ 𝓓} D_\textrm{KL}(→d) ≈ 𝔼_{→x ∼ 𝓓} 𝔼_{y∼p(y|→x; →θ)} \Big[→d^T s(→θ; →x, y) + \tfrac{1}{2}→d^T →H_{-\log p(y|→x; →θ)} →d \Big] = \tfrac{1}{2}→d^T ⇉F(→θ) →d.$$

---
style: .katex-display { margin: .7em 0 }
# The Direction of the SGD Gradient Update

Consider the
$$\lim_{ε → 0} \frac{1}{ε} \argmin_{→d, \|→d\| ≤ ε} 𝓛(𝔻; →θ + →d).$$

~~~
For a given $ε$, the Taylor expansion of order one gives
$$\argmin_{→d, \|→d\| ≤ ε} 𝓛(𝔻; →θ) + →d^T ∇_{→θ} 𝓛(𝔻; →θ).$$

~~~
We solve the constrained optimum by forming a Lagrangian and computing
a derivative with respect to $→d$:
$$∇_{→d} \big(𝓛(𝔻; →θ) + →d^T ∇_{→θ} 𝓛(𝔻; →θ) + λ(→d^T →d - ε^2)\big) = ∇_{→θ} 𝓛(𝔻; →θ) + 2 λ →d,$$
~~~
resulting in
$$→d ∝ -∇_{→θ} 𝓛(𝔻; →θ),\textrm{~~and therefore,~~} →d = ε \frac{-∇_{→θ} 𝓛(𝔻; →θ)}{\|∇_{→θ} 𝓛(𝔻; →θ)\|}.$$

---
style: .katex-display { margin: .45em 0 }
# The Direction of the FIM Gradient Update

Consider now
$$\argmin_{→d, D_\textrm{KL}(p(y|→x; →θ) \| p(y|→x; →θ + →d)) ≤ ε} 𝓛(𝔻; →θ + →d).$$

~~~
Again applying the Taylor expansion to the loss term, forming a Lagrangian, and
utilizing the fact that $𝔼_{→x ∼ 𝓓} D_\textrm{KL}(p(y|→x; →θ) \| p(y|→x; →θ + →d)) ≈ \tfrac{1}{2}→d^T ⇉F(→θ) →d,$
~~~
we get
$$∇_{→d} \big(𝓛(𝔻; →θ) + →d^T ∇_{→θ} 𝓛(𝔻; →θ) + λ(\tfrac{1}{2} →d^T ⇉F(→θ) →d - ε)\big),$$

~~~
resulting in
$$∇_{→θ} 𝓛(𝔻; →θ) + λ ⇉F(→θ) →d,$$

~~~
obtaining
$$→d ∝ - ⇉F(→θ)^{-1} ∇_{→θ} 𝓛(𝔻; →θ).$$

~~~
Note that if we consider just the diagonal of $⇉F(→θ)^{-1}$, the resulting
algorithm is similar to Adam.
~~~
Adam also computes a square root and adds $ε$ to the diagonal; both can be
considered dampening (limiting the size of an update when the FIM entries
are very close to zero).

---
section: NPG
class: section
# Natural Policy Gradient

---
# Natural Policy Gradient

Kakade (2002) introduced natural policy gradient, a second-order method
utilizing the Fisher Information Matrix.

~~~
Using policy gradient theorem, we are able to compute $∇ v_π$. Normally, we
update the parameters by using directly this gradient. This choice is justified
by the fact that a vector $→d$ which maximizes $v_π(s; →θ + →d)$ under
the constraint that $\|→d\|^2$ is bounded by a small constant is exactly
the gradient $∇ v_π$.

~~~
However, for the Fisher information matrix
$$⇉F(→θ) ≝ 𝔼_{s} 𝔼_{π(a | s; →θ)} \Big[\big(∇_{→θ} \log π(a|s; →θ)\big) \big(∇_{→θ} \log π(a|s; →θ)\big)^T \Big],$$
we might update the parameters using $→d_F ≝ F(→θ)^{-1} ∇ v_π$.

~~~
It can be shown that the Fisher information metric is the only Riemannian metric
(up to rescaling) invariant to change of parameters under sufficient statistic.

---
# Natural Policy Gradient

![w=82%,h=center](npg.svgz)

~~~
An interesting property of using the $→d_F$ to update the parameters is that
- updating $→θ$ using $∇ v_π$ will choose an arbitrary _better_ action in state
  $s$;
~~~
- updating $→θ$ using $⇉F(→θ)^{-1} ∇ v_π$ chooses the _best_ action (maximizing
  expected return), similarly to tabular greedy policy improvement.

~~~
However, computing $→d_F$ in a straightforward way is too costly.

---
# Truncated Natural Policy Gradient

Duan et al. (2016) in paper _Benchmarking Deep Reinforcement Learning for
Continuous Control_ propose a modification to the NPG to efficiently compute
$→d_F$.

~~~
Following Schulman et al. (2015), they suggest to use _conjugate gradient
algorithm_, which can solve a system of linear equations $⇉A→x = →b$
in an iterative manner, by using $⇉A$ only to compute products $⇉A→v$ for
a suitable $→v$.

~~~
Therefore, $→d_F$ is found as a solution of
$$⇉F(→θ)→d_F = ∇ v_π$$
and using only 10 iterations of the algorithm seem to suffice according to the
experiments.

~~~
Furthermore, Duan et al. suggest to use a specific learning rate suggested by
Peters et al (2008) of
$$\frac{α}{\sqrt{(∇ v_π)^T ⇉F(→θ)^{-1} ∇ v_π}}.$$

---
section: TRPO
class: section
# Trust Region Policy Optimization

---
# Trust Region Policy Optimization

Schulman et al. in 2015 wrote an influential paper introducing TRPO as an
improved variant of NPG.

~~~
Considering two policies $π, π̃$, we can write
$$v_π̃ = v_π + 𝔼_{s ∼ μ(π̃)} 𝔼_{a ∼ π̃(a | s)} a_π(s, a),$$
where $a_π(s, a)$ is the advantage function $q_π(s, a) - v_π(s)$ and
$μ(π̃)$ is the on-policy distribution of the policy $π̃$.

~~~
Analogously to policy improvement, we see that if $𝔼_{a∼π̃} a_π(s, a) ≥0$, policy
$π̃$ performance increases (or stays the same if the advantages are zero
everywhere).

~~~
However, sampling states $s ∼ μ(π̃)$ is costly. Therefore, we instead
consider
$$L_π(π̃) = v_π + 𝔼_{s ∼ μ(π)} 𝔼_{a ∼ π̃(a | s)} a_π(s, a).$$

---
# Trust Region Policy Optimization
$$L_π(π̃) = v_π + 𝔼_{s ∼ μ(π)} 𝔼_{a ∼ π̃(a | s)} a_π(s, a)$$

Using $L_π(π̃)$ is usually justified by $L_π(π) = v_π$ and $∇_π̃ L_π(π̃) |_{π̃ = π} = ∇_π̃ v_π̃ |_{π̃ = π}$.

~~~
Schulman et al. additionally proves that if we denote
$α = D_\textrm{KL}^\textrm{max}(π_\textrm{old} \| π_\textrm{new})
   = \max_s D_\textrm{KL}\big(π_\textrm{old}(⋅|s) \| π_\textrm{new}(⋅|s)\big)$, then
$$v_{π_\textrm{new}} ≥ L_{π_\textrm{old}}(π_\textrm{new}) - \frac{4εγ}{(1-γ)^2}α\textrm{~~~where~~~}ε = \max_{s, a} |a_π(s, a)|.$$

~~~
Therefore, TRPO maximizes $L_{π_{→θ_0}}(π_{→θ})$ subject to
$D_\textrm{KL}(π_{→θ_0} \| π_{→θ}) < δ$, where
- $D_\textrm{KL}^{→θ_0}(π_{→θ_0} \| π_{→θ}) = 𝔼_{s ∼ μ(π_{→θ_0})} [D_\textrm{KL}\big(π_\textrm{old}(⋅|s) \| π_\textrm{new}(⋅|s)\big)]$
  is used instead of $D_\textrm{KL}^\textrm{max}$ for performance reasons;
~~~
- $δ$ is a constant found empirically, as the one implied by the above equation
  is too small;
~~~
- importance sampling is used to account for sampling actions from $π$.

---
# Trust Region Policy Optimization

$$\textrm{maximize}~~L_{π_{→θ_0}}(π_{→θ})
 = 𝔼_{s ∼ μ(π_{→θ_0}), a ∼ π_{→θ_0}(a | s)} \Big[\tfrac{π_{→θ}(a|s)}{π_{→θ_0}(a|s)}a_{π_{→θ_0}}(s, a)\Big]
 ~~\textrm{subject to}~~D_\textrm{KL}(π_{→θ_0} \| π_{→θ}) < δ$$

The parameters are updated using $→d_F = ⇉F(→θ)^{-1} ∇ L_{π_{→θ_0}}(π_{→θ})$, utilizing the
conjugate gradient algorithm as described earlier for TNPG (note that the
algorithm was designed originally for TRPO and only later employed for TNPG).

~~~
To guarantee improvement and respect the $D_\textrm{KL}$ constraint, a line
search is in fact performed. We start by the learning rate of
$\sqrt{δ/(→d_F^T ⇉F(→θ)^{-1} →d_F)}$ and shrink it exponentially until
the constraint is satistifed and the objective improves.

---
# Trust Region Policy Optimization

![w=30%,h=center](rllib_tasks.svgz)

![w=100%](rllib_results.svgz)

---
section: PPO
class: section
# Proximal Policy Optimization

---
# Proximal Policy Optimization

PPO is a simplification of TRPO which can be implemented using a few lines of code.

~~~
Let $r_t(→θ) ≝ \frac{π(A_t|S_t; →θ)}{π(A_t|S_t; →θ_\textrm{old})}$.
~~~
PPO maximizes the objective (i.e., you should minimize its negation)
$$L^\textrm{CLIP}(→θ) ≝ 𝔼_t\Big[\min\big(r_t(→θ) Â_t, \operatorname{clip}(r_t(→θ), 1-ε, 1+ε) Â_t)\big)\Big].$$

~~~
Such a $L^\textrm{CLIP}(→θ)$ is a lower (pessimistic) bound.

![w=60%,h=center](ppo_clipping.svgz)

---
# Proximal Policy Optimization

The advantages $Â_t$ are additionally estimated using the so-called
_generalized advantage estimation_, which is just an analogue
of the truncated n-step lambda return:
$$Â_t = ∑_{i=0}^{n-1} γ^i λ^i \big(R_{t+1+i} + γ V(S_{t+i+1}) - V(S_{t + i})\big).$$

---
# Proximal Policy Optimization – The Algorithm

![w=95%,h=center](ppo_algorithm.svgz)

~~~
- The rollout phase should be usually performed using vectorized environments.

~~~
- It is important to correctly handle episodes that did not finish in a rollout,
  using bootstrapping to estimate the return from the rest of the episode.
~~~
  That way, PPO can learn in long-horizon games with $T$ much smaller than
  episode length.

~~~
- Increasing $N$ increases parallelism, while increasing $T$ increase the number
  of steps that must be performed sequentially.

---
# Proximal Policy Optimization

![w=100%,v=middle](ppo_results.svgz)

---
# Proximal Policy Optimization

Results from the SAC paper:

![w=79%,h=center](../08/sac_results.svgz)

---
# Proximal Policy Optimization

- There are a few tricks that influence the peformance of PPO significantly;
  see the following nice blogpost about many of them:

  https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/

---
# Proximal Policy Optimization

- The paper _What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale
  Study_ https://openreview.net/forum?id=nIAxjsniDzg performs a evaluation of
  many hyperparameters of the PPO algorithm.

~~~
  Main takeaways:
~~~
  - Start with clipping threshold 0.25, but try increasing/decreasing it.
~~~
  - Initialization of the last policy layer influences the results considerably;
    recommendation is to use 100 times smaller weights.
~~~
  - Use softplus to parametrize standard deviation of actions, use a negative
    offset to decrease initial standard deviation of actions, tune it if
    possible.
~~~
  - Use $\tanh$ do transform the action distribution instead of clipping.
~~~
  - Do not share weights between the policy and value network; use a wide value
    network.
~~~
  - Always normalize observations; check if normalizing value function helps.
~~~
  - Use GAE with $λ=0.9$, do not use Huber loss. Adam with 3e-4 is a safe choice.
~~~
  - Perform multiple passes over the data, recompute advantages at the beginning
    of every one of them.
~~~
  - The discount factor $γ$ is important, tune it per environment starting with
    $γ=0.99$.

---
section: TransRews
class: section
# Transformed Rewards

---
# Transformed Rewards

So far, we have clipped the rewards in DQN on Atari environments.

~~~
Consider a Bellman operator $𝓣$
$$(𝓣q)(s, a) ≝ 𝔼_{s',r ∼ p} \Big[r + γ \max_{a'} q(s', a')\Big].$$

~~~
Instead of clipping the magnitude of rewards, we might use a function
$h: ℝ → ℝ$ to reduce their scale. We define a transformed Bellman operator
$𝓣_h$ as
$$(𝓣_hq)(s, a) ≝ 𝔼_{s',r ∼ p} \Big[h\Big(r + γ \max_{a'} h^{-1} \big(q(s', a')\big)\Big)\Big].$$

---
# Transformed Rewards

It is easy to prove the following two propositions from a 2018 paper
_Observe and Look Further: Achieving Consistent Performance on Atari_ by Tobias
Pohlen et al.

~~~
1. If $h(z) = α z$ for $α > 0$, then $𝓣_h^k q \xrightarrow{k → ∞} h \circ q_* = α q_*$.

~~~
   The statement follows from the fact that it is equivalent to scaling the
   rewards by a constant $α$.

~~~
2. When $h$ is strictly monotonically increasing and the MDP is deterministic,
   then $𝓣_h^k q \xrightarrow{k → ∞} h \circ q_*$.

~~~
   This second proposition follows from
   $$h \circ q_* = h \circ 𝓣 q_* = h \circ 𝓣(h^{-1} \circ h \circ q_*) = 𝓣_h(h \circ q_*),$$
   where the last equality only holds if the MDP is deterministic.

---
# Transformed Rewards

For stochastic MDP, the authors prove that if $h$ is strictly monotonically
increasing, Lipschitz continuous with Lipschitz constant $L_h$, and has a
Lipschitz continuous inverse with Lipschitz constant $L_{h^{-1}}$, then
for $γ < \frac{1}{L_h L_{h^{-1}}}$, $𝓣_h$ is again a contraction. (Proof
in Proposition A.1.)

~~~
For the Atari environments, the authors propose the transformation
$$h(x) ≝ \sign(x)\left(\sqrt{|x| + 1} - 1\right) + εx$$
with $ε = 10^{-2}$. The additive regularization term ensures that
$h^{-1}$ is Lipschitz continuous.

~~~
It is straightforward to verify that
$$h^{-1}(x) = \sign(x)\left(\left(\frac{\sqrt{1 + 4ε (|x| + 1 + ε)} - 1}{2ε} \right)^2 - 1\right).$$

~~~
In practice, discount factor larger than $\frac{1}{L_h L_{h^{-1}}}$ is being
used; however, it seems to work.

---
section: R2D2
class: section
# Recurrent Replay Distributed DQN (R2D2)

---
# Recurrent Replay Distributed DQN (R2D2)

Proposed in 2019, to study the effects of recurrent state, experience replay and
distributed training.

~~~
R2D2 utilizes prioritized replay, $n$-step double Q-learning with $n=5$,
convolutional layers followed by a 512-dimensional LSTM passed to duelling
architecture, generating experience by a large number of actors (256; each
performing approximately 260 steps per second) and learning from batches in
a single learner (achieving 5 updates per second using mini-batches of 64
sequences of length 80).

~~~
Rewards are transformed instead of clipped, and no loss-of-life-as-episode-end
heuristic is used.

~~~
Instead of individual transitions, the replay buffer consists of fixed-length
($m=80$) sequences of $(s, a, r)$, with adjacent sequences overlapping by 40
time steps.

~~~
The prioritized replay employs a combination of the maximum and the average
absolute 5-step TD errors $δ_i$ over the sequence: $p = η \max_i δ_i + (1 - η)
δ̄$, for both $η$ and the priority exponent set to 0.9.

~~~
Several R2D2 agent videos are available at https://bit.ly/r2d2600.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=95%,mw=65%,h=right,f=right](r2d2_recurrent_staleness.svgz)

When running the recurrent network on a sequence from the replay buffer, two
strategies of initializing the hidden state are considered:
~~~
- **stored-state** uses the hidden state from the training;
- **zero-state** uses 0.

~~~
Furthermore, an optional burn-in of length 0, 20, and 40 (before the 80 states
used during training; only used for obtaining better hidden state) is considered.

~~~
The stored-state and burn-in of length 40 is used during evaluation.

---
# Recurrent Replay Distributed DQN (R2D2)

![w=35%](../01/r2d2_results.svgz)![w=65%](r2d2_result_table.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=100%,v=middle](r2d2_hyperparameters.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

![w=70%,h=center](r2d2_training_progress.svgz)

---
# Recurrent Replay Distributed DQN (R2D2)

Ablations comparing the reward clipping instead of value rescaling
(**Clipped**), smaller discount factor $γ = 0.99$ (**Discount**)
and **Feed-Forward** variant of R2D2. Furthermore, life-loss
**reset** evaluates resetting an episode on life loss, with
**roll** preventing value bootstrapping (but not LSTM unrolling).

![w=85%,h=center](r2d2_ablations.svgz)
![w=85%,h=center](r2d2_life_loss.svgz)

---
# Utilization of LSTM Memory During Inference

![w=100%](r2d2_memory_size.svgz)

The effect of restricting the policy to $k$ steps only (using either
the zero-state or stored-state initialization).

---
class: section
# Agent57

---
# Agent57

The Agent57 is an agent (from Mar 2020) capable of outperforming the standard
human benchmark on all 57 games.

~~~
Its most important components are:
- Retrace; from _Safe and Efficient Off-Policy Reinforcement Learning_ by Munos
  et al., https://arxiv.org/abs/1606.02647,
~~~
- Never give up strategy; from _Never Give Up: Learning Directed Exploration Strategies_
  by Badia et al., https://arxiv.org/abs/2002.06038,
~~~
- Agent57 itself; from _Agent57: Outperforming the Atari Human Benchmark_ by
  Badia et al., https://arxiv.org/abs/2003.13350.

---
section: Retrace
class: section
# Retrace

---
# Retrace

$\displaystyle \mathrlap{𝓡q(s, a) ≝ q(s, a) + 𝔼_b \bigg[∑_{t≥0} γ^t \left(∏\nolimits_{j=1}^t c_t\right)
  \Big(R_{t+1} + γ𝔼_{A_{t+1} ∼ π} q(S_{t+1}, A_{t+1}) - q(S_t, A_t)\Big)\bigg],}$

where there are several possibilities for defining the traces $c_t$:
~~~
- **importance sampling**, $c_t = ρ_t = \frac{π(A_t|S_t)}{b(A_t|S_t)}$,
  - the usual off-policy correction, but with possibly very high variance,
  - note that $c_t = 1$ in the on-policy case;
~~~
- **Tree-backup TB(λ)**, $c_t = λ π(A_t|S_t)$,
  - the Tree-backup algorithm extended with traces,
  - however, $c_t$ can be much smaller than 1 in the on-policy case;
~~~
- **Retrace(λ)**, $c_t = λ \min\big(1, \frac{π(A_t|S_t)}{b(A_t|S_t)}\big)$,
  - off-policy correction with limited variance, with $c_t = 1$ in the on-policy case.

~~~
The authors prove that $𝓡$ has a unique fixed point $q_π$ for any
$0 ≤ c_t ≤ \frac{π(A_t|S_t)}{b(A_t|S_t)}$.

---
section: NGU
class: section
# Never Give Up

---
# Never Give Up

The NGU (Never Give Up) agent performs _curiosity-driver exploration_, and
augment the extrinsic (MDP) rewards with an intrinsic reward. The augmented
reward at time $t$ is then $r_t^β ≝ r_t^e + β r_t^i$, with $β$ a scalar
weight of the intrinsic reward.

~~~
The intrinsic reward fulfills three goals:

~~~
1. quickly discourage visits of the same state in the same episode;

~~~
2. slowly discourage visits of the states visited many times in all episodes;

~~~
3. ignore the parts of the state not influenced by the agent's actions.

~~~
The intrinsic rewards is a combination of the episodic novelty $r_t^\textrm{episodic}$
and life-long novelty $α_t$:
$$r_t^i ≝ r_t^\textrm{episodic} ⋅ \operatorname{clip}\Big(1 ≤ α_t ≤ L=5\Big).$$

---
style: .katex-display { margin: .5em 0 }
# Never Give Up

![w=70%,f=right](ngu_novelty.png)

The episodic novelty works by storing the embedded states $f(S_t)$ visited
during the episode in episodic memory $M$.

~~~
The $r_t^\textrm{episodic}$ is then estimated as

$$r_t^\textrm{episodic} = \frac{1}{\sqrt{\textrm{visit~count~of~}f(S_t)}}.$$

~~~
The visit count is estimated using similarities of $k$-nearest neighbors of $f(S_t)$
measured via an inverse kernel $K(x, z) = \frac{ε}{\frac{d(x, z)^2}{d_m^2} + ε}$ for
$d_m$ a running mean of the $k$-nearest neighbor distance:

$$r_t^\textrm{episodic} = \frac{1}{\sqrt{∑\nolimits_{f_i ∈ N_k} K(f(S_t), f_i)}+c}\textrm{,~~with~pseudo-count~c=0.001}.$$

---
# Never Give Up

![w=70%,f=right](ngu_novelty.png)

The state embeddings are trained to ignore the parts not influenced by the actions of the agent.

~~~

To this end, Siamese network $f$ is trained to predict $p(A_t|S_t, S_{t+1})$,
i.e., the action $A_t$ taken by the agent in state $S_t$ when the resulting
state is $S_{t+1}$.

~~~
The life-long novelty $α_t=1 + \tfrac{\|ĝ - g\|^2 - μ_\textrm{err}}{σ_\textrm{err}}$
is trained using random network distillation (RND),
where a predictor network $ĝ$ tries to predict the output of an untrained
convolutional network $g$ by minimizing the mean squared error; the
$μ_\textrm{err}$ and $σ_\textrm{err}$ are the running mean and standard
deviation of the error $\|ĝ-g\|^2$.

---
# Never Give Up

![w=18%,f=right](ngu_architecture.svgz)

The NGU agent is based on R2D2 with transformed Retrace loss and augmented reward
$$r_t^i ≝ r_t^\textrm{episodic} ⋅ \operatorname{clip}\Big(1 ≤ α_t ≤ L=5\Big).$$

~~~
![w=23%,f=left](ngu_betas_gammas.svgz)

To support multiple policies concentrating either on the extrinsic or the
intrinsic reward, the NGU agent trains a parametrized action-value function $q(s, a, β_i)$
which corresponds to reward $r_t^{β_i}$ for $β_0=0$ and $γ_0=0.997$, …, $β_{N-1}=β$
and $γ_{N-1}=0.99$.

For evaluation, $q(s, a, 0)$ is employed.

---
# Never Give Up

![w=73%,h=center](ngu_results_table.svgz)
![w=75%,h=center](ngu_results.svgz)

---
# Never Give Up Ablations

![w=73%,h=center](ngu_ablations_embeddings.svgz)
![w=64%,h=center](ngu_ablations.svgz)

---
section: Agent57
class: section
# Agent57

---
# Agent57

The Agent57 is an agent (from Mar 2020) capable of outperforming the standard
human benchmark on all 57 games.

~~~
Its most important components are:
- Retrace; from _Safe and Efficient Off-Policy Reinforcement Learning_ by Munos
  et al., https://arxiv.org/abs/1606.02647,
~~~
- Never give up strategy; from _Never Give Up: Learning Directed Exploration Strategies_
  by Badia et al., https://arxiv.org/abs/2002.06038,
~~~
- Agent57 itself; from _Agent57: Outperforming the Atari Human Benchmark_ by
  Badia et al., https://arxiv.org/abs/2003.13350.

---
# Agent57

![w=32%,f=right](agent57_architecture.png)

Then Agent57 improves NGU with:
~~~
- splitting the action-value as $q(s, a, j; →θ) ≝ q(s, a, j; →θ^e) + β_j q(s, a, j; →θ^i)$, where

  - $q(s, a, j; →θ^e)$ is trained with $r_e$ as targets, and
  - $q(s, a, j; →θ^i)$ is trained with $r_i$ as targets.

~~~
- instead of considering all $(β_j, γ_j)$ equal, we train a meta-controller
  using a non-stationary multi-arm bandit algorithm, where arms correspond
  to the choice of $j$ for a whole episode (so an actor first samples a $j$
  using multi-arm bandit problem and then updates it according to the observed
  return), and the reward signal is the undiscounted extrinsic episode return;
  each actor uses a different level of $ε_l$-greedy behavior;

~~~
- $γ_{N-1}$ is increased from $0.997$ to $0.9999$.

---
# Agent57 – Results

![w=35%,h=center](agent57_results.svgz)
![w=89%,h=center](agent57_results_table.svgz)

---
# Agent57 – Ablations

![w=56%](agent57_ablations.svgz)![w=44%](agent57_ablations_arm.svgz)

