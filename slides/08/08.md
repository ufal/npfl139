title: NPFL139, Lecture 8
class: title, langtech, cc-by-sa
# Continuous Action Space: DDPG, TD3, SAC

## Milan Straka

### April 09, 2025

---
section: ContinuousActionSpace
class: section
# Continuous Action Space

---
# Continuous Action Space

Until now, the actions were discrete. However, many environments naturally
accept actions from continuous space. We now consider actions which come
from range $[a, b]$ for $a, b ∈ ℝ$, or more generally from a Cartesian product
of several such ranges:
$$∏_i \big[a_i, b_i\big].$$

~~~
![w=40%,f=right](normal_distribution.svgz)
A simple way how to parametrize the action distribution is to choose them from
the normal distribution.

Given mean $μ$ and variance $σ^2$, probability density function of $𝓝(μ, σ^2)$
is
$$p(x) ≝ \frac{1}{\sqrt{2 π σ^2}} e^{\large-\frac{(x - μ)^2}{2σ^2}}.$$

---
# Continuous Action Space in Gradient Methods

Utilizing continuous action spaces in gradient-based methods is straightforward.
Instead of the $\softmax$ distribution, we suitably parametrize the action value,
usually using the normal distribution.

~~~
Considering only one real-valued action, we therefore have
$$π(a | s; →θ) ≝ P\Big(a ∼ 𝓝\big(μ(s; →θ), σ(s; →θ)^2\big)\Big),$$
where $μ(s; →θ)$ and $σ(s; →θ)$ are function approximation of mean and standard
deviation of the action distribution.

~~~
The mean and standard deviation are usually computed from the shared
representation, with
- the mean being computed as a usual regression (i.e., one output neuron
  without activation);
~~~
- the standard deviation (which must be positive) being computed again as
  a single neuron, but with either $\exp$ or $\operatorname{softplus}$, where
  $\operatorname{softplus}(x) ≝ \log(1 + e^x)$.

---
# Continuous Action Space in Gradient Methods

During training, we compute $μ(s; →θ)$ and $σ(s; →θ)$ and then sample the action
value (clipping it to $[a, b]$ if required). To compute the loss, we utilize
the probability density function of the normal distribution (and usually also
add the entropy penalty).

~~~
```python
  mus = torch.nn.Linear(..., actions)(hidden_layer)
  sds = torch.nn.Linear(..., actions)(hidden_layer)
  sds = torch.exp(sds)   # or sds = torch.nn.softplus(sds)

  action_dist = torch.distributions.Normal(mus, sds)

  # Loss computed as - log π(a|s) * returns - entropy_regularization
  loss = - action_dist.log_prob(actions) * returns \
         - args.entropy_regularization * action_dist.entropy()
```

---
# Continuous Action Space

When the action consists of several real values, i.e., the action is a suitable
subregion of $ℝ^n$ for $n>1$, we can:
- either use multivariate Gaussian distribution;
~~~
- or factorize the probability into a product of univariate normal
  distributions.
~~~
  - This is the most commonly used approach; we then consider the action
    to be composed of several independent **action components**.

~~~
If modeling the action distribution using a unimodal normal distribution is
insufficient, a mixture of normal distributions (or mixture of logistic) can
be used, capable of representing also multimodal distributions.

---
section: DPG
class: section
# Deterministic Policy Gradient Theorem

---
# Deterministic Policy Gradient Theorem

Combining continuous actions and Deep Q Networks is not straightforward.
In order to do so, we need a different variant of the policy gradient theorem.

~~~
Recall that in policy gradient theorem,
$$∇_{→θ} J(→θ) ∝ 𝔼_{s ∼ μ} \Big[∑\nolimits_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ)\Big].$$

~~~
## Deterministic Policy Gradient Theorem
Assume that the policy $π(s; →θ)$ is deterministic and computes
an action $a∈ℝ$. Further, assume the reward $r(s, a)$ is actually
a deterministic function of the given state-action pair.
Then, under several assumptions about continuousness, the following holds:
$$∇_{→θ} J(→θ) ∝ 𝔼_{s ∼ μ} \Big[∇_{→θ} π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s;→θ)}\Big].$$

The theorem was first proven in the paper Deterministic Policy Gradient Algorithms
by David Silver et al in 2014.

---
# Deterministic Policy Gradient Theorem – Proof

The proof is very similar to the original (stochastic) policy gradient theorem.

~~~
However, we will be exchanging derivatives and integrals, for which we need
several assumptions:
~~~
- we assume that $h(s), p(s' | s, a), ∇_a p(s' | s, a), r(s, a), ∇_a r(s, a),
  π(s; →θ), ∇_{→θ} π(s; →θ)$ are continuous in all parameters and variables;
~~~
- we further assume that $h(s), p(s' | s, a), ∇_a p(s' | s, a), r(s, a), ∇_a
  r(s, a)$ are bounded.

Details (which assumptions are required and when) can be found in Appendix B
of the paper _Deterministic Policy Gradient Algorithms: Supplementary Material_ by
David Silver et al.

---
# Deterministic Policy Gradient Theorem – Proof

$\displaystyle ∇_{→θ} v_π(s) = ∇_{→θ} q_π(s, π(s; →θ))$

~~~
$\displaystyle \phantom{∇_{→θ} v_π(s)} = ∇_{→θ}\Big(r\big(s, π(s; →θ)\big) + ∫_{s'} γp\big(s'| s, π(s; →θ)\big)\big(v_π(s')\big) \d s'\Big)$

~~~
$\displaystyle \phantom{∇_{→θ} v_π(s)} = ∇_{→θ} π(s; →θ) ∇_a r(s, a) \big|_{a=π(s; →θ)} + ∇_{→θ} ∫_{s'} γp\big(s' | s, π(s; →θ)\big) v_π(s') \d s'$

~~~
$\displaystyle \phantom{∇_{→θ} v_π(s)} = ∇_{→θ} π(s; →θ) ∇_a \Big(r(s, a) + ∫_{s'} γp\big(s' | s, a\big) v_π(s') \d s' \Big) \Big|_{a=π(s; →θ)}\\
                    \qquad\qquad\qquad + ∫_{s'} γp\big(s' | s, π(s; →θ)\big) ∇_{→θ} v_π(s') \d s'$

~~~
$\displaystyle \phantom{∇_{→θ} v_π(s)} = ∇_{→θ} π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s; →θ)} + ∫_{s'} γp\big(s' | s, π(s; →θ)\big) ∇_{→θ} v_π(s') \d s'$

~~~
We finish the proof as in the gradient theorem by continually expanding $∇_{→θ} v_π(s')$, getting
$∇_{→θ} v_π(s) = ∫_{s'} ∑_{k=0}^∞ γ^k P(s → s'\textrm{~in~}k\textrm{~steps~}|π) \big[∇_{→θ} π(s'; →θ) ∇_a q_π(s', a)\big|_{a=π(s';→θ)}\big] \d s'$
~~~
and then $∇_{→θ} J(→θ) = 𝔼_{s ∼ h} ∇_{→θ} v_π(s) ∝ 𝔼_{s ∼ μ} \big[∇_{→θ} π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s;→θ)}\big]$.

---
section: DDPG
class: section
# Deep Deterministic Policy Gradients

---
# Deep Deterministic Policy Gradients

Note that the formulation of deterministic policy gradient theorem allows an
off-policy algorithm, because the loss functions no longer depends on actions
sampled from the behavior policy (similarly to how expected Sarsa is also an
off-policy algorithm).

~~~
We therefore train function approximation for both $π(s; →θ)$ and $q(s, a; →θ)$,
training $q(s, a; →θ)$ using a deterministic variant of the Bellman equation
$$q(S_t, A_t; →θ) = 𝔼_{S_{t+1}} \big[r(S_t, A_t) + γ q(S_{t+1}, π(S_{t+1}; →θ))\big]$$
and $π(s; →θ)$ according to the deterministic policy gradient theorem.

~~~
The algorithm was first described in the paper Continuous Control with Deep Reinforcement Learning
by Timothy P. Lillicrap et al. (2015).

~~~
The authors utilize a replay buffer, a target network (updated by exponential
moving average with $τ=0.001$), batch normalization for CNNs, and perform
exploration by adding a Ornstein-Uhlenbeck noise to the predicted actions.
Training is performed by Adam with learning rates of 1e-4 and 1e-3 for the
policy and the critic networks, respectively.

---
# Deep Deterministic Policy Gradients

![w=69%,h=center](ddpg.svgz)

---
# Deep Deterministic Policy Gradients

![w=100%](ddpg_ablation.svgz)

---
# Deep Deterministic Policy Gradients

![w=58%,f=right](ddpg_results.svgz)

Results using low-dimensional (_lowd_) version of the environment, pixel representation
(_pix_) and DPG reference (_cntrl_).

The architecture in the _lowd_ case consists of two hidden layers with 400 and
300 units and ReLU activation, in both the actor and the critic. The actor
additionally uses tanh activation to bound the action in a given range.

In the case of pixel representation, 3 convolution layers with 32 channels
and ReLU activation are used (no pooling), followed by two fully-connected
ReLU-activated layers with 200 units each.

---
# Ornstein-Uhlenbeck Exploration

While it is natural to use Gaussian noise for the exploration policy, the
authors claim that temporally-correlated noise is more effective for physical
control problems with inertia.

~~~
They therefore generate noise using Ornstein-Uhlenbeck process, by computing
$$n_t ← n_{t-1} + θ ⋅ (μ - n_{t-1}) + ε∼𝓝(0, σ^2),$$
utilizing hyperparameter values $θ=0.15$ and $σ=0.2$.

---
# Ornstein-Uhlenbeck Exploration Visualization

![w=50%](ou_wiener_process.svgz)![w=50%](ou_process.svgz)

- On the left, there is a continuous _Wiener process_ (a “brownian path”),
  corresponding to $θ = 0$ and $σ = 1$.
~~~
- On the right, there is Ornstein-Uhlenbeck process example with $θ = σ = 1$ and $μ=0$.
~~~

The gray area corresponds to the standard deviation of $x$ ($n$ in our notation).

---
section: MuJoCo
class: section
# MuJoCo

---
# MuJoCo

![w=94%,h=center](mujoco_environments.svgz)

See the Gymnasium documentation of the
[HalfCheetah](https://gymnasium.farama.org/environments/mujoco/half_cheetah/),
[Hopper](https://gymnasium.farama.org/environments/mujoco/hopper/),
[Walker2D](https://gymnasium.farama.org/environments/mujoco/walker2d/),
[Ant](https://gymnasium.farama.org/environments/mujoco/ant/),
[Humanoid](https://gymnasium.farama.org/environments/mujoco/humanoid/)
environments for a detailed description of observation spaces and action spaces.

---
section: TD3
class: section
# Twin Delayed Deep Deterministic Policy Gradient (TD3)

---
# Twin Delayed Deep Deterministic Policy Gradient

The paper Addressing Function Approximation Error in Actor-Critic Methods by
Scott Fujimoto et al. from February 2018 proposes improvements to DDPG which

~~~
- decrease maximization bias by training two critics and choosing the minimum of
  their predictions;

~~~
- introduce several variance-lowering optimizations:
  - delayed policy updates;
  - target policy smoothing.

~~~

In 2022, together with the SAC algorithm, the TD3 algorithm has been one of the
best algorithms for off-policy continuous-actions RL training.

---
style: .katex-display { margin: 0 }
# TD3 – Maximization Bias

Similarly to Q-learning, the DDPG algorithm suffers from maximization bias.
In Q-learning, the maximization bias was caused by the explicit $\max$ operator.
For DDPG methods, it can be caused by the gradient descent itself.

Let us have a parametric critic $q_{→φ}$ and a policy $π_{→θ}$.
~~~
Consider two different DPG update results:
~~~
- $π_\textit{approx}$, using the parametric critic $q_{→φ}$, in the direction $∇_{→θ} π(s; →θ) ∇_a q_{→φ}(s, a)\big|_{a=π(s;→θ)}$,
~~~
- $π_\textit{true}$, using the (unavailable) real critic $q_π$, in the direction $∇_{→θ} π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s;→θ)}$.
~~~

Because the gradient direction is a local maximizer, for sufficiently small
$α<ε_1$ we have
$$𝔼\big[q_{→θ}(s, π_\textit{approx})\big] ≥ 𝔼\big[q_{→θ}(s, π_\textit{true})\big].$$

~~~
However, for real $q_π$ and for sufficiently small $α<ε_2$, it holds that
$$𝔼\big[q_π(s, π_\textit{true})\big] ≥ 𝔼\big[q_π(s, π_\textit{approx})\big].$$

~~~
Therefore, if $𝔼\big[q_{→θ}(s, π_\textit{true})\big] ≥ 𝔼\big[q_π(s, π_\textit{true})\big]$,
for $α < \min(ε_1, ε_2)$
$$𝔼\big[q_{→θ}(s, π_\textit{approx})\big] ≥ 𝔼\big[q_π(s, π_\textit{approx})\big].$$

---
# TD3 – Maximization Bias

![w=50%](td3_bias.svgz)![w=50%](td3_bias_dqac.svgz)

~~~
Analogously to Double DQN we could compute the learning targets using
the current policy and the target critic, i.e., $r + γ q_{→θ'}(s', π_{→φ(s')})$
(instead of using the target policy and the target critic as in DDPG), obtaining DDQN-AC algorithm.
However, the authors found out that the policy changes too slowly and the target
and current networks are too similar.

~~~
Using the original Double Q-learning, two pairs of actors and critics could be
used, with the learning targets computed by the opposite critic, i.e.,
$r + γ q_{→θ_2}(s', π_{→φ_1}(s'))$ for updating $q_{→θ_1}$. The resulting DQ-AC
algorithm is slightly better, but still suffering from overestimation.

---
# TD3 – Algorithm

The authors instead suggest to employ two critics and one actor. The actor is
trained using one of the critics, and both critics are trained using the same
target computed using the _minimum_ value of both critics as
$$r + γ \min_{i=1,2} q_{→θ'_i}(s', π_{→φ'}(s')).$$
The resulting algorithm is called CDQ – Clipped Double Q-learning.

~~~
Furthermore, the authors suggest two additional improvements for variance
reduction.
- For obtaining higher quality target values, the authors propose to train the
  critics more often. Therefore, critics are updated each step, but the actor
  and the target networks are updated only every $d$-th step ($d=2$ is used in
  the paper).

~~~
- To explicitly model that similar actions should lead to similar results,
  a small random noise is added to the performed actions when computing the
  target value:
  $$r + γ \min_{i=1,2} q_{→θ'_i}(s', π_{→φ'}(s') + ε)~~~\textrm{for}~~~
    ε ∼ \operatorname{clip}(𝓝(0, σ), -c, c),~\textrm{with}~σ=0.2, c=0.5.$$

---
# TD3 – Algorithm

![w=43%,h=center](td3_algorithm.svgz)

---
# TD3 – Hyperparameters

![w=58%,h=center](td3_hyperparameters.svgz)

In TD3, the actor and the critic also use two fully-connected
ReLU-activated layers with 400 and 300 units, respectively.
The actor actions are bounded in a given range using a suitably
scaled tanh activation.

In TD3, the authors state that they also tried the Ornstein-Uhlenbeck noise,
but it provided no benefit compared to $𝓝(0, 0.1)$.

---
# TD3 – Results

![w=70%,h=center](td3_results_curves.svgz)
![w=70%,h=center](td3_results.svgz)

---
# TD3 – Ablations

![w=85%,h=center](td3_ablations.svgz)
![w=85%,h=center](td3_ablations_dqac.svgz)

The AHE is the authors' reimplementation of DDPG using updated architecture,
hyperparameters, and exploration. TPS is Target Policy Smoothing, DP is Delayed
Policy update, and CDQ is Clipped Double Q-learning.

---
# TD3 – Ablations

![w=65%,h=center](td3_ablations_results.svgz)

---
section: SAC
class: section
# Soft Actor Critic

---
# Soft Actor Critic

The paper _Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor_ by Tuomas Haarnoja et al. from Jan 2018
introduces a different off-policy algorithm for continuous action space.

~~~
It was followed by a continuation paper _Soft Actor-Critic Algorithms and
Applications_ in Dec 2018.

~~~
The general idea is to introduce entropy directly in the value function we want
to maximize, instead of just ad-hoc adding the entropy penalty. Such an approach
is an instance of _regularized policy optimization_.

---
# Soft Actor Critic Objective

Until now, our goal was to optimize
$$𝔼_π \big[G_0\big].$$

~~~
Assume the rewards are deterministic and that $μ_π$ is on-policy distribution of
a policy $π$.

In the soft actor-critic, the authors consider infinite-horizon MDPs and propose
to optimize the maximum entropy objective
$$\begin{aligned}
π_* &= \argmax_π 𝔼_{s∼μ_π} \Big[𝔼_{a∼π(s)}\big[r(s, a)\big] + α H(π(⋅|s))\Big] \\
    &= \argmax_π 𝔼_{s∼μ_π, a∼π(s)}\big[r(s, a) - α \log π(a|s)\big].
\end{aligned}$$

~~~
Note that the value of $α$ is dependent on the magnitude of returns and that
for a fixed policy, the entropy penalty can be “hidden” in the reward.

---
# Soft Actor Critic Objective

To maximize the regularized objective, we define the following augmented reward:
$$r_π(s, a) ≝ r(s, a) + 𝔼_{s' ∼ p(s, a)} \big[α H(π(⋅|s'))\big].$$

~~~
From now on, we consider **soft action-value** function corresponding to this
augmented reward.

---
section: SPE
class: section
# Soft Policy Evaluation

---
# Soft Policy Evaluation

Our goal is now to derive **soft policy iteration**, an analogue of policy iteration algorithm.

~~~
We start by considering soft policy evaluation. Let a modified Bellman backup
operator $𝓣_π$ be defined as
$$𝓣_π q(s, a) ≝ r(s, a) + γ 𝔼_{s' ∼ p(s, a)} \big[v(s')\big],$$
where the **soft (state-)value** function $v(s)$ is defined as
$$
v(s) = 𝔼_{a ∼ π} \big[q(s, a)\big] + αH(π(⋅|s)) = 𝔼_{a ∼ π} \big[q(s, a) - α \log π(a|s)\big].$$

~~~
This modified Bellman backup operator corresponds to the usual one for the
augmented rewards $r_π(s, a)$, and therefore the repeated application
$𝓣_π^k q$ converges to $q_π$ according to the original proof.

---
section: SPI
class: section
# Soft Policy Improvement

---
# Soft Policy Improvement

While the soft policy evaluation was a straightforward modification of the
original policy evaluation, the soft policy improvement is quite different.

~~~
Assume we have a policy $π$, its action-value function $q_π$ from the soft
policy evaluation, and we want to improve the policy. Furthermore, we should
select the improved policy from a family of parametrized distributions $Π$.

~~~
We define the improved policy $π'$ as
$$π'(⋅|s) ≝ \argmin_{π̄ ∈ Π} J_π(π̄) ≝ \argmin_{π̄ ∈ Π} D_\textrm{KL}\Bigg( π̄(⋅|s) \Bigg\| \frac{\exp\big(\frac{1}{α} q_π(s, ⋅)\big)}{z_π(s)} \Bigg),$$
where $z_π(s)$ is the partition function (i.e.,  normalization factor such that
the right-hand side is a distribution), which does not depend on the new policy
and thus can be ignored.

---
# Soft Policy Improvement

We now prove that $q_{π'}(s, a) ≥ q_π(s, a)$ for any state $s$ and action $a$.

~~~
We start by noting that $J_π(π') ≤ J_π(π)$, because we can always choose $π$ as
the improved policy.
~~~
Therefore,
$$𝔼_{a∼π'} \big[α\log π'(a|s) - q_π(s, a) + \textcolor{gray}{α\log z_π(s)}\big] ≤
  𝔼_{a∼π} \big[α\log π(a|s) - q_π(s, a) + \textcolor{gray}{α\log z_π(s)}\big],$$

~~~
which results in
$$𝔼_{a∼π'} \big[q_π(s, a) - α\log π'(a|s)\big] ≥ v_π(s).$$

~~~
We now finish the proof analogously to the original one:
$$\begin{aligned}
q_π(s, a) &= r(s, a) + γ𝔼_{s'}[v_π(s')] \\
          &≤ r(s, a) + γ𝔼_{s'}[𝔼_{a'∼π'} [q_π(s', a') - α\log π'(a'|s')] \\
          &… \\
          &≤ q_{π'}(s, a).
\end{aligned}$$

---
# Soft Policy Iteration

The soft policy iteration algorithm alternates between the soft policy
evaluation and soft policy improvement steps.

~~~
The repeated application of these two steps produce better and better policies.
In other words, we get a monotonically increasing sequence of soft action-value
functions.

~~~
If the soft action-value function is bounded (the paper assumes
a bounded reward and a finite number of actions to bound the entropy), the
repeated application converges to some $q_*$, from which we get a $π_*$ using
the soft policy improvement step.

~~~
It remains to show that the $π_*$ is indeed the optimal policy
fulfilling $q_{π_*}(s, a) ≥ q_π(s, a)$.

~~~
However, this follows from the fact that at convergence,
$J_{π_*}(π_*) ≤ J_{π_*}(π)$, and following the same reasoning as in the proof of
the soft policy improvement, we obtain the required $q_{π_*}(s, a) ≥ q_π(s, a)$.

---
# Soft Policy Improvement Derivation

The following derivation is not in the original paper, but it is my
understanding of how the softmax of the action-value function arises.
For simplicity, we assume finite number of actions, but the same
approach can be generalized to continuous actions.

~~~
Assuming we have a policy $π$ and its action-value function $q_π$,
we usually improve the policy using
$$\begin{aligned}
  ν(⋅|s)
  &= \argmax_ν 𝔼_{a∼ν(⋅|s)} \big[q_π(s, a)\big] \\
  &= \argmax_ν ∑\nolimits_a q_π(s, a) ν(a|s) \\
  &= \argmax_ν →q_π(s, ⋅)^T →ν(⋅|s), \\
\end{aligned}$$

~~~
which results in a greedy improvement with the form of
$$ν(s) = \argmax\nolimits_a q_π(s, a).$$

---
# Soft Policy Improvement Derivation

Now consider instead the regularized objective
$$\begin{aligned}
  ν(⋅|s)
  &= \argmax_ν \big( 𝔼_{a∼ν(⋅|s)} \big[q_π(s, a)\big] + αH(ν(⋅|s))\big) \\
  &= \argmax_ν \big(𝔼_{a∼ν} \big[q_π(s, a) - α \log ν(a|s)\big]\big)
\end{aligned}$$

~~~
To maximize it for a given $s$, we form a Lagrangian
$$𝓛 = \Big(∑\nolimits_a ν(a|s) \big(q_π(s, a) - α\log ν(a|s)\big)\Big) - λ\Big(1 - ∑\nolimits_a ν(a|s)\Big).$$

~~~
The derivative with respect to $ν(a|s)$ is
$$\frac{∂𝓛}{∂ν(a|s)} = q_π(s, a) - α\log ν(a|s) - α + λ.$$

~~~
Setting it to zero, we get $α\log ν(a|s) = q_π(s, a) + λ - α$, resulting in $ν(a|s) ∝ e^{\frac{1}{α} q_π(s, a)}$.

---
section: SAC Algorithm
class: section
# Soft Actor Critic Algorithm

---
# Soft Actor Critic Algorithm

Our soft actor critic will be an off-policy algorithm with continuous action
space. The model consist of two critics $q_{→θ_1}$ and $q_{→θ_2}$, two target
critics $q_{→θ̄_1}$ and $q_{→θ̄_2}$, and a single actor $π_{→φ}$.

~~~
The authors state that
- with a single critic, all the described experiments still converge;

~~~
- they adopted the two critics from the TD3 paper;
~~~
- using two critics “significantly speed up training”.

---
# Soft Actor Critic – Critic Training

To train the critic, we use the modified Bellman backup operator, resulting in
the loss
$$J_q(→θ_i) = 𝔼_{s∼μ_π, a∼π_{→φ}(s)} \Big[\big(q_{→θ_i}(s, a) - \big(r(s, a) + γ 𝔼_{s' ∼ p(s, a)} [v_\textrm{min}(s')]\big)\big)^2\Big],$$

~~~
where
$$v_\textrm{min}(s) = 𝔼_{a∼π_{→φ}(s)} \Big[\min_i\big(q_{→θ̄_i}(s, a) \big) - α \log π_{→φ}(a | s)\Big].$$

~~~
The target critics are updated using exponential moving averages with
momentum $τ$.

---
# Soft Actor Critic – Actor Training

The actor is updated by directly minimizing the KL divergence, resulting in the
loss
$$J_π(→φ) = 𝔼_{s∼μ_π, a∼π_{→φ}(s)}\Big[α \log\big(π_{→φ}(a, s)\big) - \min_i\big(q_{→θ_i}(s, a)\big)\Big].$$

~~~
Given that our critics are differentiable, in order to be able to compute the
gradient $∇_{→φ} q_{→θ_i}(s, a)$, we only need to reparametrize the policy as
$$a = f_{→φ}(s, ε).$$

~~~
Specifically, we sample $ε ∼ 𝓝(0, 1)$ and let $f_{→φ}$ produce an unbounded
Gaussian distribution $𝓝\big(μ(s; →φ), σ(s; →φ)^2\big)$, or a diagonal one if
the actions are vectors, with the sampled action $a = μ(s; →φ) + εσ(s; →φ)$.

~~~
Together, we obtain
$$J_π(→φ) = 𝔼_{s∼μ_π, ε∼𝓝(0, 1)}\Big[α \log\big(π_{→φ}(f_{→φ}(s, ε), s)\big) - \min_i\big(q_{→θ_i}(s, f_{→φ}(s, ε))\big)\Big].$$

---
# Soft Actor Critic – Bounding Actions

In practice, the actions need to be bounded.

~~~
The authors propose to apply an invertible squashing function $\tanh$
on the unbounded Gaussian distribution.

~~~
Consider that our policy produces an unbounded action $π(u | s)$.
To define a distribution $π̄(a | s)$ with $a = \tanh(u)$, we need to employ
the change of variables, resulting in
$$π̄(a | s) = π(u | s) \bigg(\frac{∂a}{∂u}\bigg)^{-1} = π(u | s) \bigg(\frac{∂\tanh(u)}{∂u}\bigg)^{-1}.$$

~~~
Therefore, the log-likelihood has quite a simple form
$$\log π̄(a | s) = \log π(u | s) - \log\big(1 - \tanh^2(u)\big).$$

---
# Soft Actor Critic – Automatic Entropy Adjustment

One of the most important hyperparameters is the entropy penalty $α$.

~~~
In the second paper, the authors presented an algorithm for automatic adjustment
of its value.

~~~
Instead of setting the entropy penalty $α$, they propose to specify target
entropy value $𝓗$ and then solve a constrained optimization problem
$$π_* = \argmax_π 𝔼_{s∼μ_π, a∼π(s)} \big[r(s, a)\big]\textrm{~~such that~~}𝔼_{s∼μ_π, a∼π(s)}\big[-\log π(a | s)\big] ≥ 𝓗.$$

~~~
We can then form a Lagrangian with a multiplier $α$
$$𝔼_{s∼μ_π, a∼π(s)} \Big[r(s, a) + α\big(-\log π(a | s) - 𝓗\big)\Big],$$
which should be maximized with respect to $π$ and minimized with respect
to $α ≥ 0$.

---
# Soft Actor Critic – Automatic Entropy Adjustment

To optimize the Lagrangian, we perform _dual gradient descent_, where we
alternate between maximization with respect to $π$ and minimization with respect
to $α$.

~~~
While such a procedure is guaranteed to converge only under the convexity
assumptions, the authors report that the dual gradient descent works in practice
also with nonlinear function approximation.

~~~
To conclude, the automatic entropy adjustment is performed by introducing
a final loss
$$J(α) = 𝔼_{s∼μ_π, a∼π(s)} \big[-α \log π(a | s) - α 𝓗\big].$$

---
# Soft Actor Critic
![w=93%,h=center](sac_algorithm.svgz)

---
# Soft Actor Critic
![w=93%,h=center](sac_hyperparameters.svgz)

---
# Soft Actor Critic
![w=86%,h=center](sac_results.svgz)

---
# Soft Actor Critic
![w=100%,v=middle](sac_ablations.svgz)

