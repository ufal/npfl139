title: NPFL139, Lecture 5
class: title, langtech, cc-by-sa
# Rainbow II, Distributional RL

## Milan Straka

### March 18, 2024

---
section: $N$-step
# Rainbow DQN Extensions
## Multi-step Learning

Instead of Q-learning, we use $n$-step variant of Q-learning, which estimates
return as
$$âˆ‘_{i=1}^n Î³^{i-1} R_i + Î³^n \max_{a'} Q(s', a'; â†’Î¸Ì„).$$

~~~
This changes the off-policy algorithm to on-policy (because the â€œinnerâ€ actions
are sampled from the behaviour distribution, but should follow the target distribution);
however, it is not discussed in any way by the authors.

---
section: NoisyNets
# Rainbow DQN Extensions

## Noisy Nets

Noisy Nets are neural networks whose weights and biases are perturbed by
a parametric function of a noise.

~~~
The parameters $â†’Î¸$ of a regular neural network are in Noisy nets represented as
$$â†’Î¸ â‰ˆ â†’Î¼ + â†’Ïƒ âŠ™ â†’Îµ,$$
where $â†’Îµ$ is zero-mean noise with fixed statistics. We therefore learn the
parameters $(â†’Î¼, â†’Ïƒ)$.

~~~
A fully connected layer $â†’y = â†’w â†’x + â†’b$ with parameters $(â†’w, â†’b)$ is
represented in the following way in Noisy nets:
$$â†’y = (â†’Î¼_w + â†’Ïƒ_w âŠ™ â†’Îµ_w) â†’x + (â†’Î¼_b + â†’Ïƒ_b âŠ™ â†’Îµ_b).$$

~~~
Each $Ïƒ_{i,j}$ is initialized to $\frac{Ïƒ_0}{\sqrt{n}}$, where $n$ is the number
of input neurons, and $Ïƒ_0$ is a hyperparameter; commonly 0.5.

---
# Rainbow DQN Extensions

## Noisy Nets

The noise $Îµ$ can be for example independent Gaussian noise. However, for
performance reasons, factorized Gaussian noise is used to generate a matrix of
noise. If $Îµ_{i, j}$ is noise corresponding to a layer with $i$ inputs and $j$
outputs, we generate independent noise $Îµ_i$ for input neurons, independent
noise $Îµ_j$ for output neurons, and set
$$Îµ_{i,j} = f(Îµ_i) f(Îµ_j)~~~\textrm{for}~~~f(x) = \operatorname{sign}(x) \sqrt{|x|}.$$
~~~
The authors generate noise samples for every batch, sharing the noise for all
batch instances (consequently, during loss computation, online and target
network use independent noise).

~~~
### Deep Q Networks
When training a DQN, $Îµ$-greedy is no longer used and all policies are greedy,
and all fully connected layers are parametrized as noisy nets (therefore, the
network is thought to generate a distribution of rewards; therefore, greedy
actions still explore).

---
# Rainbow DQN Extensions

## Noisy Nets

![w=50%,h=center](dqn_noisynets_results.svgz)

![w=65%,h=center](dqn_noisynets_curves.svgz)

---
# Rainbow DQN Extensions

## Noisy Nets

![w=100%](dqn_noisynets_noise_study.svgz)

---
section: DistributionalRL
# Rainbow DQN Extensions

## Distributional RL

Instead of an expected return $Q(s, a)$, we could estimate the distribution of
expected returns $Z(s, a)$ â€“ the _value distribution_.

~~~
The authors define the distributional Bellman operator $ğ“£^Ï€$ as:
$$ğ“£^Ï€ Z(s, a) â‰ R(s, a) + Î³ Z(S', A')~~~\textrm{for}~~~S'âˆ¼p(s, a), A'âˆ¼Ï€(S').$$

~~~
The authors of the paper prove similar properties of the distributional Bellman
operator compared to the regular Bellman operator, mainly being a contraction
under a suitable metric
~~~
(for Wasserstein metric $W_p$, the authors define
$WÌ„_p(Z_1, Z_2)â‰\sup_{s, a} W_p\big(Z_1(s, a), Z_2(s, a)\big)$ and prove that
$ğ“£^Ï€$ is a Î³-contraction in $WÌ„_p$).

---
class: dbend
# Rainbow DQN Extensions

## Wasserstein Metric

For two probability distributions $Î¼, Î½$, Wasserstein metric $W_p$ is defined as
$$W_p(Î¼, Î½) â‰ \inf_{Î³âˆˆÎ“(Î¼,Î½)} \big(ğ”¼_{(x, y)âˆ¼Î³} \|x-y\|^d\big)^{1/p},$$
~~~
where $Î“(Î¼,Î½)$ is a set of all _couplings_, each being a a joint probability
distribution whose marginals are $Î¼$ and $Î½$, respectively.
~~~
A possible intuition is the optimal transport of probability mass from $Î¼$ to
$Î½$.

~~~
For distributions over reals with CDFs $F, G$, the optimal transport has an
analytic solution:
$$W_p(Î¼, Î½) = \bigg(âˆ«\nolimits_0^1 |F^{-1}(q) - G^{-1}(q)|^p \d q\bigg)^{1/p},~~~~~
  W_1(Î¼, Î½) = âˆ«\nolimits_x \big|F(x)- G(x)\big| \d x,$$
where $F^{-1}$ and $G^{-1}$ are _quantile functions_, i.e., inverse CDFs.

---
# Rainbow DQN Extensions

## Distributional RL

The distribution of returns is modeled as a discrete distribution parametrized
by the number of atoms $N âˆˆ â„•$ and by $V_\textrm{MIN}, V_\textrm{MAX} âˆˆ â„$.
Support of the distribution are atoms
$$\{z_i â‰ V_\textrm{MIN} + i Î”z : 0 â‰¤ i < N\}\textrm{~~~for~}Î”z â‰ \frac{V_\textrm{MAX} - V_\textrm{MIN}}{N-1}.$$

~~~
The atom probabilities are predicted using a $\softmax$ distribution as
$$Z_{â†’Î¸}(s, a) = \left\{z_i\textrm{ with probability }p_i = \frac{e^{f_i(s, a; â†’Î¸)}}{âˆ‘_j e^{f_j(s, a; â†’Î¸)}}\right\}.$$

---
# Rainbow DQN Extensions

## Distributional RL

![w=30%,f=right](dqn_distributional_operator.svgz)

After the Bellman update, the support of the distribution $R(s, a) + Î³Z(s', a')$
is not the same as the original support. We therefore project it to the original
support by proportionally mapping each atom of the Bellman update to immediate
neighbors in the original support.

~~~
$$Î¦\big(R(s, a) + Î³Z(s', a')\big)_i â‰
  âˆ‘_{j=1}^N \left[ 1 - \frac{\left|[r + Î³z_j]_{V_\textrm{MIN}}^{V_\textrm{MAX}}-z_i\right|}{Î”z} \right]_0^1 p_j(s', a').$$

~~~
The network is trained to minimize the Kullbeck-Leibler divergence between the
current distribution and the (mapped) distribution of the one-step update
$$D_\textrm{KL}\Big(Î¦\big(R + Î³Z_{â†’Î¸Ì„}\big(s', \argmax_{a'} ğ”¼Z_{â†’Î¸Ì„}(s', a')\big)\big) \Big\| Z_{â†’Î¸}\big(s, a\big)\Big).$$

---
# Rainbow DQN Extensions

## Distributional RL

![w=50%,h=center](dqn_distributional_algorithm.svgz)


---
# Rainbow DQN Extensions

## Distributional RL

![w=40%,h=center](dqn_distributional_results.svgz)

![w=40%,h=center](dqn_distributional_example_distribution.svgz)

---
# Rainbow DQN Extensions

## Distributional RL

![w=100%](dqn_distributional_example_distributions.svgz)

---
# Rainbow DQN Extensions

## Distributional RL

![w=100%](dqn_distributional_atoms_ablation.svgz)

---
section: Rainbow
# Rainbow Architecture

Rainbow combines all described DQN extensions. Instead of $1$-step updates,
$n$-step updates are utilized, and KL divergence of the current and target
return distribution is minimized:
$$D_\textrm{KL}\Big(Î¦\big({\textstyle âˆ‘}_{i=0}^{n-1} Î³^i R_{t+i+1} + Î³^n Z_{â†’Î¸Ì„}\big(S_{t+n}, \argmax_{a'} ğ”¼Z_{â†’Î¸}(S_{t+n}, a')\big)\big) \Big\| Z(S_t, A_t)\Big).$$

~~~
The prioritized replay chooses transitions according to the probability
$$p_t âˆ D_\textrm{KL}\Big(Î¦\big({\textstyle âˆ‘}_{i=0}^{n-1} Î³^i R_{t+i+1} + Î³^n Z_{â†’Î¸Ì„}\big(S_{t+n}, \argmax_{a'} ğ”¼Z_{â†’Î¸}(S_{t+n}, a')\big)\big) \Big\| Z(S_t, A_t)\Big)^w.$$

~~~
Network utilizes dueling architecture feeding the shared representation $f(s; Î¶)$
into value computation $V(f(s; Î¶); Î·)$ and advantage computation $A_i(f(s; Î¶), a; Ïˆ)$ for atom $z_i$,
and the final probability of atom $z_i$ in state $s$ and action $a$ is computed as
$$p_i(s, a) â‰
  \frac{e^{V_i(f(s; Î¶); Î·) + A_i(f(s; Î¶), a; Ïˆ) - \sum_{a' âˆˆ ğ“} A_i(f(s; Î¶), a'; Ïˆ)/|ğ“|}}
  {\sum_j e^{V_j(f(s; Î¶); Î·) + A_j(f(s; Î¶), a; Ïˆ) - \sum_{a' âˆˆ ğ“} A_j(f(s; Î¶), a'; Ïˆ)/|ğ“|}}.$$

---
# Rainbow Hyperparameters

Finally, we replace all linear layers by their noisy equivalents.

~~~
![w=65%,h=center](rainbow_hyperparameters.svgz)

---
# Rainbow Results

![w=93%,mw=50%,h=center](../04/rainbow_results.svgz)![w=50%](rainbow_table.svgz)

---
# Rainbow Results

![w=93%,mw=50%,h=center](../04/rainbow_results.svgz)![w=93%,mw=50%,h=center](rainbow_results_ablations.svgz)

---
# Rainbow Ablations

![w=90%,h=center](rainbow_ablations.svgz)

---
# Rainbow Ablations

![w=84%,h=center](rainbow_ablations_per_game.svgz)

---
section: Quantile Regression
# Distributional RL with Quantile Regression

Although the authors of C51 proved that the distributional Bellman operator
is a contraction with respect to Wasserstein metric $W_p$, they were not able
to actually minimize it during training; instead, they minimize the KL
divergence between the current value distribution and one-step estimate.

![w=60%,h=center](qr_dqn_c51projection.svgz)

---
# Distributional RL with Quantile Regression

The same authors later proposed a different approach, which actually manages to minimize
the 1-Wasserstein distance.

~~~
In contrast to C51, where $Z(s, a)$ is represented using a discrete distribution
on a fixed â€œcombâ€ support of uniformly spaces locations, we now represent it
as a _quantile distribution_ â€“ as quantiles $Î¸_i(s, a)$ for a fixed
probabilities $Ï„_1, â€¦, Ï„_N$ with $Ï„_i = \frac{i}{N}$.

~~~
![w=37%,f=right](qr_dqn_1wasserstein.svgz)

Formally, we can define the quantile distribution as a uniform combination of
$N$ Diracs:
$$Z_Î¸(s, a) â‰ \frac{1}{N} âˆ‘_{i=1}^N Î´_{Î¸_i(s, a)},$$
~~~
so that the cumulative density function is a step function increasing by
$\frac{1}{N}$ on every quantile $Î¸_i$.

---
# Distributional RL with Quantile Regression

The quantile distribution offers several advantages:

~~~
- a fixed support is no longer required;

~~~
- the projection step $Î¦$ is not longer needed;

~~~
- this parametrization enables direct minimization of the Wasserstein loss.

---
# Distributional RL with Quantile Regression

Recall that 1-Wasserstein distance between two distributions $Î¼, Î½$ can be computed as
$$W_1(Î¼, Î½) = âˆ«\nolimits_0^1 \big|F_Î¼^{-1}(q) - F_Î½^{-1}(q)\big| \d q,$$
where $F_Î¼$, $F_Î½$ are their cumulative density functions.

~~~
For arbitrary distribution $Z$, the we denote the most accurate quantile
distribution as
$$Î _{W_1} Z â‰ \argmin_{Z_Î¸} W_1(Z, Z_Î¸).$$

~~~
In this case, the 1-Wasserstein distance can be written as
$$W_1(Z, Z_Î¸) = âˆ‘_{i=1}^N âˆ«\nolimits_{Ï„_{i-1}}^{Ï„_i} \big|F_Z^{-1}(q) - Î¸_i\big| \d q.$$

---
# Distributional RL with Quantile Regression

It can be proven (Lemma 2 of Dabney et al.: Distributional Reinforcement Learning with Quantile Regression)
that for continuous $F_Z^{-1}$, $W_1(Z, Z_Î¸)$ is minimized by

![w=50%,f=right](qr_dqn_1wasserstein.svgz)

$$\bigg\{Î¸_i âˆˆ â„ \bigg| F_Z(Î¸_i) = \frac{Ï„_{i-1} + Ï„_i}{2}\bigg\}.$$

~~~
We denote the _quantile midpoints_ as
$$Ï„Ì‚_i â‰ \frac{Ï„_{i-1} + Ï„_i}{2}.$$


~~~
In the paper, the authors prove that the composition
$Î _{W_1} ğ“£^Ï€$ is Î³-contraction in $WÌ„_âˆ$, so repeated
application of $Î _{W_1} ğ“£^Ï€$ converges to a unique fixed
point.

---
# Quantile Regression

Our goal is now to show that it is possible to estimate a quantile $Ï„ âˆˆ [0, 1]$
by minimizing a loss suitable for SGD.

~~~
Assume we have samples from a distribution $P$.

~~~
- Minimizing the MSE of $xÌ‚$ and the samples of $P$,
  $$xÌ‚ = ğ”¼_{xâˆ¼P} \big[(xÌ‚ - x)^2\big],$$
  yields the _mean_ of the distribution, $xÌ‚ = ğ”¼_{xâˆ¼P}[x]$.

~~~
  To show that this holds, we compute the derivative of the loss with respect to
  $xÌ‚$ and set it to 0, arriving at
  $$0 = ğ”¼_x [2(xÌ‚ - x)] = 2 ğ”¼_x[xÌ‚] - 2ğ”¼_x[x] = 2\big(xÌ‚ - ğ”¼_x[x]\big).$$

---
# Quantile Regression

Assume we have samples from a distribution $P$ with cumulative density function
$F_P$.

- Minimizing the mean absolute error (MAE) of $xÌ‚$ and the samples of $P$,
  $$xÌ‚ = ğ”¼_{xâˆ¼P} \big[|xÌ‚ - x|\big],$$
~~~
  yields the _median_ of the distribution, $xÌ‚ = F_P^{-1}(0.5)$.

~~~
  We prove this again by computing the derivative with respect to $xÌ‚$, assuming
  the functions are nice enough that the Leibnitz integral rule can be used:

  $\displaystyle \frac{âˆ‚}{âˆ‚xÌ‚} âˆ«_{-âˆ}^{âˆ} P(x) |xÌ‚ - x| \d x = \frac{âˆ‚}{âˆ‚xÌ‚} \bigg[âˆ«_{-âˆ}^{xÌ‚} P(x) (xÌ‚ - x) \d x  + âˆ«_xÌ‚^âˆ P(x) (x - xÌ‚) \d x \bigg]$

~~~
  $\displaystyle \hphantom{\frac{âˆ‚}{âˆ‚xÌ‚} âˆ«_{-âˆ}^{âˆ} P(x) |xÌ‚ - x| \d x} = âˆ«_{-âˆ}^{xÌ‚} P(x) \d x - âˆ«_xÌ‚^âˆ P(x) \d x$

~~~
  $\displaystyle \hphantom{\frac{âˆ‚}{âˆ‚xÌ‚} âˆ«_{-âˆ}^{âˆ} P(x) |xÌ‚ - x| \d x} = 2 âˆ«_{-âˆ}^{xÌ‚} P(x) \d x - 1 = 2 F_P(xÌ‚) - 1.$

---
# Quantile Regression

Assume we have samples from a distribution $P$ with cumulative density function
$F_P$.

- By generalizing the previous result, we can show that for a quantile $Ï„ âˆˆ [0,
  1]$, if
  $$xÌ‚ = ğ”¼_{xâˆ¼P} \big[(x - xÌ‚)([x â‰¥ xÌ‚] - Ï„)\big] â‰ ğ”¼_{xâˆ¼P} \big[Ï_Ï„(x - xÌ‚)\big],$$
  then $xÌ‚ = F_P^{-1}(Ï„)$.
~~~
  Note that $(x - xÌ‚)([x â‰¥ xÌ‚] - Ï„) = |x - xÌ‚| â‹… |[x â‰¥ xÌ‚] - Ï„|$.
~~~
  This loss penalizes overestimation errors with weight $Ï„$, underestimation
  errors with weight $1-Ï„$.

~~~
  $\displaystyle \frac{âˆ‚}{âˆ‚xÌ‚} âˆ«_{-âˆ}^{âˆ} P(x) (x - xÌ‚)([x â‰¥ xÌ‚] - Ï„) \d x =$

~~~
  $\displaystyle \kern2em = \frac{âˆ‚}{âˆ‚xÌ‚} \bigg[(1-Ï„) âˆ«_{-âˆ}^{xÌ‚} P(x) (x - xÌ‚) \d x  - Ï„ âˆ«_xÌ‚^âˆ P(x) (x - xÌ‚) \d x \bigg]$

~~~
  $\displaystyle \kern2em = (\textcolor{magenta}{Ï„} - \textcolor{blue}{1}) âˆ«_{-âˆ}^{xÌ‚} P(x) \d x + \textcolor{magenta}{Ï„} âˆ«_xÌ‚^âˆ P(x) \d x = \textcolor{magenta}{Ï„} - \textcolor{blue}{âˆ«_{-âˆ}^{xÌ‚} P(x) \d x} = Ï„ - F_P(xÌ‚).$


---
# Quantile Regression

Using the quantile regression, when we have a value distribution $Z$, we can
find the most accurate quantile distribution by minimizing
$$âˆ‘_{i=1}^N ğ”¼_{z âˆ¼ Z} \big[Ï_{Ï„Ì‚_i}(z - Î¸)\big].$$

~~~
However, the quantile loss is not smooth around zero, which could limit
performance when training a model. The authors therefore propose the
**quantile Huber loss**, which acts as an asymmetric squared loss
in interval $[-Îº, Îº]$ and fall backs to the standard quantile loss outside this
range.

~~~
Specifically,
$$Ï_Ï„^Îº(z - Î¸) â‰ \begin{cases}
  \big|[z â‰¥ Î¸] - Ï„\big| â‹… \tfrac{1}{2}\big(z - Î¸\big)^2 &~~\textrm{if}~~ |z - Î¸| â‰¤ Îº,\\
  \big|[z â‰¥ Î¸] - Ï„\big| â‹… Îº\big(|z - Î¸| - \tfrac{1}{2}Îº\big) &~~\textrm{otherwise}.\\
  \end{cases}$$
$$

---
# Distributional RL with Quantile Regression

To conclude, in DR-DQN-$Îº$, the network for a given state predicts $â„^{|ğ“|Ã—N}$,
so $N$ quantiles for every action.

~~~
The following loss is used:

![w=65%,h=center](qr_dqn_loss.svgz)

The $q_j$ is just $\frac{1}{N}$.

---
# Distributional RL with Quantile Regression

![w=100%](qr_dqn_approximation_errors.svgz)

Each state transition has probability of 0.1 of moving in a random direction.

---
# Distributional RL with Quantile Regression

![w=90%,h=center](qr_dqn_atari_graphs.svgz)

![w=37%,h=center](qr_dqn_atari_results.svgz)

---
section: Implicit Quantile Regression
# Implicit Quantile Networks for Distributional RL

In IQN (implicit quantile regression), the authors (again the same team as in
C51 and DR-DQN) generalize the value distribution representation to predict
_any given quantile $Ï„$_.

![w=69%,h=center](iqn_architecture_comparison.svgz)

---
# Implicit Quantile Networks for Distributional RL

The quantile $Ï„$ of the value distribution, $Z_Ï„(s, a)$, is modeled as
$$Z_Ï„(s, a) â‰ˆ f\big(Ïˆ(s) âŠ™ Ï†(Ï„)\big)_a.$$

~~~
- Other ways than multiplicative combinations were tried (concat, residual), but
  the multiplicative form delivered the best results.

~~~
- The quantile $Ï„$ is represented using trainable cosine embeddings with
  dimension $n=64$:
  $$Ï†_j(Ï„) â‰ \operatorname{ReLU}\Big(âˆ‘\nolimits_{i=0}^{n-1} \cos(Ï€ i Ï„) w_{i,j} + b_j\Big).$$

---
# Implicit Quantile Networks for Distributional RL

The overall loss is:

![w=60%,h=center](iqn_loss.svgz)

~~~

Note the different roles of $N$ and $N'$.

---
# Implicit Quantile Networks for Distributional RL

![w=75%,f=right](iqn_n_ablations.svgz)

The authors speculate that:
- large $N$ may increase sample complexity (faster
learning),
- larger $N'$ could reduce variance (like a minibatch size).

---
# Implicit Quantile Networks for Distributional RL

![w=75%,h=center](iqn_atari_graphs.svgz)
![w=52%,h=center,mw=65%](iqn_atari_results.svgz)![w=100%,mw=35%](iqn_atari_results_2.svgz)

---
# Implicit Quantile Networks for Distributional RL

The ablation experiments of the quantile representation. A full grid search with
two seeds for every configuration was performed, with the black dots
corresponding to the hyperparameters of IQN.

![w=100%](iqn_hyperparameters.svgz)

~~~
- â€œlearnâ€ is a learnt MLP embedding with a single hidden layer of size $n$;
~~~
- â€œconcatâ€ combines the state and quantile representations by concatenation, not
  $âŠ™$.
