title: NPFL139, Lecture 12
class: title, langtech, cc-by-sa
# MuZero, AlphaZero Policy Target, Gumbel-Softmax, GumbelZero

## Milan Straka

### May 6, 2024

---
section: MuZero
# MuZero

The MuZero algorithm extends the AlphaZero by a **trained model**, alleviating
the requirement for a known MDP dynamics. It is evaluated both on board games
and on the Atari domain.

~~~
At each time-step $t$, for each of $1 â‰¤ k â‰¤ K$ steps, a model $Î¼_Î¸$ with parameters $Î¸$, conditioned on past observations $o_1, â€¦, o_t$ and future actions $a_{t+1}, â€¦, a_{t+k}$, predicts three future quantities:
- the policy $â†’p^k_t â‰ˆ Ï€(a_{t+k+1} | o_1, â€¦, o_t, a_{t+1}, â€¦, a_{t+k})$,
~~~
- the value function $v^k_t â‰ˆ ğ”¼\big[u_{t+k+1} + Î³ u_{t+k+2} + â€¦ | o_1, â€¦, o_t, a_{t+1}, â€¦, a_{t+k}\big]$,
~~~
- the immediate reward $r^k_t â‰ˆ u_{t+k}$,

where $u_i$ are the observed rewards and $Ï€$ is the behaviour policy.

---
section: Î¼0Model
# MuZero

At each time-step $t$ (omitted from now on for simplicity), the model is composed of three components:
a **representation** function, a **dynamics** function, and a **prediction** function.

~~~
- The dynamics function, $(r^k, s^k) â† g_Î¸(s^{k-1}, a^k)$, simulates the MDP
  dynamics and predicts an immediate reward $r^k$ and an internal state $s^k$.
  The internal state has no explicit semantics, its only goal is to accurately
  predict rewards, values, and policies.

~~~
- The prediction function, $(â†’p^k, v^k) â† f_Î¸(s^k)$, computes the policy and the
  value function, similarly as in AlphaZero.

~~~
- The representation function, $s^0 â† h_Î¸(o_1, â€¦, o_t)$, generates an internal
  state encoding the past observations.

---
# MuZero

![w=100%](muzero_overview.svgz)

---
section: Î¼0MCTS
# MuZero â€“ MCTS

The MCTS algorithm is very similar to the one used in AlphaZero, only the
trained model is used. It produces a policy $â†’Ï€_t$ and a value estimate $Î½_t$.

~~~
- All actions, including the invalid ones, are allowed at any time, except at
  the root, where the invalid actions (available from the current state) are
  disallowed.

~~~
- No states are considered terminal during the search.

~~~
- During the backup phase, we consider a general discounted bootstrapped return
  $$G_k = âˆ‘\nolimits_{t=0}^{l-k-1} Î³^t r_{k+1+t} + Î³^{l-k} v_l.$$

~~~
- Furthermore, the expected return is generally unbounded. Therefore, MuZero
  normalize the Q-value estimates to $[0, 1]$ range by using the minimum and
  maximum the values observed in the search tree until now:
  $$QÌ„(s, a) = \frac{Q(s, a) - \min_{s',a' âˆˆ \mathrm{Tree}} Q(s', a')}{\max_{s',a' âˆˆ \mathrm{Tree}} Q(s', a') - \min_{s',a' âˆˆ \mathrm{Tree}} Q(s', a')}.$$

---
# MuZero â€“ Action Selection

To select a move, we employ a MCTS algorithm and then sample
an action from the obtained policy, $a_{t+1} âˆ¼ â†’Ï€_t$.

~~~
For games, the same strategy of sampling the actions $a_t$ as in AlphaZero is used.
~~~
In the Atari domain, the actions are sampled according to the visit counts for the
whole episode, but with a given temperature $T$:
$$Ï€(a|s) = \frac{N(s, a)^{1/T}}{âˆ‘_b N(s, b)^{1/T}},$$
where $T$ is decayed during training â€“ for first 500k steps it is 1, for the
next 250k steps it is 0.5 and for the last 250k steps it is 0.25.

~~~
While for the board games 800 simulations are used during MCTS, only 50 are used
for Atari.

~~~
In case of Atari, the replay buffer consists of 125k sequences of 200 actions.

---
section: Î¼0Training
# MuZero â€“ Training

During training, we utilize a sequence of $K$ moves. We estimate the return
using bootstrapping as $z_t = u_{t+1} + Î³ u_{t+2} + â€¦ + Î³^{n-1} u_{t+n} + Î³^n Î½_{t+n}$.
The values $K=5$ and $n=10$ are used in the paper, with batch size 2048 for the
board games and 1024 for Atari.

~~~
The loss is then composed of the following components:
$$ğ“›_t(Î¸) = âˆ‘_{k=0}^K ğ“›^r (u_{t+k}, r_t^k) + ğ“›^v(z_{t+k}, v^k_t) + ğ“›^p(â†’Ï€_{t+k}, â†’p^k_t) + c \|\theta\|^2.$$

~~~
Note that in Atari, rewards are scaled by $\sign(x)\big(\sqrt{|x| + 1} - 1\big) + Îµx$ for $Îµ=10^{-3}$,
and authors utilize a cross-entropy loss with 601 categories for values $-300, â€¦, 300$, which they claim
to be more stable (this can be considered distributional RL).

~~~
Furthermore, in Atari the discount factor $Î³=0.997$ is used, and the replay buffer elements
are sampled according to prioritized replay with priority $âˆ |Î½-z|^Î±$, and importance sampling
with exponent $Î²$ is used to account for changing the sampling distribution
($Î±=Î²=1$ is used).

---
# MuZero

$$\begin{aligned}
&\text{Model} \\
&\left. \begin{array}{ll}
  s^0 &= h_Î¸(o_1, ..., o_t) \\
  r^k, s^k &= g_Î¸(s^{k-1}, a^k) \\
  â†’p^k, v^k &= f_Î¸(s^k)
\end{array} \right\} \;\; â†’p^k, v^k, r^k = Î¼_Î¸(o_1, ..., o_t, a^1, ..., a^k)\\
\\
&\text{Search}\\
Î½_t, â†’Ï€_t &= MCTS(s^0_t, Î¼_Î¸) \\
a_t &âˆ¼ â†’Ï€_t
\end{aligned}$$

---
# MuZero

$$\begin{aligned}
&\text{Learning Rule} \\
â†’p^k_t, v^k_t, r^k_t &= Î¼_Î¸(o_1, â€¦, o_t, a_{t+1}, ..., a_{t+k}) \\
z_t &= \left\{\begin{array}{lr}
  u_T & \text{ for games } \\
  u_{t+1} + Î³ u_{t+2} + ... + Î³^{n-1} u_{t+n} + Î³^n Î½_{t+n} & \text{ for general MDPs }
\end{array}\right. \\
ğ“›_t(Î¸) &= âˆ‘_{k=0}^K ğ“›^r (u_{t+k}, r_t^k) + ğ“›^v(z_{t+k}, v^k_t) + ğ“›^p(â†’Ï€_{t+k}, â†’p^k_t)  + c \|Î¸\|^2 \\

&\text{Losses} \\
ğ“›^r(u, r) &= \left\{ \begin{array}{lr} 0 & \text{ for games } \\ -â†’Ï†(u)^T \log â†’Ï†(r) & \text{ for general MDPs } \end{array} \right. \\
ğ“›^v(z, q) &= \left\{ \begin{array}{lr} (z - q)^2 & \text{ for games } \\ -â†’Ï†(z)^T \log â†’Ï†(q) & \text{ for general MDPs } \end{array} \right. \\
ğ“›^p(â†’Ï€, p) &= -â†’Ï€^T \log â†’p
\end{aligned}$$

---
# MuZero â€“ Evaluation

![w=100%](muzero_evaluation.svgz)

---
# MuZero â€“ Atari Results

![w=100%](muzero_atari.svgz)

~~~
MuZero Reanalyze is optimized for greater sample efficiency. It revisits
past trajectories by re-running the MCTS using the network with the latest
parameters, notably
~~~
- using the fresh policy as target in 80\% of the training updates, and
~~~
- always using the fresh $v^k â† f_Î¸(s^k)$ in the bootstrapped target $z_t$.

~~~
Some hyperparameters were changed too â€“ 2.0 samples were drawn per state
instead of 0.1, the value loss was weighted down to 0.25, and the $n$-step
return was reduced to $n=5$.

---
# MuZero â€“ Planning Ablations

![w=65%,h=center](muzero_planning_ablations.svgz)

---
# MuZero â€“ Planning Ablations

![w=67%,h=center](muzero_planning_ablations_2.svgz)

---
# MuZero â€“ Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_1.svgz)

---
# MuZero â€“ Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_2.svgz)

---
section: AlphaZero Policy Target
# AlphaZero as Regularized Policy Optimization

Recall that in AlphaZero, actions are selected according to a variant of PUCT
algorithm:
$$a^* = \argmax\nolimits_a \bigg(Q(s, a) + C(s) P(s, a) \frac{\sqrt{N(s)}}{1 + N(s, a)}\bigg),$$
with a slightly time-increasing exploration rate
$C(s) = \log\left(\frac{1+N(s)+19625}{19625}\right) + 1.25 â‰ˆ 1.25$.

~~~
The paper _Jean-Bastien Grill et al.: Monte-Carlo Tree Search as Regularized
Policy Optimization_, the authors have shown how to interpret this algorithm
as a regularized policy optimization.


---
# AlphaZero as Regularized Policy Optimization

Policy optimization is usually an iterative procedure, which in every step
improves a current policy $Ï€_{â†’Î¸_0}$ according to
$$Ï€_{â†’Î¸'} â‰ \argmax_{â†’y âˆˆ ğ“¢} â†’q_{Ï€_{â†’Î¸_0}}^T â†’y - ğ“¡(â†’y, Ï€_{â†’Î¸_0}),$$
where $ğ“¢$ is a $|ğ“|$-dimensional simplex and $ğ“¡: ğ“¢^2 â†’ â„$ is an optional
(usually convex) regularization term.

~~~
- with $ğ“¡ = 0$, the above reduces to policy iteration (used for example in DQN);
~~~
- with $ğ“¡ = 0$, if the policy is updated using a single gradient step, the
  algorithm reduces to policy gradient;
~~~
- when $ğ“¡(â†’y, Ï€_{â†’Î¸_0}) = -H(â†’y)$, we recover the Soft Actor Critic objective;
~~~
- for $ğ“¡(â†’y, Ï€_{â†’Î¸_0}) = D_\textrm{KL}(Ï€_{â†’Î¸_0} \| â†’y)$ we get an analogue of
  the TRPO objective, which motivated PPO;
~~~
- the MPO algorithm (which we did not discuss) employs $ğ“¡(â†’y, Ï€_{â†’Î¸_0}) = D_\textrm{KL}(â†’y \| Ï€_{â†’Î¸_0})$.

---
# AlphaZero as Regularized Policy Optimization

Let us define the **empirical visit distribution** $Ï€Ì‚$ as
$$Ï€Ì‚(a|s) â‰ \frac{1 + N(s, a)}{|ğ“| + âˆ‘_b N(s, b)}.$$

~~~
The added plus ones makes the following analysis easier, but are not strictly
necessary.

~~~
We also define the **multiplier** $Î»_N$ as
$$Î»_N(s) â‰ C(s) â‹… \frac{\sqrt{âˆ‘_b N(s, b)}}{|ğ“| + âˆ‘_b N(s, b)}.$$

~~~
With these definitions, we can rewrite the AlphaZero action selection to
$$a^* = \argmax\nolimits_a \bigg(Q(s, a) + Î»_N â‹… \frac{Ï€_{â†’Î¸}(a|s)}{Ï€Ì‚(a|s)}\bigg).$$

---
# AlphaZero as Regularized Policy Optimization

$$a^* = \argmax\nolimits_a \bigg(Q(s, a) + Î»_N â‹… \frac{Ï€_{â†’Î¸}(a|s)}{Ï€Ì‚(a|s)}\bigg)$$

For notational simplicity, we will represent $Q(s, a)$ as a vector $â†’q$, where
$q_a = Q(s, a)$, and similarly the policies as $â†’Ï€_{â†’Î¸}$, $â†’Ï€Ì‚$.

~~~
Furthermore, for two vectors $â†’a, â†’b$, let $\frac{â†’a}{â†’b}$ denote element-wise
division with $(\frac{â†’a}{â†’b})_i â‰ \frac{a_i}{b_i}.$ 

~~~
With this notation, the action selection can be succinctly written as
$$a^* = \argmax\nolimits_a \Big(â†’q + Î»_N \frac{â†’Ï€_{â†’Î¸}}{â†’Ï€Ì‚}\Big).$$

---
# AlphaZero as Regularized Policy Optimization

Let $â†’Ï€Ì„$ be the solution of the following objective:
$$â†’Ï€Ì„ â‰ \argmax\nolimits_{â†’y âˆˆ ğ“¢} \Big(â†’q^T â†’y - Î»_N D_\textrm{KL}(â†’Ï€_{â†’Î¸} \| â†’y)\Big).$$

~~~
It can be shown that this solution can be computed explicitly as
$$â†’Ï€Ì„ = Î»_N \frac{â†’Ï€_{â†’Î¸}}{Î± - â†’q},$$
where $Î± âˆˆ â„$ is set such that the result is a proper distribution.
~~~
- Note that $Î± â‰¥ \max_{bâˆˆğ“} \big(q_b + Î»_N Ï€_{â†’Î¸}(b)\big)$, because $Ï€Ì„(a)$ must
  be at most 1.
~~~
- Furthermore, $Î± â‰¤ \max_{bâˆˆğ“} (q_b) + Î»_N$, because we need $âˆ‘_a Ï€Ì„(a) = 1$
  and we combine $âˆ‘_a \frac{Î»_N Ï€_{â†’Î¸}(a)}{\max_b (q_b) + Î»_N - q_a} â‰¤ âˆ‘_a \frac{Î»_N Ï€_{â†’Î¸}(a)}{Î»_N} = 1$
  with the fact that $âˆ‘_a \frac{Î»_N Ï€_{â†’Î¸}(a)}{Î± - q_a}$ is a decreasing
  function of $Î± â‰¥ \max_b q_b$.

~~~
Note the $Î»_N â‰ˆ 1/\sqrt N$ decreasing the regularization for increasing number
of simulations.

---
# AlphaZero as Regularized Policy Optimization

The action $a^*$ selected by the AlphaZero algorithm fulfills
$$a^* = \argmax\nolimits_a \bigg(\frac{âˆ‚}{âˆ‚ N(s, a)} \Big(â†’q^T â†’Ï€Ì‚ - Î»_N D_\textrm{KL}(â†’Ï€_{â†’Î¸} \| â†’Ï€Ì‚)\Big)\bigg).$$

~~~
In other words, $Ï€Ì‚$ â€œtracksâ€ $Ï€Ì„$.

~~~
Furthermore, it can be also shown that for the selected action $a^*$,
$$Ï€Ì‚(a^*|s) â‰¤ Ï€Ì„(a^* | s),$$
until in the limit, the two distributions coincide.

---
# AlphaZero as Regularized Policy Optimization

The $Ï€Ì„$ can be used in the AlphaZero algorithm in several ways:

- **Act**: the action in self-play games could be sampled according to
  $Ï€Ì„(â‹…|s_\textrm{root})$ instead of $Ï€Ì‚$;

~~~
- **Search**: during search, we could sample the actions stochastically
  according to $Ï€Ì„$ instead of the PUCT rule;
~~~
- **Learn**: we could use $Ï€Ì„$ as the target policy during training instead
  of $Ï€Ì‚$;
~~~
- **All**: all of the above.

---
# AlphaZero as Regularized Policy Optimization

![w=50%](muzero_regpolopt_cheetah.svgz)![w=50%](muzero_regpolopt_cheetah_curves.svgz)

---
# AlphaZero as Regularized Policy Optimization

![w=53%](muzero_regpolopt_mcpacman.svgz)![w=46%](muzero_regpolopt_atari.svgz)

---
# AlphaZero as Regularized Policy Optimization

![w=100%,v=middle](muzero_regpolopt_components.svgz)

---
section: Gumbel-Softmax
# Gumbel-Softmax

The **Gumbel-softmax** distribution was proposed independently in two papers
in Nov 2016 (under the name of **Concrete** distribution in the other paper).

~~~
It is a continuous distribution over the simplex (over categorical
distributions) that can approximate _sampling_ from a categorical distribution.

~~~
Let $z$ be a categorical variable with class probabilities $â†’p = (p_1, p_2, â€¦, p_K)$.

~~~
The Gumbel-Max trick (based on a 1954 theorem from E. J. Gumbel) states that
we can draw samples $z âˆ¼ â†’p$ using
$$z = \operatorname{one-hot}\Big(\argmax_i \big(g_i + \log p_i\big)\Big),$$
where $g_i$ are independent samples drawn from the $\operatorname{Gumbel}(0, 1)$
distribution.

To sample $g$ from the distribution $\operatorname{Gumbel}(0, 1)$, we can sample
$u âˆ¼ U(0, 1)$ and then compute $g = -\log(-\log u)$.


---
class: dbend
# Gumbel Distribution

First recall that exponential distribution $\operatorname{Exp}(Î»)$ has
$$\operatorname{PDF}(x; Î») = Î» e^{-Î»x},~~~\operatorname{CDF}(x; Î») = 1 - e^{-Î»x}.$$

~~~
The standard $\operatorname{Gumbel}(0, 1)$ distribution has
$$\operatorname{PDF}(x) = e^{-x - e^{-x}},~~~\operatorname{CDF}(x) = e^{-e^{-x}}.$$

~~~
The Gumbel distribution can be used to model the distribution of maximum
of a number of samples from the exponential distribution:
~~~
if $xÌƒ$ is a maximum of $N$ samples from the $\operatorname{Exp}(1)$
distribution, we get that
$$P(xÌƒ - \log N â‰¤ x)
  = P(xÌƒ â‰¤ x + \log N)
  = \operatorname{CDF}_{\operatorname{Exp}(1)}\big(x + \log N\big)^N
  = \Big(1 - \frac{e^{-x}}{N}\Big)^N,$$
~~~
which converges to $e^{-e^{-x}} = \operatorname{CDF}_{\operatorname{Gumbel}(0, 1)}(x)$ for $N â†’ âˆ$.

---
class: dbend
# Gumbel-Max Trick Proof

To prove the Gumbel-Max trick, we first reformulate it slightly.

Let $l_i$ be logits of a categorical distribution (so that the class probabilities
$Ï€_i âˆ e^{l_i}$), and let $g_i âˆ¼ \operatorname{Gumbel}(0, 1)$. Then
$$Ï€_k = P\big(k = \argmax\nolimits_i (g_i + l_i)\big).$$

~~~
We first observe that the theorem is invariant to a scalar shift of logits,
so we can without loss of generality assume that $âˆ‘_i e^{l_i} = 1$ and $Ï€_i
= e^{l_i}$.

~~~
For convenience, denote $u_i â‰ g_i + l_i$.

~~~
We will use both the $\operatorname{PDF}$ and $\operatorname{CDF}$ of
a $\operatorname{Gumbel}(0, 1)$ distribution:
$$\begin{aligned}
  \operatorname{PDF}(g_i) &= e^{-g_i - e^{-g_i}}, \\
  \operatorname{CDF}(g_i) &= e^{-e^{-g_i}}.
\end{aligned}$$

---
class: dbend
# Gumbel-Max Trick Proof

To finish the proof, we compute

$\displaystyle\kern12em{}\mathllap{P\big(k = \argmax\nolimits_i (g_i + l_i)\big)} = P(u_k â‰¥ u_i, âˆ€_{iâ‰ k})$

~~~
$\displaystyle\kern12em{} = âˆ« P(u_k) âˆ\nolimits_{iâ‰ k} P(u_k â‰¥ u_i | u_k) \d u_k$

~~~
$\displaystyle\kern12em{} = âˆ« P(g_k | g_k = u_k - l_k) âˆ\nolimits_{iâ‰ k} P(g_i â‰¤ u_k - l_i | u_k) \d u_k$

~~~
$\displaystyle\kern12em{} = âˆ« e^{\textcolor{blue}{l_k}-u_k-e^{\textcolor{darkgreen}{l_k}-u_k}} âˆ\nolimits_{iâ‰ k} e^{-e^{\textcolor{magenta}{l_i}-u_k}} \d u_k$

~~~
$\displaystyle\kern12em{} = âˆ« \textcolor{blue}{Ï€_k}e^{-u_k-\textcolor{darkgreen}{Ï€_k}\textcolor{red}{e^{-u_k}}} âˆ\nolimits_{iâ‰ k} e^{-\textcolor{magenta}{Ï€_i}\textcolor{red}{e^{-u_k}}} \d u_k$

~~~
$\displaystyle\kern12em{} = Ï€_k âˆ« e^{-u_k-\textcolor{red}{e^{-u_k}} âˆ‘_i Ï€_i} \d u_k$

~~~
$\displaystyle\kern12em{} = Ï€_k âˆ« e^{-g_k-e^{-g_k}} \d g_k = Ï€_k â‹… 1 = Ï€_k.$

---
# Gumbel-Softmax

To obtain a continuous distribution, we relax the $\argmax$ into a $\softmax$
with temperature $T$ as
$$z_i = \frac{e^{(g_i + \log p_i)/T}}{âˆ‘_j e^{(g_j + \log p_j)/T}}.$$

~~~
As the temperature $T$ goes to zero, the generated samples become one-hot, and
therefore the Gumbel-softmax distribution converges to the categorical
distribution $p(z)$.

![w=74%,h=center](gumbel_temperatures.svgz)

---
section: GumbelZero
# Gumbel AlphaZero and MuZero

In AlphaZero, using the MCTS visit counts as the target policy fails to improve
the policy for small number of visits.

<br>
![w=70%,h=center](gumbelzero_9x9_go_simulations.svgz)

~~~
In _Ivo Danihelka et al.: Policy Improvement by Planning with Gumbel_, several
AlphaZero/MuZero improvements are proposed; among other a different target
policy, which guarantees improvement.

---
# Gumbel AlphaZero and MuZero

Let $Ï€$ be a categorical distributions parametrized with $\mathit{logits}(a)$.
Let $â†’g âˆˆ â„^k$ be a vector of independent Gumbel(0, 1) random variables.

~~~
The Gumbel-Max trick states that
$$A = \argmax\nolimits_a \big(g(a) + \mathit{logits}(a)\big)$$
has a distribution $A âˆ¼ Ï€$.

~~~
The Gumbel-Max trick can be generalized to **Gumbel-Top-k** trick, capable of
producing $n$ actions without replacement by considering the top $n$ scoring
actions $\operatorname{argtop}(â†’g + \boldsymbol{logits}, n)$:
$$\begin{aligned}
  A_1 &= \argmax\nolimits_a \big(g(a) + \mathit{logits}(a)\big), \\
  A_2 &= \argmax\nolimits_{a â‰  A_1} \big(g(a) + \mathit{logits}(a)\big), \\
  \vdots \\
  A_n &= \argmax\nolimits_{a \notâˆˆ \{A_1, â€¦, A_{n-1}\}} \big(g(a) + \mathit{logits}(a)\big). \\
\end{aligned}$$

---
# GumbelZero, A. Guaranteed Policy Improvement

## A. Guaranteed Policy Improvement

For a small number of simulations, PUCT does not guarantee policy improvement.
~~~
- Consider for example three actions with prior policy $(50\%, 30\%, 20\%)$ and
action values $(0, 0, 1)$.
~~~
- The PUCT rule will select the first two actions.
~~~
- However, the value function of any policy considering just the first two actions
  is 0, which is worse than the value function of the prior policy.

~~~
In GumbelZero, we start by sampling $n$ actions without replacement using the
Gumbel-Max trick with Gumbel noise $â†’g$.

~~~
Our first attempt is to define a one-hot policy selecting an action $A_{n+1}$
such that
$$A_{n+1} = \argmax_{a âˆˆ \{A_1, â€¦, A_n\}} \big(g(a) + \mathit{logits}(a) + Ïƒ(q(a))\big),$$
where $Ïƒ$ can be any monotonically increasing transformation.

---
# GumbelZero, A. Guaranteed Policy Improvement

![w=100%,v=middle](gumbelzero_policy_improvement_algorithm.svgz)

---
# GumbelZero, A. Guaranteed Policy Improvement

A policy choosing the action
$$A_{n+1} = \argmax_{a âˆˆ \{A_1, â€¦, A_n\}} \big(g(a) + \mathit{logits}(a) + Ïƒ(q(a))\big),$$
guarantees policy improvement, i.e., $ğ”¼[q(A_{n+1})] â‰¥ ğ”¼_{aâˆ¼Ï€}[q(a)]$.

~~~
Considering a fixed Gumbel noise $â†’g$, we get that
$$q\big(\argmax_{a âˆˆ \{A_1, â€¦, A_n\}} g(a) + \mathit{logits}(a) + Ïƒ(q(a))\big) â‰¥ q\big(\argmax_{a âˆˆ \{A_1, â€¦, A_n\}} g(a) + \mathit{logits}(a)\big),$$
~~~
- either the action chosen on both sides is the same and we get an equality, or
~~~
- the action on the left side is different, meaning it has larger $q(a)$.

~~~
Finally, if the inequality holds for any $â†’g$, it holds also in expectation.
With the Gumbel-Max trick transforming the expectation of the right side to sampling
an action $a âˆ¼ Ï€$, we get the required $ğ”¼[q(A_{n+1})] â‰¥ ğ”¼_{aâˆ¼Ï€}[q(a)]$.

---
# GumbelZero, B. Planning on Stochastic Bandit

## B. Planning on Stochastic Bandit

When we get only an estimate $qÌ‚(a)$ of the action-value function, it is probably
beneficial to visit an action multiple times.

~~~
Furthermore, choosing actions in the root using a UCB-like rule is not optimal:
~~~
- UCB minimizes cumulative regret, i.e., maximizes the sum of the obtained
  returns;
~~~
- in the root our goal is to obtain the best possible $A_{n+1}$, i.e.,
  maximize just $ğ”¼[q(A_{n+1})]$.

~~~
The authors evaluated several simple regret minimization algorithms, and chose
Sequential Halving (because it was easier to tune and does not have
problem-dependent parameters).

---
# GumbelZero, B. Planning on Stochastic Bandit

![w=82%,h=center](gumbelzero_sequential_halving.svgz)

---
# GumbelZero, B. Planning on Stochastic Bandit

![w=100%](gumbelzero_sequential_halving_algorithm.svgz)

The authors utilize $m = \min(n, 16)$, and visit each action at least
once even when $n$ is small by visiting each action
$\max\Big(1, \big\lfloor\frac{n}{\lceil \log_2 m\rceil m}\big\rfloor\Big)$;
after $n$ simulation, the search is always stopped.

---
# GumbelZero, C. Better Improved Policy

Using a one-hot policy based on
$$A_{n+1} = \argmax_{a âˆˆ \{A_1, â€¦, A_n\}} \big(g(a) + \mathit{logits}(a) + Ïƒ(q(a))\big)$$
results in using a simple policy loss
$$L_\textrm{simple}(Ï€) = -\log Ï€(A_{n+1}).$$

---
style: .katex-display { margin: .9em 0 }
# GumbelZero, C. Better Improved Policy

However, more information from the search might be extracted by using all action-value functions
$q(a)$ produced by the search.

~~~
- First, we complete the action values using
  $$\mathit{completedQ}(a) â‰ \begin{cases} q(a) & \textrm{if~}N(a) > 0, \\ v_Ï€
  & \textrm{otherwise}.\end{cases}$$

~~~
- Then, we define improved policy as
  $$Ï€' = \softmax\big(\mathit{logits}(a) + Ïƒ(\mathit{completedQ}(a))\big).$$

~~~
  It can be again proven (appendix C of the paper) that $Ï€'$ is an improved policy, so $ğ”¼_{aâˆ¼Ï€'}[q(a)] â‰¥ ğ”¼_{aâˆ¼Ï€}[q(a)]$.

~~~
- A natural loss is then
  $$L_\textrm{completed}(Ï€) = D_\textrm{KL}(Ï€' \| Ï€).$$

---
# GumbelZero, C. Better Improved Policy

The authors propose to use
$$Ïƒ\big(qÌ‚(a)\big) â‰ \big(c_\textrm{visit} + \max_b N(b)\big) c_\textrm{scale} qÌ‚(a),$$
for $c_\textrm{visit} = 50$, $c_\textrm{scale} = 1.0$.

~~~
Furthermore, the authors propose a consistent approximation to $v_Ï€$ based on
a network-predicted $vÌ‚_Ï€$ and the $q(a)$ of the visited actions:
$$v_\textrm{mix} â‰ \frac{1}{1 + âˆ‘_b N(b)} \Bigg(vÌ‚_Ï€ + \Big(âˆ‘\nolimits_b N(b)\Big)\frac{âˆ‘_{a, N(a) > 0} Ï€(a) q(a)}{âˆ‘_{a, N(a) > 0} Ï€(a)}\Bigg).$$

~~~
Overall, the algorithm denoted in the paper as Gumbel MuZero utilizes Sequential
Halving with Gumbel and trains using the improved policy combining logits
and action values completed by $v_\textrm{mix}$; otherwise it is the same as
MuZero.

---
# GumbelZero, D. Action Selection in Non-Root Nodes

## D. Action Selection in Non-Root Nodes

We might consider utilizing the improved policy $Ï€'$ also in the non-root nodes,
by for example sampling actions from it. Additionally, the authors provide
a deterministic algorithm of choosing non-root actions minimizing the
difference between $Ï€'$ and the current visit counts:
$$a^* = \argmin_a âˆ‘_b \Bigg(Ï€'(b) - \underbrace{\frac{N(b) + [a = b]}{1 + âˆ‘_c N(c)}}_{\textrm{normalized visit counts if taking~}a}\Bigg)^2.$$

~~~
This formula can be simplified to
$$a^* = \argmax_a \Bigg(Ï€'(a) - \frac{N(a)}{1 + âˆ‘_b N(b)}\Bigg).$$

~~~
When this action selection is used, the authors call the algorithm
Full Gumbel MuZero.

---
class: dbend
# GumbelZero, D. Action Selection in Non-Root Nodes

$\displaystyle \kern3em\mathllap{a^*} = \argmin_a âˆ‘_b \Bigg(Ï€'(b) - \frac{N(b) + [a = b]}{1 + âˆ‘_c N(c)}\Bigg)^2$

~~~
$\displaystyle \kern3em{} = \argmin_a âˆ‘_b \Bigg(\bigg(Ï€'(b) - \frac{N(b)}{1 + âˆ‘_c N(c)}\bigg) - \frac{[a = b]}{1 + âˆ‘_c N(c)}\Bigg)^2$

~~~
$\displaystyle \kern3em{} = \argmin_a âˆ‘_b -2\Bigg(Ï€'(b) - \frac{N(b)}{1 + âˆ‘_c N(c)}\Bigg) \frac{[a = b]}{1 + âˆ‘_c N(c)}$

~~~
$\displaystyle \kern3em{} = \argmin_a -âˆ‘_b \Bigg(Ï€'(b) - \frac{N(b)}{1 + âˆ‘_c N(c)}\Bigg) [a = b]$

~~~
$\displaystyle \kern3em{} = \argmax_a \Bigg(Ï€'(a) - \frac{N(a)}{1 + âˆ‘_b N(b)}\Bigg)$

---
# Gumbel AlphaZero and MuZero

![w=100%](gumbelzero_9x9_go_simulations.svgz)

â€œReplacementâ€ is a Gumbel MuZero ablation sampling actions with replacement.  
â€œTRPO MuZeroâ€, â€œMPO MuZeroâ€ use Act+Search+Learn using the previously described
regularized policy with $D_\textrm{KL}(Ï€ \| Ï€_\textrm{new})$ and
$D_\textrm{KL}(Ï€_\textrm{new} \| Ï€)$ regularizer, respectively.

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_9x9_go_ablations.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_ablations_losses.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_ablations_action_selection.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_ablations_visits.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_go_chess.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_atari.svgz)
