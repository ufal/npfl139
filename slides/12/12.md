title: NPFL139, Lecture 12
class: title, langtech, cc-by-sa
# MuZero, AlphaZero Policy Target, Gumbel-Max, GumbelZero

## Milan Straka

### May 7, 2025

---
section: MuZero
class: section
# MuZero

---
# MuZero

The MuZero algorithm extends the AlphaZero by a **trained model**, alleviating
the requirement for a known MDP dynamics. It is evaluated both on board games
and on the Atari domain.

~~~
At each time-step $t$, for each of $1 ≤ k ≤ K$ steps, a model $μ_θ$ with parameters $θ$, conditioned on past observations $o_1, …, o_t$ and future actions $a_{t+1}, …, a_{t+k}$, predicts three future quantities:
- the policy $→p^k_t ≈ π(a_{t+k+1} | o_1, …, o_t, a_{t+1}, …, a_{t+k})$,
~~~
- the value function $v^k_t ≈ 𝔼\big[u_{t+k+1} + γ u_{t+k+2} + … | o_1, …, o_t, a_{t+1}, …, a_{t+k}\big]$,
~~~
- the immediate reward $r^k_t ≈ 𝔼\big[u_{t+k} | o_1, …, o_t, a_{t+1}, …, a_{t+k}\big]$,

where $u_i$ are the observed rewards and $π$ is the behaviour policy.

---
section: μ0Model
class: section
# MuZero – Model

---
# MuZero

At each time-step $t$ (omitted from now on for simplicity), the model is composed of three components:
a **representation** function, a **dynamics** function, and a **prediction** function.

~~~
- The dynamics function, $(r^k, s^k) ← g_θ(s^{k-1}, a^k)$, simulates the MDP
  dynamics and predicts an immediate reward $r^k$ and an internal state $s^k$.
  The internal state has no explicit semantics, its only goal is to accurately
  predict rewards, values, and policies.

~~~
- The prediction function, $(→p^k, v^k) ← f_θ(s^k)$, computes the policy and the
  value function, similarly as in AlphaZero.

~~~
- The representation function, $s^0 ← h_θ(o_1, …, o_t)$, generates an internal
  state encoding the past observations.

---
# MuZero

![w=100%](muzero_overview.svgz)

---
section: μ0MCTS
class: section
# MuZero – MCTS

---
# MuZero – MCTS

The MCTS algorithm is very similar to the one used in AlphaZero, only the
trained model is used. It produces a policy $→π_t$ and a value estimate $ν_t$.

~~~
- All actions, including the invalid ones, are allowed at any time, except at
  the root, where the invalid actions (available from the current state) are
  disallowed.

~~~
- No states are considered terminal during the search.

~~~
- During the backup phase, we consider a general discounted bootstrapped return
  $$G_k = ∑\nolimits_{t=0}^{l-k-1} γ^t r_{k+1+t} + γ^{l-k} v_l.$$

~~~
- Furthermore, the expected return is generally unbounded. Therefore, MuZero
  normalize the Q-value estimates to $[0, 1]$ range by using the minimum and
  the maximum the values observed in the search tree until now:
  $$Q̄(s, a) = \frac{Q(s, a) - \min_{s',a' ∈ \mathrm{Tree}} Q(s', a')}{\max_{s',a' ∈ \mathrm{Tree}} Q(s', a') - \min_{s',a' ∈ \mathrm{Tree}} Q(s', a')}.$$

---
# MuZero – Action Selection

To select a move, we employ the MCTS algorithm and then sample
an action from the obtained policy, $a_{t+1} ∼ →π_t$.

~~~
For games, the same strategy of sampling the actions $a_t$ as in AlphaZero is used.
~~~
In the Atari domain, the actions are sampled according to the visit counts for the
whole episode, but with a given temperature $T$:
$$π(a|s) = \frac{N(s, a)^{1/T}}{∑_b N(s, b)^{1/T}},$$
where $T$ is decayed during training – for first 500k steps it is 1, for the
next 250k steps it is 0.5 and for the last 250k steps it is 0.25.

~~~
While for the board games 800 simulations are used during MCTS, only 50 are used
for Atari.

~~~
In case of Atari, the replay buffer consists of 125k sequences of 200 actions.

---
section: μ0Training
# MuZero – Training

During training, we utilize a sequence of $K$ moves. We estimate the return
using bootstrapping as $z_t = u_{t+1} + γ u_{t+2} + … + γ^{n-1} u_{t+n} + γ^n ν_{t+n}$.
The values $K=5$ and $n=10$ are used in the paper, with batch size 2048 for the
board games and 1024 for Atari.

~~~
The loss is then composed of the following components:
$$𝓛_t(θ) = ∑_{k=0}^K 𝓛^r (u_{t+k}, r_t^k) + 𝓛^v(z_{t+k}, v^k_t) + 𝓛^p(→π_{t+k}, →p^k_t) + c \|\theta\|^2.$$

~~~
Note that in Atari, rewards are scaled by $\sign(x)\big(\sqrt{|x| + 1} - 1\big) + εx$ for $ε=10^{-3}$,
and authors utilize a cross-entropy loss with 601 categories for values $-300, …, 300$, which they claim
to be more stable (this can be considered distributional RL).

~~~
Furthermore, in Atari the discount factor $γ=0.997$ is used, and the replay buffer elements
are sampled according to prioritized replay with priority $∝ |ν-z|^α$; importance sampling
with exponent $β$ is used to account for changing the sampling distribution
($α=β=1$ is used).

---
# MuZero

$$\begin{aligned}
&\text{Model} \\
&\left. \begin{array}{ll}
  s^0 &= h_θ(o_1, ..., o_t) \\
  r^k, s^k &= g_θ(s^{k-1}, a^k) \\
  →p^k, v^k &= f_θ(s^k)
\end{array} \right\} \;\; →p^k, v^k, r^k = μ_θ(o_1, ..., o_t, a^1, ..., a^k)\\
\\
&\text{Search}\\
ν_t, →π_t &= MCTS(s^0_t, μ_θ) \\
a_t &∼ →π_t
\end{aligned}$$

---
# MuZero

$$\begin{aligned}
&\text{Learning Rule} \\
→p^k_t, v^k_t, r^k_t &= μ_θ(o_1, …, o_t, a_{t+1}, ..., a_{t+k}) \\
z_t &= \left\{\begin{array}{lr}
  u_T & \text{ for games } \\
  u_{t+1} + γ u_{t+2} + ... + γ^{n-1} u_{t+n} + γ^n ν_{t+n} & \text{ for general MDPs }
\end{array}\right. \\
𝓛_t(θ) &= ∑_{k=0}^K 𝓛^r (u_{t+k}, r_t^k) + 𝓛^v(z_{t+k}, v^k_t) + 𝓛^p(→π_{t+k}, →p^k_t)  + c \|θ\|^2 \\

&\text{Losses} \\
𝓛^r(u, r) &= \left\{ \begin{array}{lr} 0 & \text{ for games } \\ -→φ(u)^T \log →φ(r) & \text{ for general MDPs } \end{array} \right. \\
𝓛^v(z, v) &= \left\{ \begin{array}{lr} (z - v)^2 & \text{ for games } \\ -→φ(z)^T \log →φ(v) & \text{ for general MDPs } \end{array} \right. \\
𝓛^p(→π, p) &= -→π^T \log →p
\end{aligned}$$

---
# MuZero – Evaluation

![w=100%](muzero_evaluation.svgz)

---
# MuZero – Atari Results

![w=100%](muzero_atari.svgz)

~~~
MuZero Reanalyze is optimized for greater sample efficiency. It revisits
past trajectories by re-running the MCTS using the network with the latest
parameters, notably
~~~
- using the fresh policy as target in 80\% of the training updates, and
~~~
- always using the fresh $v^k ← f_θ(s^k)$ in the bootstrapped target $z_t$.

~~~
Some hyperparameters were changed too – 2.0 samples were drawn per state
instead of 0.1, the value loss was weighted down to 0.25, and the $n$-step
return was reduced to $n=5$.

---
# MuZero – Planning Ablations

![w=65%,f=right](muzero_planning_ablations.svgz)

~~~
(A) Go evaluation, two trained models, each with 800 simulations corresponding
to 0.1s search.

~~~
(B) Atari evaluation, model trained with 50 simulations.

~~~
(C) Ms. Pac-Man, R2D2 best baseline.

~~~
(D) Ms. Pac-Man, different number of simulations during training, all evaluated
with 50 simulations.

---
# MuZero – Planning Ablations

![w=67%,f=right](muzero_planning_ablations_2.svgz)

(A-B) The search depth in previous figure A and B.

(C-D) Policy improvement when trained with 50 simulations and evaluated using less
simulations, in Ms. Pac-Man and in Go, respectively.

---
# MuZero – Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_1.svgz)

---
# MuZero – Detailed Atari Results

![w=78%,h=center](muzero_atari_detailed_2.svgz)

---
section: AlphaZero Policy Target
class: section
# AlphaZero as Regularized Policy Optimization

---
# AlphaZero as Regularized Policy Optimization

Recall that in AlphaZero, actions are selected according to a variant of PUCT
algorithm:
$$a^* = \argmax\nolimits_a \bigg(Q(s, a) + C(s) P(s, a) \frac{\sqrt{N(s)}}{1 + N(s, a)}\bigg),$$
with a slightly time-increasing exploration rate
$C(s) = \log\left(\frac{1+N(s)+19625}{19625}\right) + 1.25 ≈ 1.25$.

~~~
The paper _Jean-Bastien Grill et al.: Monte-Carlo Tree Search as Regularized
Policy Optimization_, the authors have shown how to interpret this algorithm
as a regularized policy optimization.


---
# AlphaZero as Regularized Policy Optimization

Policy optimization is usually an iterative procedure, which in every step
improves a current policy $π_{→θ_0}$ according to
$$π_{→θ'} ≝ \argmax_{→y ∈ 𝓢} →q_{π_{→θ_0}}^T →y - 𝓡(→y, π_{→θ_0}),$$
where $𝓢$ is a $|𝓐|$-dimensional simplex and $𝓡: 𝓢^2 → ℝ$ is an optional
(usually convex) regularization term.

~~~
- with $𝓡 = 0$, the above reduces to policy iteration (used for example in DQN);
~~~
- with $𝓡 = 0$, if the policy is updated using a single gradient step, the
  algorithm reduces to policy gradient;
~~~
- when $𝓡(→y, π_{→θ_0}) = -H(→y)$, we recover the Soft Actor Critic objective;
~~~
- for $𝓡(→y, π_{→θ_0}) = D_\textrm{KL}(π_{→θ_0} \| →y)$ we get an analogue of
  the TRPO objective, which motivated PPO;
~~~
- the MPO algorithm (which we did not discuss) employs $𝓡(→y, π_{→θ_0}) = D_\textrm{KL}(→y \| π_{→θ_0})$.

---
# AlphaZero as Regularized Policy Optimization

Let us define the **empirical visit distribution** $π̂$ as
$$π̂(a|s) ≝ \frac{1 + N(s, a)}{|𝓐| + ∑_b N(s, b)}.$$

~~~
The added plus ones makes the following analysis easier, but are not strictly
necessary.

~~~
We also define the **multiplier** $λ_N$ as
$$λ_N(s) ≝ C(s) ⋅ \frac{\sqrt{∑_b N(s, b)}}{|𝓐| + ∑_b N(s, b)}.$$

~~~
With these definitions, we can rewrite the AlphaZero action selection to
$$a^* = \argmax\nolimits_a \bigg(Q(s, a) + λ_N ⋅ \frac{π_{→θ}(a|s)}{π̂(a|s)}\bigg).$$

---
# AlphaZero as Regularized Policy Optimization

$$a^* = \argmax\nolimits_a \bigg(Q(s, a) + λ_N ⋅ \frac{π_{→θ}(a|s)}{π̂(a|s)}\bigg)$$

For notational simplicity, we will represent $Q(s, a)$ as a vector $→q$, where
$q_a = Q(s, a)$, and similarly the policies as $→π_{→θ}$, $→π̂$.

~~~
Furthermore, for two vectors $→a, →b$, let $\frac{→a}{→b}$ denote element-wise
division with $(\frac{→a}{→b})_i ≝ \frac{a_i}{b_i}.$ 

~~~
With this notation, the action selection can be succinctly written as
$$a^* = \argmax\nolimits_a \Big(→q + λ_N \frac{→π_{→θ}}{→π̂}\Big).$$

---
# AlphaZero as Regularized Policy Optimization

Let $→π̄$ be the solution of the following objective:
$$→π̄ ≝ \argmax\nolimits_{→y ∈ 𝓢} \Big(→q^T →y - λ_N D_\textrm{KL}(→π_{→θ} \| →y)\Big).$$

~~~
The solution to this objective can be computed explicitly as
$$→π̄ = λ_N \frac{→π_{→θ}}{α - →q},$$
where $α ∈ ℝ$ is set (using binary search) such that the result is a proper distribution.
~~~
- Note that $α ≥ \max_{b∈𝓐} \big(q_b + λ_N π_{→θ}(b)\big)$, because $π̄(a)$ must
  be at most 1.
~~~
- Furthermore, $α ≤ \max_{b∈𝓐} (q_b) + λ_N$, because we need $∑_a π̄(a) = 1$
  and we combine $∑_a \frac{λ_N π_{→θ}(a)}{\max_b (q_b) + λ_N - q_a} ≤ ∑_a \frac{λ_N π_{→θ}(a)}{λ_N} = 1$
  with the fact that $∑_a \frac{λ_N π_{→θ}(a)}{α - q_a}$ is a decreasing
  function of $α ≥ \max_b q_b$.

~~~
Note the $λ_N ≈ 1/\sqrt N$ decreasing the regularization for increasing number
of simulations.

---
# AlphaZero as Regularized Policy Optimization

In the paper, it is proven that the action $a^*$ selected by the AlphaZero algorithm fulfills
$$a^* = \argmax\nolimits_a \bigg(\frac{∂}{∂ N(s, a)} \Big(→q^T →π̂ - λ_N D_\textrm{KL}(→π_{→θ} \| →π̂)\Big)\bigg).$$

~~~
In other words, $π̂$ “tracks” $π̄$.

~~~
Furthermore, it can be also shown that for the selected action $a^*$,
$$π̂(a^*|s) ≤ π̄(a^* | s),$$
until in the limit, the two distributions coincide.

~~~
If you are interested in the proof, see Appendix D (pages 19-22).

---
# AlphaZero as Regularized Policy Optimization

The $π̄$ can be used in the AlphaZero algorithm in several ways:

- **Act**: the action in self-play games could be sampled according to
  $π̄(⋅|s_\textrm{root})$ instead of $π̂$;

~~~
- **Search**: during search, we could sample the actions stochastically
  according to $π̄$ instead of the PUCT rule;
~~~
- **Learn**: we could use $π̄$ as the target policy during training instead
  of $π̂$;
~~~
- **All**: all of the above.

---
# AlphaZero as Regularized Policy Optimization

![w=50%](muzero_regpolopt_cheetah.svgz)![w=50%](muzero_regpolopt_cheetah_curves.svgz)

---
# AlphaZero as Regularized Policy Optimization

![w=53%](muzero_regpolopt_mcpacman.svgz)![w=46%](muzero_regpolopt_atari.svgz)

---
# AlphaZero as Regularized Policy Optimization

![w=100%,v=middle](muzero_regpolopt_components.svgz)

---
section: Gumbel-Max
class: section
# Gumbel-Max Trick

---
# Gumbel-Max Trick

Let $z$ be a categorical variable with class probabilities $→p = (p_1, p_2, …, p_K)$.

~~~
The Gumbel-Max trick (based on a 1954 theorem from E. J. Gumbel) states that
we can draw samples $z ∼ →p$ using
$$z = \operatorname{one-hot}\bigg(\argmax_{i∈\{1, …, K\}} \big(g_i + \log p_i\big)\bigg),$$

![w=38%,f=right](gumbel_vs_normal.svgz)

where $g_i$ are independent samples drawn from the $\operatorname{Gumbel}(0, 1)$
distribution.

~~~
To sample $g$ from the distribution $\operatorname{Gumbel}(0, 1)$, we can sample
$u ∼ U(0, 1)$ and then compute
$$g = -\log(-\log u).$$

---
class: dbend
# Gumbel Distribution

First recall that exponential distribution $\operatorname{Exp}(λ)$ has
$$\operatorname{PDF}(x; λ) = λ e^{-λx},~~~\operatorname{CDF}(x; λ) = 1 - e^{-λx}.$$

~~~
The standard $\operatorname{Gumbel}(0, 1)$ distribution has
$$\operatorname{PDF}(x) = e^{-x - e^{-x}},~~~\operatorname{CDF}(x) = e^{-e^{-x}}.$$

~~~
The Gumbel distribution can be used to model the distribution of maximum
of a number of samples from the exponential distribution:
~~~
if $x̃$ is a maximum of $N$ samples from the $\operatorname{Exp}(1)$
distribution, we get that
$$P(x̃ - \log N ≤ x)
  = P(x̃ ≤ x + \log N)
  = \operatorname{CDF}_{\operatorname{Exp}(1)}\big(x + \log N\big)^N
  = \Big(1 - \frac{e^{-x}}{N}\Big)^N,$$
~~~
which converges to $e^{-e^{-x}} = \operatorname{CDF}_{\operatorname{Gumbel}(0, 1)}(x)$ for $N → ∞$.

---
class: dbend
# Gumbel-Max Trick Proof

To prove the Gumbel-Max trick, we first reformulate it slightly.

Let $l_i$ be logits of a categorical distribution (so that the class probabilities
$π_i ∝ e^{l_i}$), and let $g_i ∼ \operatorname{Gumbel}(0, 1)$. Then
$$π_k = P\big(k = \argmax\nolimits_i (g_i + l_i)\big).$$

~~~
We first observe that the theorem is invariant to a scalar shift of logits,
so we can without loss of generality assume that $∑_i e^{l_i} = 1$ and $π_i
= e^{l_i}$.

~~~
For convenience, denote $u_i ≝ g_i + l_i$.

~~~
We will use both the $\operatorname{PDF}$ and $\operatorname{CDF}$ of
a $\operatorname{Gumbel}(0, 1)$ distribution:
$$\begin{aligned}
  \operatorname{PDF}(g_i) &= e^{-g_i - e^{-g_i}}, \\
  \operatorname{CDF}(g_i) &= e^{-e^{-g_i}}.
\end{aligned}$$

---
class: dbend
# Gumbel-Max Trick Proof

To finish the proof, we compute

$\displaystyle\kern12em{}\mathllap{P\big(k = \argmax\nolimits_i (g_i + l_i)\big)} = P(u_k ≥ u_i, ∀_{i≠k})$

~~~
$\displaystyle\kern12em{} = ∫ P(u_k) ∏\nolimits_{i≠k} P(u_k ≥ u_i | u_k) \d u_k$

~~~
$\displaystyle\kern12em{} = ∫ P(g_k | g_k = u_k - l_k) ∏\nolimits_{i≠k} P(g_i ≤ u_k - l_i | u_k) \d u_k$

~~~
$\displaystyle\kern12em{} = ∫ e^{\textcolor{blue}{l_k}-u_k-e^{\textcolor{darkgreen}{l_k}-u_k}} ∏\nolimits_{i≠k} e^{-e^{\textcolor{magenta}{l_i}-u_k}} \d u_k$

~~~
$\displaystyle\kern12em{} = ∫ \textcolor{blue}{π_k}e^{-u_k-\textcolor{darkgreen}{π_k}\textcolor{red}{e^{-u_k}}} ∏\nolimits_{i≠k} e^{-\textcolor{magenta}{π_i}\textcolor{red}{e^{-u_k}}} \d u_k$

~~~
$\displaystyle\kern12em{} = π_k ∫ e^{-u_k-\textcolor{red}{e^{-u_k}} ∑_i π_i} \d u_k$

~~~
$\displaystyle\kern12em{} = π_k ∫ e^{-g_k-e^{-g_k}} \d g_k = π_k ⋅ 1 = π_k.$

---
section: GumbelZero
class: section
# Gumbel AlphaZero and MuZero

---
# Gumbel AlphaZero and MuZero

In AlphaZero, using the MCTS visit counts as the target policy fails to improve
the policy for small number of visits.

<br>
![w=70%,h=center](gumbelzero_9x9_go_simulations.svgz)

~~~
In _Ivo Danihelka et al.: Policy Improvement by Planning with Gumbel_, several
AlphaZero/MuZero improvements are proposed; among other a different target
policy, which guarantees improvement.

---
# Gumbel AlphaZero and MuZero

Let $π$ be a categorical distributions parametrized with $\mathit{logits}(a)$.
Let $→g ∈ ℝ^k$ be a vector of independent Gumbel(0, 1) random variables.

~~~
The Gumbel-Max trick states that
$$A = \argmax\nolimits_a \big(g(a) + \mathit{logits}(a)\big)$$
has a distribution $A ∼ π$.

~~~
The Gumbel-Max trick can be generalized to **Gumbel-Top-k** trick, capable of
producing $n$ actions without replacement by considering the top $n$ scoring
actions $\operatorname{argtop}(→g + \boldsymbol{logits}, n)$:
$$\begin{aligned}
  A_1 &= \argmax\nolimits_a \big(g(a) + \mathit{logits}(a)\big), \\
  A_2 &= \argmax\nolimits_{a ≠ A_1} \big(g(a) + \mathit{logits}(a)\big), \\
  \vdots \\
  A_n &= \argmax\nolimits_{a \not∈ \{A_1, …, A_{n-1}\}} \big(g(a) + \mathit{logits}(a)\big). \\
\end{aligned}$$

---
# GumbelZero, A. Guaranteed Policy Improvement

## A. Guaranteed Policy Improvement

For a small number of simulations, PUCT does not guarantee policy improvement.
~~~
- Consider for example three actions with prior policy $(50\%, 30\%, 20\%)$ and
action values $(0, 0, 1)$.
~~~
- The PUCT rule will select the first two actions.
~~~
- However, the value function of any policy considering just the first two actions
  is 0, which is worse than the value function of the prior policy.

~~~
In GumbelZero, we start by sampling $n$ actions without replacement using the
Gumbel-Max trick with Gumbel noise $→g$.

~~~
Our first attempt is to define a one-hot policy selecting an action $A_{n+1}$
such that
$$A_{n+1} = \argmax_{a ∈ \{A_1, …, A_n\}} \big(g(a) + \mathit{logits}(a) + σ(q(a))\big),$$
where $σ$ can be any monotonically increasing transformation.

---
# GumbelZero, A. Guaranteed Policy Improvement

![w=100%,v=middle](gumbelzero_policy_improvement_algorithm.svgz)

---
style: .katex-display { margin: .8em 0 }
# GumbelZero, A. Guaranteed Policy Improvement

The policy choosing the action
$$A_{n+1} = \argmax_{a ∈ \{A_1, …, A_n\}} \Big(g(a) + \mathit{logits}(a) + σ\big(q(a)\big)\Big)$$
guarantees policy improvement, i.e., $𝔼[q(A_{n+1})] ≥ 𝔼_{a∼π}[q(a)]$.

~~~
Considering a fixed Gumbel noise $→g$, we get that
$$q\bigg(\argmax_{a ∈ \{A_1, …, A_n\}} \Big(g(a) + \mathit{logits}(a) + σ\big(q(a)\big)\Big)\bigg) ≥ q\bigg(\argmax_{a ∈ \{A_1, …, A_n\}} \Big(g(a) + \mathit{logits}(a)\Big)\bigg),$$
~~~
- either the action chosen on both sides is the same and we get an equality, or
~~~
- the action on the left side is different, meaning it has larger $q(a)$.

~~~
Finally, if the inequality holds for any $→g$, it holds also in expectation.
With the Gumbel-Max trick transforming the expectation of the right side to sampling
an action $a ∼ π$, we get the required $𝔼[q(A_{n+1})] ≥ 𝔼_{a∼π}[q(a)]$.

---
# GumbelZero, B. Planning on Stochastic Bandit

## B. Planning on Stochastic Bandit

When we get only an estimate $q̂(a)$ of the action-value function, it is probably
beneficial to visit an action multiple times.

~~~
Furthermore, choosing actions in the root using a UCB-like rule is not optimal:
~~~
- UCB minimizes cumulative regret, i.e., maximizes the sum of the obtained
  returns;
~~~
- in the root our goal is to obtain the best possible $A_{n+1}$, i.e.,
  maximize just $𝔼[q(A_{n+1})]$.

~~~
The authors evaluated several simple regret minimization algorithms, and chose
Sequential Halving (because it was easier to tune and does not have
problem-dependent parameters).

---
# GumbelZero, B. Planning on Stochastic Bandit

![w=82%,h=center](gumbelzero_sequential_halving.svgz)

---
# GumbelZero, B. Planning on Stochastic Bandit

![w=100%](gumbelzero_sequential_halving_algorithm.svgz)

The authors utilize $m = \min(n, 16)$, and visit each action at least
once even when $n$ is small by visiting each action
$\max\Big(1, \big\lfloor\frac{n}{\lceil \log_2 m\rceil m}\big\rfloor\Big)$;
after $n$ simulation, the search is always stopped.

---
# GumbelZero, C. Better Improved Policy

## C. Better Improved Policy
Using a one-hot policy based on
$$A_{n+1} = \argmax_{a ∈ \{A_1, …, A_n\}} \big(g(a) + \mathit{logits}(a) + σ(q(a))\big)$$
results in using a simple policy loss
$$L_\textrm{simple}(π) = -\log π(A_{n+1}).$$

---
style: .katex-display { margin: .9em 0 }
# GumbelZero, C. Better Improved Policy

However, more information from the search might be extracted by using all action-value functions
$q(a)$ produced by the search.

~~~
- First, we complete the action values using
  $$\mathit{completedQ}(a) ≝ \begin{cases} q(a) & \textrm{if~}N(a) > 0, \\ v_π
  & \textrm{otherwise}.\end{cases}$$

~~~
- Then, we define improved policy as
  $$π' = \softmax\big(\mathit{logits}(a) + σ(\mathit{completedQ}(a))\big).$$

~~~
  It can be again proven (appendix C of the paper) that $π'$ is an improved policy, so $𝔼_{a∼π'}[q(a)] ≥ 𝔼_{a∼π}[q(a)]$.

~~~
- A natural loss is then
  $$L_\textrm{completed}(π) = D_\textrm{KL}(π' \| π).$$

---
# GumbelZero, C. Better Improved Policy

The authors propose to use
$$σ\big(q̂(a)\big) ≝ \big(c_\textrm{visit} + \max_b N(b)\big) c_\textrm{scale} q̂(a),$$
for $c_\textrm{visit} = 50$, $c_\textrm{scale} = 1.0$.

~~~
Furthermore, the authors propose a consistent approximation to $v_π$ based on
a network-predicted $v̂_π$ and the $q(a)$ of the visited actions:
$$v_\textrm{mix} ≝ \frac{1}{1 + ∑_b N(b)} \Bigg(v̂_π + \Big(∑\nolimits_b N(b)\Big)\frac{∑_{a, N(a) > 0} π(a) q(a)}{∑_{a, N(a) > 0} π(a)}\Bigg).$$

~~~
Overall, the algorithm denoted in the paper as Gumbel MuZero utilizes Sequential
Halving with Gumbel and trains using the improved policy combining logits
and action values completed by $v_\textrm{mix}$; otherwise it is the same as
MuZero.

---
# GumbelZero, D. Action Selection in Non-Root Nodes

## D. Action Selection in Non-Root Nodes

We might consider utilizing the improved policy $π'$ also in the non-root nodes,
by for example sampling actions from it. Additionally, the authors provide
a deterministic algorithm of choosing non-root actions minimizing the
difference between $π'$ and the current visit counts:
$$a^* = \argmin_a ∑_b \Bigg(π'(b) - \underbrace{\frac{N(b) + [a = b]}{1 + ∑_c N(c)}}_{\textrm{normalized visit counts if taking~}a}\Bigg)^2.$$

~~~
This formula can be simplified to
$$a^* = \argmax_a \Bigg(π'(a) - \frac{N(a)}{1 + ∑_b N(b)}\Bigg).$$

~~~
When this action selection is used, the authors call the algorithm
Full Gumbel MuZero.

---
class: dbend
# GumbelZero, D. Action Selection in Non-Root Nodes

$\displaystyle \kern3em\mathllap{a^*} = \argmin_a ∑_b \Bigg(π'(b) - \frac{N(b) + [a = b]}{1 + ∑_c N(c)}\Bigg)^2$

~~~
$\displaystyle \kern3em{} = \argmin_a ∑_b \Bigg(\bigg(π'(b) - \frac{N(b)}{1 + ∑_c N(c)}\bigg) - \frac{[a = b]}{1 + ∑_c N(c)}\Bigg)^2$

~~~
$\displaystyle \kern3em{} = \argmin_a ∑_b -2\Bigg(π'(b) - \frac{N(b)}{1 + ∑_c N(c)}\Bigg) \frac{[a = b]}{1 + ∑_c N(c)}$

~~~
$\displaystyle \kern3em{} = \argmin_a -∑_b \Bigg(π'(b) - \frac{N(b)}{1 + ∑_c N(c)}\Bigg) [a = b]$

~~~
$\displaystyle \kern3em{} = \argmax_a \Bigg(π'(a) - \frac{N(a)}{1 + ∑_b N(b)}\Bigg)$

---
# Gumbel AlphaZero and MuZero

![w=100%](gumbelzero_9x9_go_simulations.svgz)

“Replacement” is a Gumbel MuZero ablation sampling actions with replacement.  
“TRPO MuZero”, “MPO MuZero” use Act+Search+Learn using the previously described
regularized policy with $D_\textrm{KL}(π \| π_\textrm{new})$ and
$D_\textrm{KL}(π_\textrm{new} \| π)$ regularizer, respectively.

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_9x9_go_ablations.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_ablations_losses.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_ablations_action_selection.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_ablations_visits.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_go_chess.svgz)

---
# Gumbel AlphaZero and MuZero

![w=100%,v=middle](gumbelzero_atari.svgz)
