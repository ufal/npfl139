title: NPFL139, Lecture 9
class: title, langtech, cc-by-sa
# Eligibility Traces, Impala

## Milan Straka

### April 16, 2025

---
section: ControlVariates
class: section
# Off-policy Correction Using Control Variates

---
# Off-policy Correction Using Control Variates

Let $G_{t:t+n}$ be the estimated $n$-step return
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + \Big[\textrm{episode still running in }t+n\Big] γ^n V(S_{t+n}),$$

~~~
which can be written recursively as
$$G_{t:t+n} \begin{cases}
  0 & \mathrm{if~episode~ended~before~}t, \\
  V(S_t) & \mathrm{if~}n=0, \\
  R_{t+1} + γ G_{t+1:t+n} & \mathrm{otherwise}.
\end{cases}$$

~~~
For simplicity, we do not explicitly handle the first case (“the episode
has already ended”) in the following.

---
style: .katex-display { margin: .8em 0 }
# Off-policy Correction Using Control Variates

Note that we can write
$$\begin{aligned}
G_{t:t+n} - V(S_t)
  &= R_{t+1} + γ G_{t+1:t+n} - V(S_t) \\
  &= R_{t+1} + γ \big(G_{t+1:t+n} - V(S_{t+1})\big) + γV(S_{t+1}) - V(S_t),
\end{aligned}$$

~~~
which yields
$$G_{t:t+n} - V(S_t) = R_{t+1} + γV(S_{t+1}) - V(S_t) + γ\big(G_{t+1:t+n} - V(S_{t+1})\big).$$

~~~
Denoting the TD error as $δ_t ≝ R_{t+1} + γV(S_{t+1}) - V(S_t)$, we can
therefore write the $n$-step estimated return as a sum of TD errors:
$$G_{t:t+n} = V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}.$$

~~~
To correctly handle the “the episode has already ended” case, we might define the TD error as
$δ_t ≝ R_{t+1} + [¬\textrm{done}]⋅γV(S_{t+1}) - V(S_t)$ if the
state $S_t$ exists, and to $δ_t = 0$ otherwise.

---
class: tablewide
style: table {line-height: 1}
# Return Formulations

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} ≝ R_{t+1} + γ G_{t+1:t+n}$                                                         | $V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}$                    |

---
# Off-policy Correction Using Control Variates

Now consider applying the IS off-policy correction to $G_{t:t+n}$ using the
importance sampling ratio
$$ρ_t ≝ \frac{π(A_t | S_t)}{b(A_t | S_t)},~~~ρ_{t:t+n} ≝ ∏_{i=0}^n ρ_{t+i}.$$

~~~
First note that
$$𝔼_{A_t ∼ b} \big[ρ_t\big] = ∑_{A_t} b(A_t | S_t) \frac{π(A_t | S_t)}{b(A_t | S_t)} = 1,$$

~~~
which can be extended to
$$𝔼_b \big[ρ_{t:t+n}\big] = 1.$$

---
# Off-policy Correction Using Control Variates

Until now, we used
$$G_{t:t+n}^\mathrm{IS} ≝ ρ_{t:t+n-1} G_{t:t+n}.$$

~~~
However, such correction has unnecessary variance. Notably, when expanding
$G_{t:t+n}$
$$G_{t:t+n}^\mathrm{IS} = ρ_{t:t+n-1} \big(R_{t+1} + γ G_{t+1:t+n}\big),$$

~~~
the $R_{t+1}$ depends only on $ρ_t$, not on $ρ_{t+1:t+n-1}$, and given that
the expectation of the importance sampling ratio is 1, we can simplify to
$$G_{t:t+n}^\mathrm{IS} = ρ_t R_{t+1} + ρ_{t:t+n-1} γ G_{t+1:t+n}.$$

~~~
Such an estimate can be written recursively as
$$G_{t:t+n}^\mathrm{IS} = ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big).$$

---
class: tablewide
style: table {line-height: 1}
# Return Formulations

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} ≝ R_{t+1} + γ G_{t+1:t+n}$                                                         | $V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}$                    |
| $G_{t:t+n}^\mathrm{IS} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big)$                   |                                                         |

---
# Off-policy Correction Using Control Variates

We can reduce the variance even further – when $ρ_t=0$, we might consider
estimating the return using $V(S_t)$ instead of 0.

~~~
To utilize this idea, we turn to **control variates**, which is
a general method of reducing variance of Monte Carlo estimators. Let $μ$ be
an unknown expectation, which we estimate using an unbiased estimator $m$.
Assume we have another **correlated** statistic $k$ with a known expectation $κ$.

~~~
We can then use an estimate $m^* ≝ m - c(k - κ)$, which is also an unbiased
estimator of $μ$, with variance
$$\Var(m^*) = \Var(m) + c^2 \Var(k) - 2c\Cov(m, k).$$

~~~
To arrive at the optimal value of $c$, we can set the derivative of $\Var(m^*)$
to 0, obtaining
$$c = \frac{\Cov(m, k)}{\Var(k)}.$$

---
# Off-policy Correction Using Control Variates

In case of the value function estimate
$$G_{t:t+n}^\mathrm{IS} = ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big),$$
we might consider using $ρ_t$ as the correlated statistic $k$, with known
expectation $κ=1$, because if $ρ_t ≫ 1$, then our return estimate is probably an
overestimate, and vice versa.

~~~
The optimal value of $c$ should then be
$$c = \frac{\Cov(m, k)}{\Var(k)} = \frac{𝔼_b\big[(G_{t:t+n}^\mathrm{IS} - v_π(S_t))(ρ_t-1)\big]}{𝔼_b\big[(ρ_t-1)^2\big]},$$
which is however difficult to compute.
~~~
Instead, considering the estimate when $ρ_t = 0$, we get
$$ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big) + c(1 - ρ_t) \xlongequal{ρ_t=0} c.$$
~~~
Because a reasonable estimate in case of $ρ_t = 0$ is $V(S_t)$, we use $c = V(S_t)$.

---
# Off-policy Correction Using Control Variates

The estimate with the **control variate** term is therefore
$$G_{t:t+n}^\mathrm{CV} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - ρ_t)V(S_t),$$
which adds no bias, since the expected value of $1-ρ_t$ is zero and $ρ_t$ and $S_t$
are independent.

~~~
Similarly as before, rewriting to
$$\begin{aligned}
G_{t:t+n}^\mathrm{CV} - V(S_t)
  &= ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) - ρ_tV(S_t) \\
  &= ρ_t \big(R_{t+1} + γ V(S_{t+1}) - V(S_t) + γ (G_{t+1:t+n}^\mathrm{CV} - V(S_{t+1}))\big)
\end{aligned}$$

~~~
results in
$$G_{t:t+n}^\mathrm{CV} = V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i ρ_{t:t+i} δ_{t+i}.$$

---
class: tablewide
style: table {line-height: 1}
# Return Formulations

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} ≝ R_{t+1} + γ G_{t+1:t+n}$                                                         | $V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}$                    |
| $G_{t:t+n}^\mathrm{IS} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big)$                   |                                                         |
| $G_{t:t+n}^\mathrm{CV} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - ρ_t)V(S_t)$ | $V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i ρ_{t:t+i} δ_{t+i}$ |

---
section: EligibilityTraces
class: section
# Eligibility Traces

---
# Eligibility Traces

Eligibility traces are a mechanism of combining multiple $n$-step return
estimates for various values of $n$.

~~~
First note that instead of an $n$-step return, we can use any average of $n$-step
returns for different values of $n$, for example
$\frac{2}{3}G_{t:t+2} + \frac{1}{3}G_{t:t+4}$.

---
# $λ$-return

For a given $λ ∈ [0,1]$, we define **$λ$-return** as
$$G_t^λ ≝ (1 - λ) ∑_{i=1}^∞ λ^{i-1} G_{t:t+i}.$$

~~~
![w=75%,f=right](traces_weighting.svgz)

~~~
Alternatively, the $λ$-return can be written recursively as
$$\begin{aligned}
G_t^λ &= (1 - λ) G_{t:t+1} \\
      &+ λ (R_{t+1} + γ G_{t+1}^λ).
\end{aligned}$$

---
# $λ$-return

In an episodic task with time of termination $T$, we can rewrite the $λ$-return
to
$$G_t^λ = (1 - λ) ∑_{i=1}^{T-t-1} λ^{i-1} G_{t:t+i} + λ^{T-t-1} G_t.$$

~~~
![w=60%,h=center](traces_example.svgz)

---
# Truncated $λ$-return

We might also set a limit on the largest value of $n$, obtaining
**truncated $λ$-return**
$$G_{t:t+n}^λ ≝ (1 - λ) ∑_{i=1}^{n-1} λ^{i-1} G_{t:t+i} + λ^{n-1} G_{t:t+n}.$$

~~~
The truncated $λ$ return can be again written recursively as

$$G_{t:t+n}^λ = (1 - λ) G_{t:t+1} + λ (R_{t+1} + γ G_{t+1:t+n}^λ),~~G_{t:t+1}^λ = G_{t:t+1}.$$

~~~
Similarly to before, we can express the truncated $λ$ return as a sum of TD
errors

$$\begin{aligned}
  G_{t:t+n}^λ - V(S_t)
  & = (1 - λ) \big(R_{t+1} + γV(S_{t+1})\big) + λ (R_{t+1} + γ G_{t+1:t+n}^λ) - V(S_t) \\
  & = R_{t+1} + γV(S_{t+1}) - V(S_t) + λ γ \big(G_{t+1:t+n}^λ - V(S_{t+1})\big),
\end{aligned}$$

~~~
obtaining an analogous estimate $G_{t:t+n}^λ = V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i λ^i δ_{t+i}.$

---
# Variable $λ$s

The (truncated) $λ$-return can be generalized to utilize different $λ_i$ at each
step $i$. Notably, we can generalize the recursive definition

$$G_{t:t+n}^λ = (1 - λ) G_{t:t+1} + λ (R_{t+1} + γ G_{t+1:t+n}^λ)$$

~~~
to
$$G_{t:t+n}^{λ_i} = (1 - λ_{t+1}) G_{t:t+1} + λ_{t+1} (R_{t+1} + γ G_{t+1:t+n}^{λ_i}),$$

~~~
and express this quantity again by a sum of TD errors:

$$G_{t:t+n}^{λ_i} = V(S_t) + ∑_{i=0}^{n-1} γ^i \left(∏_{j=1}^i λ_{t+j}\right) δ_{t+i}.$$

---
class: tablewide
style: table {line-height: 1}
# Return Formulations

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} ≝ R_{t+1} + γ G_{t+1:t+n}$                                                         | $V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}$                    |
| $G_{t:t+n}^\mathrm{IS} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big)$                   |                                                         |
| $G_{t:t+n}^\mathrm{CV} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - ρ_t)V(S_t)$ | $V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i ρ_{t:t+i} δ_{t+i}$ |
| $G_{t:t+n}^λ ≝ (1 - λ) G_{t:t+1} + λ (R_{t+1} + γ G_{t+1:t+n}^λ)$                             | $V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i λ^i δ_{t+i}$       |
| $G_{t:t+n}^{λ_i} ≝ (1 - λ_{t+1}) G_{t:t+1} + λ_{t+1} (R_{t+1} + γ G_{t+1:t+n}^{λ_i})$         | $V(S_t) + ∑_{i=0}^{n-1} γ^i \left({\scriptstyle ∏_{j=1}^i λ_{t+j}}\right) δ_{t+i}$ |

---
# Off-policy Traces with Control Variates

Finally, we can combine the eligibility traces with off-policy estimation using
control variates:
$$G_{t:t+n}^{λ,\mathrm{CV}} ≝ (1 - λ) ∑_{i=1}^{n-1} λ^{i-1} G_{t:t+i}^\mathrm{CV} + λ^{n-1} G_{t:t+n}^\mathrm{CV}.$$

~~~
Recalling that
$$G_{t:t+n}^\mathrm{CV} = ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - ρ_t)V(S_t),$$
~~~

we can rewrite $G_{t:t+n}^{λ,\mathrm{CV}}$ recursively as
$$G_{t:t+n}^{λ,\mathrm{CV}} = (1 - λ) G_{t:t+1}^\mathrm{CV} + λ \Big(ρ_t\big(R_{t+1} + γ G_{t+1:t+n}^{λ,\mathrm{CV}}\big) + (1-ρ_t)V(S_t)\Big),$$

~~~
which we can simplify by expanding $G_{t:t+1}^\mathrm{CV}=ρ_t(R_{t+1} + γV(S_{t+1})) + (1-ρ_t)V(S_t)$ to
$$G_{t:t+n}^{λ,\mathrm{CV}} - V(S_t) = ρ_t \big(R_{t+1} + γV(S_{t+1}) - V(S_t)\big) + γλρ_t \big(G_{t+1:t+n}^{λ,\mathrm{CV}} - V(S_{t+1})\big).$$

---
# Off-policy Traces with Control Variates

Consequently, analogously as before, we can write the off-policy traces estimate
with control variates as

$$G_{t:t+n}^{λ,\mathrm{CV}} = V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i λ^i ρ_{t:t+i} δ_{t+i},$$

~~~
and by repeating the above derivation we can extend the result also for time-variable $λ_i$, we obtain
$$G_{t:t+n}^{λ_i,\mathrm{CV}} = V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i \left(∏_{j=1}^i λ_{t+j}\right) ρ_{t:t+i} δ_{t+i}.$$

---
section: Returns
class: tablewide
style: table {line-height: 1}
# Return Recapitulation

| Recursive definition                                                                          | Formulation with TD errors                              |
|-----------------------------------------------------------------------------------------------|---------------------------------------------------------|
| $G_{t:t+n} ≝ R_{t+1} + γ G_{t+1:t+n}$                                                         | $V(S_t) + ∑_{i=0}^{n-1} γ^i δ_{t+i}$                    |
| $G_{t:t+n}^\mathrm{IS} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{IS}\big)$                   |                                                         |
| $G_{t:t+n}^\mathrm{CV} ≝ ρ_t \big(R_{t+1} + γ G_{t+1:t+n}^\mathrm{CV}\big) + (1 - ρ_t)V(S_t)$ | $V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i ρ_{t:t+i} δ_{t+i}$ |
| $G_{t:t+n}^λ ≝ (1 - λ) G_{t:t+1} + λ (R_{t+1} + γ G_{t+1:t+n}^λ)$                             | $V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i λ^i δ_{t+i}$       |
| $G_{t:t+n}^{λ_i} ≝ (1 - λ_{t+1}) G_{t:t+1} + λ_{t+1} (R_{t+1} + γ G_{t+1:t+n}^{λ_i})$         | $V(S_t) + ∑_{i=0}^{n-1} γ^i \left({\scriptstyle ∏_{j=1}^i λ_{t+j}}\right) δ_{t+i}$ |
| $\begin{aligned}G_{t:t+n}^{λ,\mathrm{CV}} &≝ (1 - λ) G_{t:t+1}^\mathrm{CV} \\&+ λ \big(ρ_t\big(R_{t+1} + γ G_{t+1:t+n}^{λ,\mathrm{CV}}\big) + (1-ρ_t)V(S_t)\big)\end{aligned}$ | $V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i λ^i ρ_{t:t+i} δ_{t+i}$ |
| $\begin{aligned}G_{t:t+n}^{λ_i,\mathrm{CV}} &≝ (1 - λ_{t+1}) G_{t:t+1}^\mathrm{CV} \\+& λ_{t+1} \big(ρ_t\big(R_{t+1} + γ G_{t+1:t+n}^{λ_i,\mathrm{CV}}\big) + (1-ρ_t)V(S_t)\big)\end{aligned}$ | $\begin{aligned}&V(S_t)\\&\textstyle + ∑\nolimits_{i=0}^{n-1} γ^i \left({\scriptstyle ∏_{j=1}^i λ_{t+j}}\right) ρ_{t:t+i} δ_{t+i}\end{aligned}$ |

---
section: TD($λ$)
class: section
# The TD($λ$) Algorithm

---
# TD($λ$)

We have defined the $λ$-return in the so-called **forward view**.

![w=100%,mh=85%,v=bottom](traces_forward.svgz)

---
# TD($λ$)

However, to allow on-line updates, we might consider also the **backward view**

![w=98%,h=center](traces_backward.svgz)

---
# TD($λ$)

TD($λ$) is an algorithm implementing on-line policy evaluation utilizing the
backward view.

![w=55%](traces_td_lambda.svgz)![w=45%](td_lambda_backup.svgz)

Note that TD(0) is just the usual 1-step TD policy evaluation, while
TD(1) is a fully-online algorithm that is reminiscent to a Monte Carlo
algorithm, because the estimated returns are just discounted sums
of all rewards.

---
# TD($λ$)

![w=50%,v=middle](traces_example.svgz)![w=50%,v=middle](td_lambda_performance.svgz)

---
# Sarsa($λ$)

Sarsa($λ$) is an extension of TD($λ$) to action-value methods, notably Sarsa.

![w=49%,f=left](sarsa_lambda_algorithm.svgz)

![w=98%,mw=49%,h=right](sarsa_lambda_gridworld.svgz)![w=98%,mw=49%,h=right](sarsa_lambda_performance.svgz)

---
section: Vtrace
class: section
# V-trace

---
# V-trace

V-trace is a modified version of $n$-step return with off-policy correction,
defined in the Feb 2018 IMPALA paper as (using the notation from the paper):

$$G_{t:t+n}^\textrm{V-trace} ≝ V(S_t) + ∑_{i=0}^{n-1} γ^i \left(∏\nolimits_{j=0}^{i-1} c̄_{t+j}\right) ρ̄_{t+i} δ_{t+i},$$

~~~
where $ρ̄_t$ and $c̄_t$ are the truncated importance sampling ratios for $ρ̄ ≥ c̄$:
$$ρ̄_t ≝ \min\left(ρ̄, \frac{π(A_t | S_t)}{b(A_t | S_t)}\right),~~~~c̄_t ≝ \min\left(c̄, \frac{π(A_t | S_t)}{b(A_t | S_t)}\right).$$

~~~
Note that if $b=π$ and assuming $c̄ ≥ 1$, $v_s$ reduces to $n$-step Bellman
target.

---
# V-trace

Note that the truncated IS weights $ρ̄_t$ and $c̄_t$ play different roles:

~~~
- The $ρ̄_t$ defines the fixed point of the update rule. For $ρ̄=∞$, the
  target is the value function $v_π$, if $ρ̄<∞$, the fixed point is somewhere
  between $v_π$ and $v_b$. Notice that we do not compute a product of these
  $ρ̄_t$ coefficients.

~~~
  Concretely, the fixed point of an operator defined by $G_{t:t+n}^\textrm{V-trace}$
  corresponds to a value function of the policy
  $$π_ρ̄(a|s) ∝ \min\big(ρ̄b(a|s), π(a|s)\big).$$

~~~
- The $c̄_t$ impacts the speed of convergence (the contraction rate of the
  Bellman operator), not the sought policy. Because a product of the $c̄_t$
  ratios is computed, it plays an important role in variance reduction.

~~~
However, the paper utilizes $c̄=1$ and out of $ρ̄ ∈ \{1, 10, 100\}$, $ρ̄=1$ works
empirically the best, so the distinction between $c̄_t$ and $ρ̄_t$ is not useful in
practice.

---
class: dbend
# V-trace Analysis

Let us define the (untruncated for simplicity; similar results can be proven for
a truncated one) V-trace operator $𝓡$ as:
$$𝓡 V(S_t) ≝ V(S_t) + 𝔼_b \left[∑\nolimits_{i ≥ 0} γ^i \left(∏\nolimits_{j=0}^{i-1} c̄_{t+j}\right) ρ̄_{t+i} δ_{t+i}\right],$$
where the expectation $𝔼_b$ is with respect to trajectories generated by behavior policy $b$.
~~~

Assuming there exists $β ∈ (0, 1]$ such that $𝔼_b ρ̄_0 ≥ β$,
~~~
it can be proven (see Theorem 1 in Appendix A.1 in the Impala paper if interested) that
such an operator is a contraction with a contraction constant
$$γ^{-1} - \big(γ^{-1} - 1\big) \underbrace{∑\nolimits_{i ≥ 0} γ^i 𝔼_b \left[\left(∏\nolimits_{j=0}^{i-1} c̄_j\right) ρ̄_i \right]}_{≥ 1 + γ𝔼_b ρ̄_0} ≤ 1-(1-γ)β<1,$$
therefore, $𝓡$ has a unique fixed point.

---
# V-trace Analysis

We now prove that the fixed point of $𝓡$ is $V^{π_ρ̄}$. Considering $δ_t$ corresponding to $V^{π_ρ̄}$, we get:

$\displaystyle 𝔼_b \big[ρ̄_t δ_t\big] = 𝔼_b \Big[ ρ̄_t\big(R_{t+1} + γ V^{π_ρ̄}(S_{t+1}) - V^{π_ρ̄}(S_t)\big)\big| S_t\Big]$

~~~
$\displaystyle \kern1em = ∑\nolimits_a b(a|S_t) \min\left(\bar \rho, \frac{π(a|S_t)}{b(a|S_t)} \right) \Big[R_{t+1} + γ 𝔼_{s' ∼ p(S_t, a)} V^{π_ρ̄}(s') - V^{π_ρ̄}(S_t)\Big]$

~~~
$\displaystyle \kern1em = \underbrace{∑\nolimits_a π_ρ̄(a|S_t) \Big[R_{t+1} + γ 𝔼_{s' ∼ p(S_t, a)} V^{π_ρ̄}(s') - V^{π_ρ̄}(S_t)\Big]}_{=0} ∑_{a'} \min\big(ρ̄ b(a'|S_t), π(a'|S_t) \big)$

~~~
$\displaystyle \kern1em = 0,$

where the tagged part is zero, since it is the Bellman equation for $V^{π_ρ̄}$.
~~~
This shows that $𝓡 V^{π_ρ̄}(s) = V^{π_ρ̄}(s) + 𝔼_b \left[∑\nolimits_{i ≥ 0} γ^i \left(∏\nolimits_{j=0}^{i-1} c̄_{t+j}\right) ρ̄_{t+i} δ_{t+i}\right]
= V^{π_ρ̄}$, and therefore $V^{π_ρ̄}$ is the unique fixed point of $𝓡$.

~~~
Consequently, in
$G_{t:t+n}^{λ_i,\mathrm{CV}} = V(S_t) + ∑\nolimits_{i=0}^{n-1} γ^i \left(\scriptstyle ∏_{j=1}^i λ_{t+j}\right) ρ_{t:t+i} δ_{t+i},$
only the last $ρ_{t+i}$ from every $ρ_{t:t+i}$ is actually needed for off-policy
correction; $ρ_{t:t+i-1}$ can be considered as traces.

---
section: IMPALA
class: section
# IMPALA

---
# IMPALA

Impala (**Imp**ortance Weighted **A**ctor-**L**earner **A**rchitecture) was
suggested in Feb 2018 paper and allows massively distributed implementation
of an actor-critic-like learning algorithm.

~~~
Compared to A3C-based agents, which communicate gradients with respect to the
parameters of the policy, IMPALA actors communicate trajectories to the
centralized learner.

~~~
![w=50%](impala_overview.svgz)
~~~ ~~
![w=50%](impala_overview.svgz)![w=50%](impala_comparison.svgz)

~~~
If many actors are used, the policy used to generate a trajectory can lag behind
the latest policy. Therefore, the V-trace off-policy actor-critic
algorithm is employed.

---
# IMPALA

Consider a parametrized functions computing $v(s; →θ)$ and $π(a|s; →ω)$,
we update the critic in the direction of
$$\Big(G_{t:t+n}^\textrm{V-trace} - v(S_t; →θ)\Big) ∇_{→θ} v(S_t; →θ),$$

~~~
and the actor in the direction of the policy gradient
$$ρ̄_t ∇_{→ω} \log π(A_t | S_t; →ω)\big(R_{t+1} + γG_{t+1:t+n}^\textrm{V-trace} - v(S_t; →θ)\big).$$

~~~
Finally, we again add the entropy regularization term $β H\big(π(⋅ | S_t; →ω)\big)$ to the
loss function.

---
# IMPALA

![w=60%,h=center](impala_throughput.svgz)

---
# IMPALA – Population Based Training

For Atari experiments, population based training with a population of 24 agents
is used to adapt entropy regularization, learning rate, RMSProp $ε$ and the
global gradient norm clipping threshold.

~~~
![w=80%,h=center](pbt_overview.svgz)

---
# IMPALA – Population Based Training

For Atari experiments, population based training with a population of 24 agents
is used to adapt entropy regularization, learning rate, RMSProp $ε$ and the
global gradient norm clipping threshold.

In population based training, several agents are trained in parallel. When an
agent is _ready_ (after 5000 episodes), then:
~~~
- it may be overwritten by parameters and hyperparameters of another randomly
  chosen agent, if it is sufficiently better (5000 episode mean capped human
  normalized score returns are 5% better);
~~~
- and independently, the hyperparameters may undergo a change (multiplied by
  either 1.2 or 1/1.2 with 33% chance).

---
# IMPALA – Architecture
![w=80%,h=center](impala_architecture.svgz)

---
# IMPALA

![w=100%,v=middle](impala_results.svgz)

---
# IMPALA – Learning Curves

![w=32%,h=center](impala_curves.svgz)

---
# IMPALA – Atari Games

![w=60%,h=center,v=middle](impala_results_atari.svgz)

---
# IMPALA – Atari Hyperparameters

![w=52%,h=center](impala_hyperparameters.svgz)

---
# IMPALA – Ablations

![w=60%,f=right](impala_ablations_table.svgz)

- **No-correction**: no off-policy correction;
- **$ε$-correction**: add a small value $ε=10^{-6}$
  during gradient calculation to prevent $π$ to be
  very small and lead to unstabilities during $\log π$
  computation;
- **1-step**: no off-policy correction in the update of the value function,
  TD errors in the policy gradient are multiplied by the corresponding $ρ$ but
  no $c$s; it can be considered V-trace “without traces”.

---
# IMPALA – Ablations

![w=63%,mw=80%,h=center,f=right](impala_ablations_graphs.svgz)

The effect of the policy lag (the number of updates the
actor is behind the learned policy) on the performance.

---
section: PopArt
class: section
# PopArt Normalization

---
# PopArt Normalization

An improvement of IMPALA from Sep 2018, which performs normalization of task
rewards instead of just reward clipping. PopArt stands for _Preserving Outputs
Precisely, while Adaptively Rescaling Targets_.

~~~
Assume the value estimate $v(s; →θ, σ, μ)$ is computed using a normalized value
predictor $n(s; →θ)$
$$v(s; →θ, σ, μ) ≝ σ n(s; →θ) + μ,$$
and further assume that $n(s; →θ)$ is an output of a linear function
$$n(s; →θ) ≝ →ω^T f(s; →θ-\{→ω, b\}) + b.$$

~~~
We can update the $σ$ and $μ$ using exponentially moving average with decay rate
$β$ (in the paper, first moment $μ$ and second moment $υ$ is tracked, and
the standard deviation is computed as $σ=\sqrt{υ-μ^2}$; decay rate $β=3 ⋅ 10^{-4}$ is employed).

---
# PopArt Normalization

Utilizing the parameters $μ$ and $σ$, we can normalize the observed (unnormalized) returns as
$(G - μ) / σ$, and use an actor-critic algorithm with advantage $(G - μ)/σ - n(S; →θ)$.

~~~
However, in order to make sure the value function estimate does not change when
the normalization parameters change, the parameters $→ω, b$ used to compute the
value estimate
$$v(s; →θ, σ, μ) ≝ σ ⋅ \Big(→ω^T f(s; →θ-\{→ω, b\}) + b\Big) + μ$$
are updated under any change $μ → μ'$ and $σ → σ'$ as
$$\begin{aligned}
  →ω' &← \frac{σ}{σ'}→ω,\\
  b' &← \frac{σb + μ - μ'}{σ'}.
\end{aligned}$$

~~~
In multi-task settings, we train a task-agnostic policy and task-specific value
functions (therefore, $→μ$, $→σ$, and $→n(s; →θ)$ are vectors).

---
# PopArt Results

![w=80%,h=center](popart_results.svgz)

~~~
![w=100%](popart_atari_curves.svgz)

---
# PopArt Results

![w=85%,h=center](popart_atari_statistics.svgz)

Normalization statistics on chosen environments.
